"""
Ã‰valuation automatique du RAG-DPO

Score chaque rÃ©ponse sur 4 axes :
1. Retrieval Correctness  â€” Les bons chunks ont Ã©tÃ© rÃ©cupÃ©rÃ©s ?
2. Answer Correctness     â€” La rÃ©ponse contient les Ã©lÃ©ments attendus ?
3. Faithfulness           â€” Pas d'hallucination / invention ?
4. Conciseness            â€” RÃ©ponse disciplinÃ©e (pas de sur-justification) ?

Usage :
    python eval/run_eval.py                     # Toutes les questions
    python eval/run_eval.py --ids q01 q05 q09   # Questions spÃ©cifiques
    python eval/run_eval.py --dry-run            # Affiche les questions sans exÃ©cuter
    python eval/run_eval.py --verbose            # Affiche les rÃ©ponses complÃ¨tes
"""
import json
import sys
import time
import re
import argparse
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple

# Setup path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))
sys.path.insert(0, str(project_root))

logging.basicConfig(level=logging.WARNING, format='%(asctime)s - %(levelname)s - %(message)s')
for noisy in ("httpx", "ollama", "chromadb", "sentence_transformers", "urllib3"):
    logging.getLogger(noisy).setLevel(logging.ERROR)
logger = logging.getLogger(__name__)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Scoring Functions
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def normalize_text(text: str) -> str:
    """Normalise le texte pour comparaison flexible."""
    text = text.lower()
    # Accents dÃ©jÃ  gÃ©rÃ©s par Python, on normalise les espaces
    text = re.sub(r'\s+', ' ', text).strip()
    return text


# Nombres Ã©crits en lettres â†’ chiffres (pour matching flexible)
_NUMBER_WORDS = {
    "zÃ©ro": "0", "un": "1", "une": "1", "deux": "2", "trois": "3",
    "quatre": "4", "cinq": "5", "six": "6", "sept": "7", "huit": "8",
    "neuf": "9", "dix": "10", "onze": "11", "douze": "12", "treize": "13",
    "quatorze": "14", "quinze": "15", "vingt": "20", "trente": "30",
}


def _normalize_numbers(text: str) -> str:
    """Remplace les nombres Ã©crits en lettres par leurs chiffres."""
    words = text.split()
    result = []
    for w in words:
        result.append(_NUMBER_WORDS.get(w, w))
    return " ".join(result)


def _flexible_match(item_norm: str, answer_norm: str) -> bool:
    """
    Matching flexible : gÃ¨re singulier/pluriel, accents, nombres lettres/chiffres.
    
    StratÃ©gie :
    1. Match exact
    2. Match avec normalisation des nombres ("deux ans" â†’ "2 ans")
    3. Match par mots individuels avec tolÃ©rance pluriel
       ("risque Ã©levÃ©" matche "risques Ã©levÃ©s")
    """
    # 1. Match exact
    if item_norm in answer_norm:
        return True
    
    # 2. Match avec normalisation des nombres
    item_num = _normalize_numbers(item_norm)
    answer_num = _normalize_numbers(answer_norm)
    if item_num in answer_num:
        return True
    
    # 3. Match par mots avec tolÃ©rance pluriel (s, es, Ã©es, Ã©s)
    item_words = item_norm.split()
    if len(item_words) >= 2:
        # Chaque mot de l'item doit apparaÃ®tre dans la rÃ©ponse (avec tolÃ©rance pluriel)
        all_found = True
        for word in item_words:
            # Chercher le mot exact ou ses variantes singulier/pluriel
            stems = {word}
            # Ajouter le pluriel
            stems.add(word + "s")
            stems.add(word + "es")
            # Retirer le pluriel (si le mot finit par s/es)
            if word.endswith("es") and len(word) > 3:
                stems.add(word[:-2])
            if word.endswith("s") and len(word) > 2:
                stems.add(word[:-1])
            # Variantes fÃ©minines/masculines
            if word.endswith("Ã©"):
                stems.add(word + "e")
                stems.add(word + "es")
                stems.add(word + "s")
            if word.endswith("Ã©e"):
                stems.add(word[:-1])
                stems.add(word + "s")
            if word.endswith("Ã©es"):
                stems.add(word[:-1])
                stems.add(word[:-2])
                stems.add(word[:-3])
            
            # VÃ©rifier qu'au moins une variante est dans la rÃ©ponse
            word_found = False
            for stem in stems:
                # Match en tant que mot (pas substring partiel)
                if re.search(r'\b' + re.escape(stem) + r'\b', answer_norm):
                    word_found = True
                    break
                # Aussi vÃ©rifier avec normalisation nombres
                stem_num = _normalize_numbers(stem)
                if stem_num != stem and re.search(r'\b' + re.escape(stem_num) + r'\b', answer_num):
                    word_found = True
                    break
            
            if not word_found:
                all_found = False
                break
        
        if all_found:
            return True
    
    return False


def _match_item_or_alternates(item: str, answer_norm: str) -> bool:
    """
    Matche un item (Ã©ventuellement avec alternatives sÃ©parÃ©es par |).
    Ex: "contrat de travail|contrat|exÃ©cution du contrat" â†’ match si au moins un trouvÃ©.
    """
    alternates = [normalize_text(alt.strip()) for alt in item.split("|")]
    for alt in alternates:
        if _flexible_match(alt, answer_norm):
            return True
    
    # Variantes sÃ©mantiques hardcodÃ©es (rÃ©tro-compatibilitÃ©)
    first = alternates[0]
    if first == "non":
        if any(neg in answer_norm for neg in ["ne ", "n'", "pas possible", "n'autorise pas", "pas de", "pas d'"]):
            return True
    if first == "pas possible":
        if any(neg in answer_norm for neg in ["ne peut pas", "n'est pas possible", "pas possible", "ne pas", "impossible", "interdit", "illÃ©gal"]):
            return True
    if first in ("obligatoire", "obligation"):
        if any(v in answer_norm for v in ["obligatoire", "obligation", "obligatoirement", "imposÃ©", "impose", "doit Ãªtre rÃ©alisÃ©e", "doit Ãªtre rÃ©alisÃ©"]):
            return True
    if first == "sanction":
        if any(v in answer_norm for v in ["sanction", "sanctions", "sanctionner", "sanctionnÃ©", "peine", "amende"]):
            return True
    return False


def score_must_include(
    answer: str,
    must_include: List[str],
    must_include_any: Optional[Dict] = None,
) -> Tuple[float, List[str], List[str]]:
    """
    VÃ©rifie que les Ã©lÃ©ments obligatoires sont prÃ©sents.
    Matching flexible : gÃ¨re singulier/pluriel, nombres en lettres, variantes.
    
    Supporte deux modes :
    - must_include : TOUS les items doivent Ãªtre prÃ©sents (AND)
    - must_include_any : Au moins N items parmi une liste (N parmi M)
      Format: {"min_count": N, "items": ["item1", "item2", ...]}
      Chaque item peut contenir des alternatives sÃ©parÃ©es par | :
      "contrat de travail|exÃ©cution du contrat"
    
    Le score combine les deux modes si les deux sont prÃ©sents.
    
    Returns:
        (score 0-1, found_items, missing_items)
    """
    answer_norm = normalize_text(answer)
    found = []
    missing = []
    scores = []
    
    # --- Mode classique : TOUS les items (AND) ---
    if must_include:
        for item in must_include:
            if _match_item_or_alternates(item, answer_norm):
                found.append(item)
            else:
                missing.append(item)
        scores.append(len(found) / len(must_include))
    
    # --- Mode "N parmi M" (must_include_any) ---
    if must_include_any:
        any_items = must_include_any.get("items", [])
        min_count = must_include_any.get("min_count", 1)
        any_found = []
        any_missing = []
        
        for item in any_items:
            if _match_item_or_alternates(item, answer_norm):
                any_found.append(item)
            else:
                any_missing.append(item)
        
        n_matched = len(any_found)
        if n_matched >= min_count:
            any_score = 1.0
        else:
            any_score = n_matched / min_count
        scores.append(any_score)
        
        # Ajouter au rapport
        found.extend([f"[any:{it}]" for it in any_found])
        if n_matched < min_count:
            missing.append(f"[any: {n_matched}/{min_count} trouvÃ©s, besoin {min_count}]")
    
    if not scores:
        return 1.0, [], []
    
    score = sum(scores) / len(scores)
    return score, found, missing


def score_must_not_include(answer: str, must_not_include: List[str]) -> Tuple[float, List[str]]:
    """
    VÃ©rifie que les Ã©lÃ©ments interdits sont absents.
    
    Returns:
        (score 0-1, violations)
    """
    if not must_not_include:
        return 1.0, []
    
    answer_norm = normalize_text(answer)
    violations = []
    
    for item in must_not_include:
        item_norm = normalize_text(item)
        if item_norm in answer_norm:
            violations.append(item)
    
    score = 1.0 - (len(violations) / len(must_not_include))
    return score, violations


def score_conciseness(answer: str, category: str) -> Tuple[float, str]:
    """
    Ã‰value la concision de la rÃ©ponse.
    
    Heuristiques :
    - DÃ©finitions simples : < 300 mots idÃ©al
    - Questions moyennes : < 500 mots idÃ©al
    - Refus hors pÃ©rimÃ¨tre : < 100 mots idÃ©al
    
    Returns:
        (score 0-1, assessment)
    """
    word_count = len(answer.split())
    
    # Limites par catÃ©gorie
    limits = {
        "definition": (100, 300),       # (idÃ©al, max)
        "obligation": (150, 500),
        "recommandation": (150, 500),
        "piÃ¨ge": (50, 200),
        "hors_perimetre": (30, 100),
    }
    
    ideal, max_words = limits.get(category, (150, 400))
    
    if word_count <= ideal:
        return 1.0, f"âœ… {word_count} mots (idÃ©al â‰¤{ideal})"
    elif word_count <= max_words:
        # Score linÃ©aire entre ideal et max
        score = 1.0 - 0.5 * (word_count - ideal) / (max_words - ideal)
        return score, f"âš ï¸ {word_count} mots (idÃ©al â‰¤{ideal}, max {max_words})"
    else:
        # Au-delÃ  du max : score faible
        score = max(0.2, 0.5 - 0.3 * (word_count - max_words) / max_words)
        return score, f"âŒ {word_count} mots (trop long, max {max_words})"


def score_source_quality(answer: str) -> Tuple[float, Dict]:
    """
    Ã‰value la qualitÃ© de l'usage des sources.
    
    Checks :
    - PrÃ©sence de [Source X] citations
    - Pas de source inventÃ©e (numÃ©ro > 12 suspect)
    - Pas de chunk non pertinent affichÃ© (art. 20 pour question sur art. 99)
    
    Returns:
        (score 0-1, details)
    """
    # Compter les citations
    citations = re.findall(r'\[Source\s*(\d+)\]', answer)
    unique_sources = set(citations)
    n_citations = len(citations)
    n_unique_sources = len(unique_sources)
    
    details = {
        "n_citations": n_citations,
        "n_unique_sources": n_unique_sources,
        "source_ids": sorted(unique_sources),
    }
    
    # Score de base
    if n_citations == 0:
        # Acceptable si c'est un refus ou hors pÃ©rimÃ¨tre
        return 0.5, details
    
    # VÃ©rifier cohÃ©rence des numÃ©ros de source
    max_source_id = max(int(s) for s in unique_sources)
    if max_source_id > 12:
        details["warning"] = f"Source {max_source_id} suspicieusement Ã©levÃ©e"
        return 0.3, details
    
    # Score basÃ© sur la diversitÃ© (mais pas trop)
    if n_unique_sources >= 1 and n_unique_sources <= 5:
        return 1.0, details
    elif n_unique_sources > 5:
        return 0.7, details  # Trop de sources â†’ dilution probable
    
    return 0.8, details


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LLM-as-Judge Scoring
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_llm_judge_provider = None  # InitialisÃ© une seule fois


def _get_llm_judge():
    """Singleton du provider LLM pour le judge."""
    global _llm_judge_provider
    if _llm_judge_provider is None:
        from src.utils.llm_provider import OllamaProvider
        _llm_judge_provider = OllamaProvider(
            base_url="http://localhost:11434",
            model="mistral-nemo"
        )
    return _llm_judge_provider


def llm_judge_correctness(
    question: str,
    expected_answer: str,
    actual_answer: str,
) -> Tuple[float, str]:
    """
    Utilise le LLM pour juger si la rÃ©ponse fournie couvre les concepts
    de la rÃ©ponse attendue.
    
    Avantage vs keyword matching : comprend les synonymes, paraphrases,
    reformulations ("2 ans" = "24 mois" = "deux annÃ©es").
    
    Returns:
        (score 0-1, justification)
    """
    provider = _get_llm_judge()
    
    prompt = f"""Tu es un Ã©valuateur expert en RGPD/CNIL. Compare la rÃ©ponse fournie avec la rÃ©ponse attendue.

Question : {question}

RÃ©ponse attendue (rÃ©fÃ©rence) :
{expected_answer}

RÃ©ponse fournie (Ã  Ã©valuer) :
{actual_answer}

Ã‰value si la rÃ©ponse fournie couvre les CONCEPTS CLÃ‰S de la rÃ©ponse attendue.
- Ne juge PAS les mots exacts, mais le SENS ("2 ans" = "24 mois" = "deux annÃ©es")
- Une rÃ©ponse qui dit la mÃªme chose avec des mots diffÃ©rents est CORRECTE
- Une rÃ©ponse partielle (certains concepts manquants) mÃ©rite un score proportionnel
- Une rÃ©ponse hors-sujet ou incorrecte = 0

RÃ©ponds UNIQUEMENT avec ce format exact :
SCORE: [nombre entier de 0 Ã  100]
JUSTIFICATION: [une phrase courte expliquant le score]"""
    
    try:
        response = provider.generate(
            prompt,
            temperature=0.0,
            max_tokens=150,
        )
        
        # Parser la rÃ©ponse
        score = 0.0
        justification = "Parsing failed"
        
        for line in response.strip().split("\n"):
            line = line.strip()
            if line.upper().startswith("SCORE"):
                # Extraire le nombre
                import re as _re
                numbers = _re.findall(r'\d+', line)
                if numbers:
                    raw_score = int(numbers[0])
                    score = max(0.0, min(1.0, raw_score / 100.0))
            elif line.upper().startswith("JUSTIFICATION"):
                justification = line.split(":", 1)[-1].strip()
        
        return score, justification
        
    except Exception as e:
        logger.warning(f"âš ï¸  LLM judge error: {e}")
        return -1.0, f"Error: {e}"  # -1 = signal to fall back to keyword-only


def evaluate_single(qa_item: Dict, answer: str, sources: List[Dict] = None, use_llm_judge: bool = True) -> Dict:
    """
    Ã‰value une seule rÃ©ponse du RAG contre le dataset attendu.
    
    Args:
        qa_item: Question du dataset avec expected_answer_summary, must_include, etc.
        answer: RÃ©ponse du RAG
        sources: Sources retournÃ©es (optionnel)
        use_llm_judge: Si True, utilise le LLM pour juger la correctness (plus fiable)
    
    Returns:
        Dict avec scores dÃ©taillÃ©s
    """
    result = {
        "id": qa_item["id"],
        "question": qa_item["question"],
        "category": qa_item["category"],
        "difficulty": qa_item["difficulty"],
    }
    
    # 1. Answer Correctness â€” Must Include
    include_score, found, missing = score_must_include(
        answer, qa_item.get("must_include", []),
        must_include_any=qa_item.get("must_include_any"),
    )
    result["answer_correctness"] = {
        "score": include_score,
        "found": found,
        "missing": missing,
    }
    
    # 1b. LLM Judge â€” Ã‰valuation sÃ©mantique par le LLM
    llm_score = -1.0
    llm_justification = "disabled"
    if use_llm_judge and qa_item.get("expected_answer_summary"):
        llm_score, llm_justification = llm_judge_correctness(
            question=qa_item["question"],
            expected_answer=qa_item["expected_answer_summary"],
            actual_answer=answer,
        )
    
    # Combiner keyword + LLM judge pour le score final de correctness
    if llm_score >= 0:  # LLM judge a fonctionnÃ©
        # LLM judge 70%, keyword 30% â€” le LLM comprend les synonymes
        combined_correctness = 0.70 * llm_score + 0.30 * include_score
        result["answer_correctness"]["llm_judge_score"] = round(llm_score, 2)
        result["answer_correctness"]["llm_justification"] = llm_justification
        result["answer_correctness"]["keyword_score"] = round(include_score, 2)
        result["answer_correctness"]["score"] = round(combined_correctness, 2)
    else:
        # Fallback keyword-only
        combined_correctness = include_score
        result["answer_correctness"]["llm_judge_score"] = None
        result["answer_correctness"]["llm_justification"] = llm_justification
    
    # 2. Faithfulness â€” Must Not Include (anti-hallucination)
    not_include_score, violations = score_must_not_include(
        answer, qa_item.get("must_not_include", [])
    )
    result["faithfulness"] = {
        "score": not_include_score,
        "violations": violations,
    }
    
    # 3. Conciseness
    concise_score, concise_assessment = score_conciseness(
        answer, qa_item["category"]
    )
    result["conciseness"] = {
        "score": concise_score,
        "assessment": concise_assessment,
    }
    
    # 4. Source Quality
    source_score, source_details = score_source_quality(answer)
    result["source_quality"] = {
        "score": source_score,
        **source_details,
    }
    
    # Score global pondÃ©rÃ©
    # Answer Correctness : 40%, Faithfulness : 30%, Conciseness : 15%, Sources : 15%
    result["global_score"] = round(
        0.40 * combined_correctness +
        0.30 * not_include_score +
        0.15 * concise_score +
        0.15 * source_score,
        2
    )
    
    return result


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Pipeline Init
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def init_pipeline():
    """Initialise le pipeline RAG pour l'Ã©valuation."""
    import chromadb
    from src.utils.llm_provider import OllamaProvider
    from src.rag.pipeline import create_pipeline
    
    vectordb_path = project_root / "data" / "vectordb" / "chromadb"
    if not vectordb_path.exists():
        print(f"âŒ VectorDB introuvable : {vectordb_path}")
        sys.exit(1)
    
    client = chromadb.PersistentClient(path=str(vectordb_path))
    collection = client.get_collection("rag_dpo_chunks")
    
    llm_provider = OllamaProvider(
        base_url="http://localhost:11434",
        model="mistral-nemo"
    )
    
    pipeline = create_pipeline(
        collection=collection,
        llm_provider=llm_provider,
        n_documents=5,
        n_chunks_per_doc=3,
        enable_hybrid=True,
        enable_reranker=True,
        enable_summary_prefilter=True,
        enable_validation=True,
        model="mistral-nemo",
        temperature=0.0,
        debug_mode=True,  # Pour rÃ©cupÃ©rer le contexte et les sources
    )
    
    return pipeline


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Main Evaluation Loop
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def run_evaluation(
    dataset_path: str,
    question_ids: Optional[List[str]] = None,
    dry_run: bool = False,
    verbose: bool = False,
    use_llm_judge: bool = True,
) -> Dict:
    """
    ExÃ©cute l'Ã©valuation complÃ¨te.
    
    Args:
        dataset_path: Chemin vers qa_dataset.json
        question_ids: Liste d'IDs Ã  Ã©valuer (None = tous)
        dry_run: Si True, affiche les questions sans les exÃ©cuter
        verbose: Si True, affiche les rÃ©ponses complÃ¨tes
    
    Returns:
        RÃ©sultats complets de l'Ã©valuation
    """
    # Charger le dataset
    with open(dataset_path, "r", encoding="utf-8") as f:
        dataset = json.load(f)
    
    # Filtrer si nÃ©cessaire
    if question_ids:
        dataset = [q for q in dataset if any(qid in q["id"] for qid in question_ids)]
    
    if not dataset:
        print("âŒ Aucune question trouvÃ©e avec les IDs spÃ©cifiÃ©s")
        return {}
    
    print(f"\n{'='*70}")
    print(f"ğŸ§ª Ã‰VALUATION RAG-DPO â€” {len(dataset)} questions")
    print(f"{'='*70}\n")
    
    if dry_run:
        for i, q in enumerate(dataset, 1):
            print(f"  {i:2d}. [{q['category']:20s}] {q['question']}")
            print(f"      Expected: {q['expected_answer_summary'][:100]}...")
            print()
        return {}
    
    # Init pipeline
    print("â³ Initialisation du pipeline RAG...")
    pipeline = init_pipeline()
    print("âœ… Pipeline prÃªt\n")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PHASE 1 : GÃ©nÃ©ration des rÃ©ponses RAG + scoring keyword
    # (Le pipeline RAG monopolise le GPU â€” on ne fait PAS de LLM judge ici)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    results = []
    answers_for_judge = []  # (index, qa_item, answer) pour phase 2
    total_time = 0
    
    print("ğŸ“ Phase 1 : GÃ©nÃ©ration des rÃ©ponses RAG\n")
    
    for i, qa_item in enumerate(dataset, 1):
        question = qa_item["question"]
        qid = qa_item["id"]
        
        print(f"â”€â”€ [{i}/{len(dataset)}] {qid} {'â”€'*40}")
        print(f"   Q: {question}")
        
        # ExÃ©cuter la query
        t_start = time.time()
        try:
            response = pipeline.query(question)
            elapsed = time.time() - t_start
            total_time += elapsed
            
            answer = response.answer
            sources = response.sources if response.sources else []
            
            # Ã‰valuer (keyword-only pour l'instant, LLM judge en phase 2)
            eval_result = evaluate_single(qa_item, answer, sources, use_llm_judge=False)
            eval_result["elapsed_seconds"] = round(elapsed, 1)
            eval_result["answer_length_words"] = len(answer.split())
            eval_result["_raw_answer"] = answer  # Conserver pour phase 2
            
            if verbose:
                print(f"   R: {answer[:500]}{'...' if len(answer) > 500 else ''}")
            
            # RÃ©sumÃ© rapide (keyword-only)
            global_score = eval_result["global_score"]
            icon = "ğŸŸ¢" if global_score >= 0.8 else "ğŸŸ¡" if global_score >= 0.5 else "ğŸ”´"
            
            ac = eval_result["answer_correctness"]
            ff = eval_result["faithfulness"]
            cc = eval_result["conciseness"]
            sq = eval_result["source_quality"]
            
            print(f"   {icon} Score: {global_score:.0%}  "
                  f"(Correct[kw]: {ac['score']:.0%} | "
                  f"Faithful: {ff['score']:.0%} | "
                  f"Concis: {cc['score']:.0%} | "
                  f"Sources: {sq['score']:.0%})  "
                  f"[{elapsed:.1f}s, {eval_result['answer_length_words']}w]")
            
            if ac["missing"]:
                print(f"   âš ï¸  Manquant (kw): {', '.join(ac['missing'])}")
            if ff["violations"]:
                print(f"   ğŸš« Violations: {', '.join(ff['violations'])}")
            
            # Stocker pour phase 2
            if use_llm_judge:
                answers_for_judge.append((len(results), qa_item, answer))
            
            results.append(eval_result)
            
        except Exception as e:
            elapsed = time.time() - t_start
            print(f"   âŒ ERREUR: {e}")
            results.append({
                "id": qid,
                "question": question,
                "error": str(e),
                "global_score": 0.0,
                "elapsed_seconds": round(elapsed, 1),
            })
        
        print()
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PHASE 2 : LLM-as-Judge (sÃ©quentiel, aprÃ¨s libÃ©ration du GPU par le pipeline)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    if use_llm_judge and answers_for_judge:
        # LibÃ©rer le pipeline RAG de la mÃ©moire avant le LLM judge
        del pipeline
        import gc
        gc.collect()
        
        print(f"\n{'='*70}")
        print(f"ğŸ¤– Phase 2 : LLM-as-Judge â€” {len(answers_for_judge)} rÃ©ponses Ã  Ã©valuer")
        print(f"{'='*70}\n")
        
        for idx, qa_item, answer in answers_for_judge:
            qid = qa_item["id"]
            expected = qa_item.get("expected_answer_summary", "")
            
            if not expected:
                continue
            
            print(f"   ğŸ¤– [{qid}] ", end="", flush=True)
            
            llm_score, llm_justification = llm_judge_correctness(
                question=qa_item["question"],
                expected_answer=expected,
                actual_answer=answer,
            )
            
            if llm_score >= 0:
                r = results[idx]
                kw_score = r["answer_correctness"]["score"]
                combined = round(0.70 * llm_score + 0.30 * kw_score, 2)
                
                r["answer_correctness"]["llm_judge_score"] = round(llm_score, 2)
                r["answer_correctness"]["llm_justification"] = llm_justification
                r["answer_correctness"]["keyword_score"] = round(kw_score, 2)
                r["answer_correctness"]["score"] = combined
                
                # Recalculer le score global
                r["global_score"] = round(
                    0.40 * combined +
                    0.30 * r["faithfulness"]["score"] +
                    0.15 * r["conciseness"]["score"] +
                    0.15 * r["source_quality"]["score"],
                    2
                )
                
                icon = "ğŸŸ¢" if r["global_score"] >= 0.8 else "ğŸŸ¡" if r["global_score"] >= 0.5 else "ğŸ”´"
                print(f"{icon} LLM={llm_score:.0%} KW={kw_score:.0%} â†’ {combined:.0%} (global={r['global_score']:.0%})")
                if verbose:
                    print(f"      ğŸ“ {llm_justification}")
            else:
                print(f"âš ï¸  Erreur â€” fallback keyword")
        
        print()
    
    # Nettoyer les champs internes
    for r in results:
        r.pop("_raw_answer", None)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Rapport Final
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    print(f"\n{'='*70}")
    print(f"ğŸ“Š RAPPORT D'Ã‰VALUATION")
    print(f"{'='*70}\n")
    
    # Scores globaux
    valid_results = [r for r in results if "error" not in r]
    if valid_results:
        avg_global = sum(r["global_score"] for r in valid_results) / len(valid_results)
        avg_correct = sum(r["answer_correctness"]["score"] for r in valid_results) / len(valid_results)
        avg_faithful = sum(r["faithfulness"]["score"] for r in valid_results) / len(valid_results)
        avg_concise = sum(r["conciseness"]["score"] for r in valid_results) / len(valid_results)
        avg_sources = sum(r["source_quality"]["score"] for r in valid_results) / len(valid_results)
        avg_time = sum(r["elapsed_seconds"] for r in valid_results) / len(valid_results)
        
        print(f"  ğŸ“ˆ Score Global Moyen   : {avg_global:.0%}")
        print(f"  âœ… Answer Correctness   : {avg_correct:.0%}")
        print(f"  ğŸ›¡ï¸  Faithfulness         : {avg_faithful:.0%}")
        print(f"  ğŸ“ Conciseness          : {avg_concise:.0%}")
        print(f"  ğŸ“š Source Quality        : {avg_sources:.0%}")
        print(f"  â±ï¸  Temps moyen/question : {avg_time:.1f}s")
        print(f"  â±ï¸  Temps total          : {total_time:.0f}s")
        
        # Par catÃ©gorie
        categories = set(r["category"] for r in valid_results)
        print(f"\n  Par catÃ©gorie :")
        for cat in sorted(categories):
            cat_results = [r for r in valid_results if r["category"] == cat]
            cat_avg = sum(r["global_score"] for r in cat_results) / len(cat_results)
            icon = "ğŸŸ¢" if cat_avg >= 0.8 else "ğŸŸ¡" if cat_avg >= 0.5 else "ğŸ”´"
            print(f"    {icon} {cat:20s} : {cat_avg:.0%} ({len(cat_results)} questions)")
        
        # Top 3 pires
        sorted_results = sorted(valid_results, key=lambda r: r["global_score"])
        print(f"\n  ğŸ”´ Points faibles (3 pires) :")
        for r in sorted_results[:3]:
            print(f"    â€¢ {r['id']:35s} : {r['global_score']:.0%} â€” {r['question'][:60]}")
        
        # Top 3 meilleurs
        print(f"\n  ğŸŸ¢ Points forts (3 meilleurs) :")
        for r in sorted_results[-3:]:
            print(f"    â€¢ {r['id']:35s} : {r['global_score']:.0%} â€” {r['question'][:60]}")
    
    if results:
        errors = [r for r in results if "error" in r]
        if errors:
            print(f"\n  âŒ {len(errors)} erreur(s) :")
            for r in errors:
                print(f"    â€¢ {r['id']} : {r['error'][:100]}")
    
    # Sauvegarder les rÃ©sultats
    output_path = project_root / "eval" / f"results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump({
            "timestamp": datetime.now().isoformat(),
            "n_questions": len(dataset),
            "n_valid": len(valid_results),
            "n_errors": len(results) - len(valid_results),
            "avg_global_score": round(avg_global, 3) if valid_results else 0,
            "avg_time_per_question": round(avg_time, 1) if valid_results else 0,
            "results": results,
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ RÃ©sultats sauvegardÃ©s : {output_path}")
    print(f"{'='*70}\n")
    
    return {
        "avg_global": avg_global if valid_results else 0,
        "results": results,
        "output_path": str(output_path),
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CLI
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    parser = argparse.ArgumentParser(description="Ã‰valuation automatique du RAG-DPO")
    parser.add_argument("--ids", nargs="+", help="IDs des questions Ã  Ã©valuer (ex: q01 q05)")
    parser.add_argument("--dry-run", action="store_true", help="Affiche les questions sans exÃ©cuter")
    parser.add_argument("--verbose", action="store_true", help="Affiche les rÃ©ponses complÃ¨tes")
    parser.add_argument("--no-llm-judge", action="store_true", help="DÃ©sactive le LLM-as-judge (keyword matching seul)")
    parser.add_argument("--dataset", default=None, help="Chemin vers le dataset JSON")
    
    args = parser.parse_args()
    
    dataset_path = args.dataset or str(project_root / "eval" / "qa_dataset.json")
    
    if not Path(dataset_path).exists():
        print(f"âŒ Dataset introuvable : {dataset_path}")
        sys.exit(1)
    
    run_evaluation(
        dataset_path=dataset_path,
        question_ids=args.ids,
        dry_run=args.dry_run,
        verbose=args.verbose,
        use_llm_judge=not args.no_llm_judge,
    )


if __name__ == "__main__":
    main()
