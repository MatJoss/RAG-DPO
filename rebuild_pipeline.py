"""
ðŸ”„ Script de reconstruction du pipeline RAG-DPO
Relance les Ã©tapes de traitement avec Mistral Nemo 12B + extraction region-content.

Ã‰tapes disponibles :
  3  : Classification hybride Keywords + LLM (hybrid_filter.py)
  4  : Organisation keep/archive (organize_keep_archive.py)
  5a : PrÃ©-catÃ©gorisation documents (classify_documents.py)
  5b : Chunking + classification chunk-level (process_and_chunk.py)
  6a : Indexation ChromaDB (create_chromadb_index.py)
  6b : GÃ©nÃ©ration rÃ©sumÃ©s structurÃ©s (generate_document_summaries.py)

Usage :
  python rebuild_pipeline.py              # Tout relancer (3 â†’ 6b)
  python rebuild_pipeline.py --from 5b    # Reprendre depuis 5b
  python rebuild_pipeline.py --only 6b    # Seulement 6b
  python rebuild_pipeline.py --steps 5a 5b 6a  # Seulement ces Ã©tapes
  python rebuild_pipeline.py --check      # VÃ©rifier l'Ã©tat du pipeline
  python rebuild_pipeline.py --test 10    # Mode test : N documents max
  python rebuild_pipeline.py --fresh      # Ignorer rÃ©sultats existants
"""

import subprocess
import sys
import time
import json
import argparse
from pathlib import Path

PROJECT_ROOT = Path(__file__).parent

# DÃ©finition ordonnÃ©e de toutes les Ã©tapes
STEPS = {
    '3':  {
        'name': 'Phase 3 : Classification hybride Keywords + LLM (Nemo)',
        'script': 'src/processing/hybrid_filter.py',
        'args': [],
        'description': 'Classe les ~8000 HTML bruts en keep/exclude via LLM',
    },
    '4':  {
        'name': 'Phase 4 : Organisation keep/archive',
        'script': 'src/processing/organize_keep_archive.py',
        'args': ['--execute', '--yes', '--clean'],
        'description': 'Copie les fichiers pertinents dans data/keep/',
    },
    '4b': {
        'name': 'Phase 4B : Classification images (OCR + LLaVA)',
        'script': 'src/processing/classify_images.py',
        'args': [],
        'description': 'Tesseract OCR + LLaVA vision â†’ trie schÃ©mas DPO vs photos dÃ©co',
    },
    '4c': {
        'name': 'Phase 4C : DÃ©duplication corpus',
        'script': 'src/processing/deduplicate_corpus.py',
        'args': [],
        'description': 'Ã‰limine 51% de doublons (mÃªmes contenus sous URLs diffÃ©rentes)',
    },
    '5a': {
        'name': 'Phase 5A : PrÃ©-catÃ©gorisation documents (Nemo)',
        'script': 'src/processing/classify_documents.py',
        'args': [],
        'description': 'Classifie nature juridique: DOCTRINE/GUIDE/SANCTION/TECHNIQUE',
    },
    '5b': {
        'name': 'Phase 5B : Chunking + Classification chunk-level',
        'script': 'src/processing/process_and_chunk.py',
        'args': [],
        'description': 'DÃ©coupe en chunks structurels + classification heuristique/LLM',
    },
    '6a': {
        'name': 'Phase 6A : Indexation ChromaDB (reset)',
        'script': 'src/processing/create_chromadb_index.py',
        'args': ['--mode', 'reset'],
        'description': 'GÃ©nÃ¨re embeddings nomic-embed-text et indexe dans ChromaDB',
    },
    '6b': {
        'name': 'Phase 6B : GÃ©nÃ©ration rÃ©sumÃ©s structurÃ©s (Nemo)',
        'script': 'src/processing/generate_document_summaries.py',
        'args': [],
        'description': 'GÃ©nÃ¨re fiches synthÃ©tiques par document pour recherche hiÃ©rarchique',
    },
    '6c': {
        'name': 'Phase 6C : Nettoyage post-rÃ©sumÃ©s',
        'script': 'src/processing/phase_6c_cleanup.py',
        'args': [],
        'description': 'Purge pages navigation, archive fichiers, nettoie ChromaDB/JSONL',
    },
}

STEP_ORDER = ['3', '4', '4b', '4c', '5a', '5b', '6a', '6b', '6c']


def run_step(step_id: str, test_mode: int = None, fresh: bool = False):
    """ExÃ©cute une Ã©tape du pipeline."""
    step = STEPS[step_id]
    
    print("\n" + "=" * 70)
    print(f"  {step['name']}")
    print(f"  {step['description']}")
    print("=" * 70)
    
    cmd = [sys.executable, str(PROJECT_ROOT / step['script'])]
    cmd.extend(step['args'])
    
    # Mode test : ajouter --test N si supportÃ©
    if test_mode:
        cmd.extend(['--test', str(test_mode)])
    
    # Mode fresh : passer --fresh aux scripts qui le supportent
    if fresh and step_id in ('3', '4b', '4c', '5a', '5b'):
        cmd.append('--fresh')
    
    start = time.time()
    result = subprocess.run(cmd, cwd=str(PROJECT_ROOT))
    elapsed = time.time() - start
    
    if result.returncode != 0:
        print(f"\n  ECHEC: {step['name']} (code {result.returncode})")
        print(f"  Temps: {elapsed:.0f}s")
        
        resp = input("\n  Continuer malgrÃ© l'erreur ? (o/N) : ").strip().lower()
        if resp != 'o':
            sys.exit(1)
    else:
        print(f"\n  OK: {step['name']} en {elapsed:.0f}s ({elapsed/60:.1f} min)")
    
    return elapsed


def sanity_check(step_id: str, test_mode: int = None) -> bool:
    """VÃ©rifie la cohÃ©rence des donnÃ©es aprÃ¨s une phase.
    
    Retourne True si tout est OK, False si des problÃ¨mes dÃ©tectÃ©s.
    """
    print(f"\nðŸ” Sanity check post-phase {step_id}...")
    
    issues = []
    warnings = []
    
    if step_id == '3':
        # Phase 3 : hybrid_classification.json doit exister et Ãªtre cohÃ©rent
        hc_file = PROJECT_ROOT / "data" / "raw" / "cnil" / "hybrid_classification.json"
        if not hc_file.exists():
            issues.append("hybrid_classification.json MANQUANT")
        else:
            try:
                with open(hc_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                llm_count = len(data.get('llm_classified', {}))
                obvious_count = len(data.get('excluded_obvious', []))
                keyword_count = len(data.get('excluded_keywords', []))
                total_processed = llm_count + obvious_count + keyword_count
                
                # Compter les erreurs LLM
                error_count = sum(1 for v in data.get('llm_classified', {}).values() 
                                 if v.get('error') or 'Erreur' in v.get('raison', ''))
                
                # Compter les pertinents
                kept_count = sum(1 for v in data.get('llm_classified', {}).values() 
                                if v.get('pertinent', False))
                
                print(f"   ðŸ“Š Total traitÃ©s    : {total_processed}")
                print(f"   ðŸ“Š ClassifiÃ©s LLM   : {llm_count}")
                print(f"   ðŸ“Š Exclus URL       : {obvious_count}")
                print(f"   ðŸ“Š Exclus keywords  : {keyword_count}")
                print(f"   ðŸ“Š GardÃ©s (pertinents): {kept_count}")
                
                if error_count > 0:
                    error_pct = error_count / max(1, llm_count) * 100
                    if error_pct > 10:
                        issues.append(f"{error_count} erreurs LLM ({error_pct:.0f}%) â€” taux trop Ã©levÃ©")
                    else:
                        warnings.append(f"{error_count} erreurs LLM ({error_pct:.1f}%)")
                
                if not test_mode:
                    raw_html_count = sum(1 for _ in (PROJECT_ROOT / "data" / "raw" / "html").glob('*.html'))
                    if total_processed < raw_html_count * 0.9:
                        warnings.append(f"Seulement {total_processed}/{raw_html_count} HTML traitÃ©s â€” possible interruption")
                
                if kept_count == 0:
                    issues.append("Aucun document gardÃ© ! ProblÃ¨me de classification ?")
                
            except json.JSONDecodeError as e:
                issues.append(f"JSON invalide: {e}")
    
    elif step_id == '4':
        # Phase 4 : les fichiers keep doivent exister
        keep_html = PROJECT_ROOT / "data" / "keep" / "cnil" / "html"
        keep_pdf = PROJECT_ROOT / "data" / "keep" / "cnil" / "pdf"
        keep_docs = PROJECT_ROOT / "data" / "keep" / "cnil" / "docs"
        
        html_count = sum(1 for _ in keep_html.glob('*.html')) if keep_html.exists() else 0
        pdf_count = sum(1 for _ in keep_pdf.glob('*.pdf')) if keep_pdf.exists() else 0
        doc_count = sum(1 for _ in keep_docs.iterdir()) if keep_docs.exists() else 0
        
        print(f"   ðŸ“Š HTML keep : {html_count}")
        print(f"   ðŸ“Š PDF keep  : {pdf_count}")
        print(f"   ðŸ“Š Docs keep : {doc_count}")
        
        if html_count == 0:
            issues.append("Aucun HTML dans keep/ â€” Phase 3 ou 4 a Ã©chouÃ© ?")
        
        # VÃ©rifier que keep_manifest existe
        manifest = PROJECT_ROOT / "data" / "raw" / "cnil" / "keep_manifest.json"
        if not manifest.exists():
            issues.append("keep_manifest.json MANQUANT")
    
    elif step_id == '4b':
        # Phase 4B : image_classification.json doit exister
        ic_file = PROJECT_ROOT / "data" / "raw" / "cnil" / "image_classification.json"
        keep_images = PROJECT_ROOT / "data" / "keep" / "cnil" / "images"
        
        if ic_file.exists():
            with open(ic_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            stats = data.get('stats', {})
            total = stats.get('total', 0)
            schema = stats.get('schema_dpo', 0)
            infog = stats.get('infographie', 0)
            deco = stats.get('photo_deco', 0)
            removed = stats.get('removed_from_keep', 0)
            
            print(f"   ðŸ“Š Images totales     : {total}")
            print(f"   ðŸ“Š SchÃ©mas DPO        : {schema}")
            print(f"   ðŸ“Š Infographies       : {infog}")
            print(f"   ðŸ“Š Photos/DÃ©co        : {deco} (retirÃ©es)")
            print(f"   ðŸ“Š RetirÃ©es de keep/  : {removed}")
            
            # VÃ©rification cohÃ©rence
            img_count = sum(1 for _ in keep_images.glob('*')) if keep_images.exists() else 0
            print(f"   ðŸ“Š Images restantes   : {img_count}")
            
            if schema + infog == 0:
                warnings.append("Aucun schÃ©ma/infographie gardÃ© â€” seuils trop stricts ?")
        else:
            issues.append("image_classification.json MANQUANT")
    
    elif step_id == '4c':
        # Phase 4C : dÃ©duplication â€” rapport doit exister, manifest mis Ã  jour
        report_file = PROJECT_ROOT / "data" / "raw" / "cnil" / "dedup_report.json"
        manifest_file = PROJECT_ROOT / "data" / "raw" / "cnil" / "keep_manifest.json"
        
        if report_file.exists():
            with open(report_file, 'r', encoding='utf-8') as f:
                report = json.load(f)
            stats = report.get('stats', {})
            
            for typ in ['html', 'pdf', 'docs', 'images']:
                before = stats.get(f'{typ}_before', 0)
                after = stats.get(f'{typ}_after', 0)
                removed = stats.get(f'{typ}_removed', 0)
                print(f"   ðŸ“Š {typ.upper():6s} : {before} â†’ {after} (-{removed})")
            
            total_before = sum(stats.get(f'{t}_before', 0) for t in ['html', 'pdf', 'docs', 'images'])
            total_after = sum(stats.get(f'{t}_after', 0) for t in ['html', 'pdf', 'docs', 'images'])
            print(f"   ðŸ“Š TOTAL  : {total_before} â†’ {total_after} (-{total_before - total_after})")
            
            # VÃ©rifier le manifest
            if manifest_file.exists():
                with open(manifest_file, 'r', encoding='utf-8') as f:
                    mf = json.load(f)
                manifest_total = len(mf.get('html',[])) + len(mf.get('pdfs',[])) + len(mf.get('docs',[])) + len(mf.get('images',[]))
                if manifest_total != total_after:
                    issues.append(f"Manifest ({manifest_total}) != rapport ({total_after})")
                else:
                    print(f"   ðŸ“Š Manifest cohÃ©rent : {manifest_total} documents")
            
            if total_after == 0:
                issues.append("Corpus vide aprÃ¨s dÃ©duplication !")
        else:
            issues.append("dedup_report.json MANQUANT")
    
    elif step_id == '5a':
        # Phase 5A : document_metadata.json doit exister avec classifications
        dm_file = PROJECT_ROOT / "data" / "raw" / "cnil" / "document_metadata.json"
        if not dm_file.exists():
            issues.append("document_metadata.json MANQUANT")
        else:
            try:
                with open(dm_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                metadata = data.get('metadata', {})
                classified = sum(1 for v in metadata.values() if v.get('nature'))
                error_count = sum(1 for v in metadata.values() if v.get('error'))
                
                print(f"   ðŸ“Š Documents classifiÃ©s : {classified}/{len(metadata)}")
                
                if error_count > 0:
                    error_pct = error_count / max(1, len(metadata)) * 100
                    if error_pct > 10:
                        issues.append(f"{error_count} erreurs classification ({error_pct:.0f}%)")
                    else:
                        warnings.append(f"{error_count} erreurs classification ({error_pct:.1f}%)")
                
                # Distribution natures
                natures = {}
                for v in metadata.values():
                    n = v.get('nature', 'UNKNOWN')
                    natures[n] = natures.get(n, 0) + 1
                for n, c in sorted(natures.items()):
                    print(f"   ðŸ“Š   {n:12s} : {c}")
                
            except json.JSONDecodeError as e:
                issues.append(f"JSON invalide: {e}")
    
    elif step_id == '5b':
        # Phase 5B : processed_chunks.jsonl doit exister et Ãªtre non vide
        chunks_file = PROJECT_ROOT / "data" / "raw" / "cnil" / "processed_chunks.jsonl"
        if not chunks_file.exists():
            issues.append("processed_chunks.jsonl MANQUANT")
        else:
            chunk_count = 0
            doc_ids = set()
            errors = 0
            
            with open(chunks_file, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        chunk = json.loads(line)
                        chunk_count += 1
                        doc_ids.add(chunk.get('document_id', ''))
                    except json.JSONDecodeError:
                        errors += 1
            
            print(f"   ðŸ“Š Chunks totaux    : {chunk_count}")
            print(f"   ðŸ“Š Documents uniques: {len(doc_ids)}")
            print(f"   ðŸ“Š Moyenne          : {chunk_count / max(1, len(doc_ids)):.1f} chunks/doc")
            
            if errors > 0:
                warnings.append(f"{errors} lignes JSON invalides dans JSONL")
            
            if chunk_count == 0:
                issues.append("JSONL vide â€” aucun chunk produit")
    
    elif step_id == '6a':
        # Phase 6A : ChromaDB doit exister
        chromadb_dir = PROJECT_ROOT / "data" / "vectordb" / "chromadb"
        if not chromadb_dir.exists():
            issues.append("Dossier ChromaDB MANQUANT")
        else:
            # Essayer de compter les documents indexÃ©s
            try:
                import chromadb
                client = chromadb.PersistentClient(path=str(chromadb_dir))
                collection = client.get_collection("rag_dpo_chunks")
                count = collection.count()
                print(f"   ðŸ“Š Documents indexÃ©s : {count}")
                
                if count == 0:
                    issues.append("ChromaDB vide â€” aucun document indexÃ©")
            except Exception as e:
                warnings.append(f"Impossible de vÃ©rifier ChromaDB: {e}")
    
    elif step_id == '6b':
        # Phase 6B : document_summaries.json doit exister
        summaries_file = PROJECT_ROOT / "data" / "keep" / "cnil" / "document_summaries.json"
        if not summaries_file.exists():
            issues.append("document_summaries.json MANQUANT")
        else:
            try:
                with open(summaries_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # Format: dict {doc_path: {summary, ...}} 
                total = len(data)
                nav_count = sum(1 for s in data.values() if s.get('error') == 'navigation_content')
                empty_count = sum(1 for s in data.values() if s.get('error') == 'empty_content')
                error_count = sum(1 for s in data.values() if s.get('error') and s.get('error') not in ('navigation_content', 'empty_content'))
                ok_count = total - nav_count - empty_count - error_count
                
                print(f"   ðŸ“Š RÃ©sumÃ©s gÃ©nÃ©rÃ©s  : {ok_count}")
                print(f"   ðŸ“Š Navigation (skip): {nav_count}")
                if empty_count:
                    print(f"   ðŸ“Š Vides (skip)     : {empty_count}")
                
                if error_count > 0:
                    error_pct = error_count / max(1, total) * 100
                    if error_pct > 10:
                        issues.append(f"{error_count} erreurs rÃ©sumÃ©s ({error_pct:.0f}%)")
                    else:
                        warnings.append(f"{error_count} erreurs rÃ©sumÃ©s ({error_pct:.1f}%)")
                
                if ok_count == 0:
                    issues.append("Aucun rÃ©sumÃ© gÃ©nÃ©rÃ©")
                    
            except json.JSONDecodeError as e:
                issues.append(f"JSON invalide: {e}")
    
    elif step_id == '6c':
        # Phase 6C : vÃ©rifier nettoyage post-rÃ©sumÃ©s
        summaries_file = PROJECT_ROOT / "data" / "keep" / "cnil" / "document_summaries.json"
        if not summaries_file.exists():
            issues.append("document_summaries.json MANQUANT")
        else:
            try:
                with open(summaries_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                cleaned = sum(1 for s in data.values() if s.get('cleaned'))
                nav_remaining = sum(1 for s in data.values() if s.get('error') == 'navigation_content')
                ok_count = sum(1 for s in data.values() if s.get('summary') and 'error' not in s and not s.get('cleaned'))
                
                print(f"   ðŸ“Š RÃ©sumÃ©s OK       : {ok_count}")
                print(f"   ðŸ“Š Nav nettoyÃ©s     : {cleaned}")
                print(f"   ðŸ“Š Nav restants     : {nav_remaining}")
                
                if nav_remaining > 0:
                    warnings.append(f"{nav_remaining} entrÃ©es navigation non nettoyÃ©es")
                
                # VÃ©rifier cohÃ©rence ChromaDB
                chromadb_dir = PROJECT_ROOT / "data" / "vectordb" / "chromadb"
                if chromadb_dir.exists():
                    try:
                        import chromadb
                        client = chromadb.PersistentClient(path=str(chromadb_dir))
                        collection = client.get_collection("rag_dpo_chunks")
                        print(f"   ðŸ“Š ChromaDB chunks  : {collection.count()}")
                    except Exception:
                        pass
                
            except json.JSONDecodeError as e:
                issues.append(f"JSON invalide: {e}")
    
    # Bilan
    if warnings:
        for w in warnings:
            print(f"   âš ï¸  {w}")
    
    if issues:
        for issue in issues:
            print(f"   âŒ {issue}")
        print(f"\n   âŒ Sanity check Phase {step_id} : {len(issues)} PROBLÃˆME(S)")
        return False
    else:
        print(f"   âœ… Sanity check Phase {step_id} : OK")
        return True


def check_state():
    """VÃ©rifie l'Ã©tat actuel du pipeline."""
    print("=" * 70)
    print("  Ã‰TAT DU PIPELINE RAG-DPO")
    print("=" * 70)
    
    checks = {
        "HTML bruts (raw)": ("dir", PROJECT_ROOT / "data" / "raw" / "cnil" / "html"),
        "PDF bruts (raw)": ("dir", PROJECT_ROOT / "data" / "raw" / "cnil" / "pdf"),
        "Docs bruts (raw)": ("dir", PROJECT_ROOT / "data" / "raw" / "cnil" / "docs"),
        "Classification hybride": ("file", PROJECT_ROOT / "data" / "raw" / "cnil" / "hybrid_classification.json"),
        "Keep manifest": ("file", PROJECT_ROOT / "data" / "raw" / "cnil" / "keep_manifest.json"),
        "HTML gardÃ©s (keep)": ("dir", PROJECT_ROOT / "data" / "keep" / "cnil" / "html"),
        "PDF gardÃ©s (keep)": ("dir", PROJECT_ROOT / "data" / "keep" / "cnil" / "pdf"),
        "Docs gardÃ©s (keep)": ("dir", PROJECT_ROOT / "data" / "keep" / "cnil" / "docs"),
        "Images gardÃ©es (keep)": ("dir", PROJECT_ROOT / "data" / "keep" / "cnil" / "images"),
        "Metadata (keep)": ("dir", PROJECT_ROOT / "data" / "keep" / "cnil" / "metadata"),
        "Classification images": ("file", PROJECT_ROOT / "data" / "raw" / "cnil" / "image_classification.json"),
        "Rapport dÃ©duplication": ("file", PROJECT_ROOT / "data" / "raw" / "cnil" / "dedup_report.json"),
        "Classification docs": ("file", PROJECT_ROOT / "data" / "raw" / "cnil" / "document_metadata.json"),
        "Chunks (processed)": ("file", PROJECT_ROOT / "data" / "raw" / "cnil" / "processed_chunks.jsonl"),
        "VectorDB ChromaDB": ("dir", PROJECT_ROOT / "data" / "vectordb" / "chromadb"),
        "RÃ©sumÃ©s documents": ("file", PROJECT_ROOT / "data" / "keep" / "cnil" / "document_summaries.json"),
    }
    
    for label, (kind, path) in checks.items():
        if path.exists():
            if kind == "file":
                size_mb = path.stat().st_size / (1024 * 1024)
                print(f"  OK  {label:30s} ({size_mb:.1f} MB)")
            else:
                count = sum(1 for _ in path.iterdir()) if path.is_dir() else 0
                print(f"  OK  {label:30s} ({count} fichiers)")
        else:
            print(f"  --  {label:30s} MANQUANT")
    
    print("=" * 70)


def main():
    parser = argparse.ArgumentParser(
        description="Reconstruction pipeline RAG-DPO",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples :
  python rebuild_pipeline.py                    # Pipeline complet (3â†’6b)
  python rebuild_pipeline.py --from 5b          # Reprendre depuis 5b
  python rebuild_pipeline.py --only 6a          # Seulement l'indexation
  python rebuild_pipeline.py --steps 5a 5b 6a   # Ã‰tapes spÃ©cifiques
  python rebuild_pipeline.py --check            # Ã‰tat du pipeline
  python rebuild_pipeline.py --only 3 --test 20 # Test Phase 3 sur 20 docs
  python rebuild_pipeline.py --fresh            # Ignorer rÃ©sultats existants
  python rebuild_pipeline.py --no-sanity        # Pas de vÃ©rification post-phase
        """
    )
    parser.add_argument("--from", dest="from_step", choices=STEP_ORDER,
                       help="Reprendre depuis une Ã©tape spÃ©cifique")
    parser.add_argument("--only", choices=STEP_ORDER,
                       help="ExÃ©cuter seulement une Ã©tape")
    parser.add_argument("--steps", nargs='+', choices=STEP_ORDER,
                       help="ExÃ©cuter ces Ã©tapes spÃ©cifiques (dans l'ordre)")
    parser.add_argument("--check", action="store_true",
                       help="VÃ©rifier l'Ã©tat du pipeline")
    parser.add_argument("--test", type=int, default=None,
                       help="Mode test : limiter Ã  N documents")
    parser.add_argument("--fresh", action="store_true",
                       help="Ignorer les rÃ©sultats existants (recommencer Ã  zÃ©ro)")
    parser.add_argument("--no-sanity", action="store_true",
                       help="DÃ©sactiver les sanity checks post-phase")
    
    args = parser.parse_args()
    
    if args.check:
        check_state()
        # Lancer aussi les sanity checks dÃ©taillÃ©s
        print("\n" + "=" * 70)
        print("  SANITY CHECKS DÃ‰TAILLÃ‰S")
        print("=" * 70)
        for step_id in STEP_ORDER:
            sanity_check(step_id, test_mode=args.test)
        return
    
    # DÃ©terminer quelles Ã©tapes exÃ©cuter
    if args.only:
        steps_to_run = [args.only]
    elif args.steps:
        # Respecter l'ordre canonique
        steps_to_run = [s for s in STEP_ORDER if s in args.steps]
    elif args.from_step:
        start_idx = STEP_ORDER.index(args.from_step)
        steps_to_run = STEP_ORDER[start_idx:]
    else:
        steps_to_run = STEP_ORDER[:]
    
    # Afficher plan
    print("=" * 70)
    print("  RECONSTRUCTION PIPELINE RAG-DPO")
    print("  ModÃ¨le: mistral-nemo (12B, 128K ctx)")
    print("  Embeddings: nomic-embed-text (768 dim)")
    if args.test:
        print(f"  MODE TEST: {args.test} documents max")
    if args.fresh:
        print(f"  MODE FRESH: rÃ©sultats existants ignorÃ©s")
    print(f"  Resume: {'dÃ©sactivÃ© (fresh)' if args.fresh else 'activÃ© (reprise auto)'}")
    print(f"  Sanity checks: {'dÃ©sactivÃ©s' if args.no_sanity else 'activÃ©s (post-phase)'}")
    print("=" * 70)
    
    print(f"\n  Ã‰tapes planifiÃ©es ({len(steps_to_run)}) :")
    for s in steps_to_run:
        print(f"    [{s:>2s}] {STEPS[s]['name']}")
    
    if not args.test:
        resp = input("\n  Lancer ? (O/n) : ").strip().lower()
        if resp == 'n':
            print("  AnnulÃ©.")
            return
    
    # ExÃ©cution
    total_start = time.time()
    results = []
    sanity_results = []
    
    for step_id in steps_to_run:
        elapsed = run_step(step_id, test_mode=args.test, fresh=args.fresh)
        results.append((step_id, STEPS[step_id]['name'], elapsed))
        
        # Sanity check post-phase
        if not args.no_sanity:
            ok = sanity_check(step_id, test_mode=args.test)
            sanity_results.append((step_id, ok))
            
            if not ok:
                print(f"\nâš ï¸  ProblÃ¨mes dÃ©tectÃ©s en Phase {step_id}.")
                resp = input("  [C]ontinuer / [R]elancer la phase / [A]rrÃªter ? (c/r/a) : ").strip().lower()
                if resp == 'r':
                    print(f"  ðŸ”„ Relance Phase {step_id}...")
                    elapsed2 = run_step(step_id, test_mode=args.test, fresh=args.fresh)
                    results[-1] = (step_id, STEPS[step_id]['name'], elapsed + elapsed2)
                    ok2 = sanity_check(step_id, test_mode=args.test)
                    sanity_results[-1] = (step_id, ok2)
                    if not ok2:
                        resp2 = input("  Toujours KO. Continuer quand mÃªme ? (o/N) : ").strip().lower()
                        if resp2 != 'o':
                            break
                elif resp == 'a':
                    print("  ðŸ›‘ ArrÃªt du pipeline.")
                    break
        else:
            # Pause entre Ã©tapes pour confirmer
            if step_id != steps_to_run[-1]:
                resp = input(f"\n  Phase {step_id} terminÃ©e. Continuer ? (O/n) : ").strip().lower()
                if resp == 'n':
                    print("  ðŸ›‘ Pipeline interrompu.")
                    break
    
    # Bilan
    total_elapsed = time.time() - total_start
    print("\n" + "=" * 70)
    print("  RECONSTRUCTION TERMINÃ‰E")
    print("=" * 70)
    for step_id, name, elapsed in results:
        # Chercher le status sanity
        sanity_status = ""
        for sid, ok in sanity_results:
            if sid == step_id:
                sanity_status = " âœ…" if ok else " âŒ"
                break
        print(f"  [{step_id:>2s}] {name:50s} : {elapsed/60:.1f} min{sanity_status}")
    print(f"  {'TOTAL':55s} : {total_elapsed/60:.1f} min ({total_elapsed/3600:.1f}h)")
    print("=" * 70)
    
    # Ã‰tat final
    check_state()


if __name__ == "__main__":
    main()
