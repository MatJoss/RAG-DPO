"""
Analyse des Ressources Li√©es aux Documents Pertinents - VERSION ROBUSTE
- PDFs avec extraction multi-m√©thodes (PyPDF2, pdfplumber, PyMuPDF, OCR)
- Nettoyage robuste des caract√®res probl√©matiques
- Support LLaVA pour analyse images
- Docs (ODT/XLSX)
"""

import os
import json
from pathlib import Path
import sys
import logging
from typing import Dict, List, Set, Optional, Tuple
from tqdm import tqdm
import time
import mimetypes
from bs4 import BeautifulSoup
import hashlib
import base64

# Ajouter le chemin utils pour llm_provider
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root / 'src' / 'utils'))

from llm_provider import LLMFactory, RAGConfig

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)
logging.getLogger("httpx").setLevel(logging.WARNING)


class ResourceAnalyzer:
    """Analyse les ressources li√©es aux documents pertinents"""
    
    # Prompt pour documents (PDF, ODT, XLSX)
    DOCUMENT_PROMPT = """Tu es un expert RGPD jouant le r√¥le de DPO.

√âvalue si ce document attach√© est utile pour un DPO.

Un document attach√© EST pertinent s'il :
- Fournit des mod√®les/templates pratiques (registre, AIPD, etc.)
- D√©taille une m√©thodologie op√©rationnelle
- Contient des exemples concrets de mise en conformit√©
- Est un formulaire officiel CNIL
- Est un guide technique d√©taill√©

Un document attach√© N'EST PAS pertinent s'il :
- Est purement d√©coratif/marketing
- R√©p√®te des infos d√©j√† dans le HTML parent
- Est obsol√®te ou non applicable

R√©ponds UNIQUEMENT en JSON :
{
  "pertinent": true/false,
  "score": 0-10,
  "categorie": "essential" | "useful" | "duplicate" | "obsolete",
  "raison": "courte explication",
  "tags": ["tag1", "tag2"]
}"""

    def __init__(self, project_root: str = '.'):
        self.project_root = Path(project_root)
        self.data_path = self.project_root / 'data'
        self.html_dir = self.data_path / 'raw' / 'html'
        self.pdf_dir = self.data_path / 'raw' / 'pdf'
        self.docs_dir = self.data_path / 'raw' / 'docs'
        self.images_dir = self.data_path / 'raw' / 'images'
        self.metadata_dir = self.data_path / 'metadata'
        
        # Fichiers de r√©sultats
        self.classification_file = self.data_path / 'hybrid_classification.json'
        self.resource_index_file = self.metadata_dir / 'resource_index_by_source.json'
        self.cache_file = self.data_path / 'resource_classification_cache.json'
        self.results_file = self.data_path / 'resource_analysis.json'
        
        # Cache
        self.cache = self._load_cache()
        
        # LLM
        try:
            config = RAGConfig()
            self.llm = config.llm_provider
            self.mode = config.mode
            logger.info(f"ü§ñ LLM initialis√© en mode : {self.mode}")
        except Exception as e:
            logger.error(f"‚ùå Erreur init LLM : {e}")
            raise
        
        # V√©rifier disponibilit√© LLaVA
        self.llava_available = self._check_llava()
        
        # Stats
        self.stats = {
            'total_resources': 0,
            'pdfs_analyzed': 0,
            'docs_analyzed': 0,
            'images_analyzed': 0,
            'pdfs_kept': 0,
            'docs_kept': 0,
            'images_kept': 0,
            'cached': 0,
            'llava_used': 0,
            'heuristic_used': 0,
        }
    
    def _check_llava(self) -> bool:
        """V√©rifie si LLaVA est disponible"""
        try:
            import requests
            response = requests.get('http://localhost:11434/api/tags', timeout=5)
            if response.status_code == 200:
                models = response.json().get('models', [])
                for model in models:
                    if 'llava' in model.get('name', '').lower():
                        logger.info(f"‚úÖ LLaVA disponible : {model['name']}")
                        return True
            logger.info(f"‚ö†Ô∏è  LLaVA non disponible - utilisation heuristique pour images")
            return False
        except:
            logger.info(f"‚ö†Ô∏è  Ollama non accessible - utilisation heuristique pour images")
            return False
    
    def _load_cache(self) -> Dict:
        """Charge le cache"""
        if self.cache_file.exists():
            try:
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    cache = json.load(f)
                logger.info(f"üì¶ Cache charg√© : {len(cache)} ressources")
                return cache
            except:
                return {}
        return {}
    
    def _save_cache(self):
        """Sauvegarde le cache"""
        try:
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.cache, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logger.error(f"‚ùå Erreur sauvegarde cache : {e}")
    
    def _get_file_hash(self, url: str) -> str:
        """Hash MD5 d'une URL"""
        return hashlib.md5(url.encode()).hexdigest()[:12]
    
    def clean_extracted_text(self, text: str) -> str:
        """Nettoie le texte extrait (caract√®res probl√©matiques)"""
        if not text:
            return ""
        
        # Supprimer caract√®res null et BOM
        text = text.replace('\x00', '')
        text = text.replace('\ufeff', '')
        text = text.replace('\ufffd', '')  # Caract√®re de remplacement
        
        # Normaliser guillemets et apostrophes
        text = text.replace('"', '"').replace('"', '"')
        text = text.replace("'", "'").replace("'", "'")
        text = text.replace('¬´', '"').replace('¬ª', '"')
        
        # Supprimer caract√®res de contr√¥le (sauf \n, \r, \t)
        text = ''.join(c for c in text if c.isprintable() or c in '\n\r\t')
        
        # Normaliser espaces multiples
        text = ' '.join(text.split())
        
        return text.strip()
    
    def extract_text_from_pdf(self, file_path: Path) -> str:
        """Extrait texte d'un PDF avec fallbacks multiples"""
        
        # M√©thode 1 : PyPDF2 (rapide, standard)
        try:
            import PyPDF2
            with open(file_path, 'rb') as f:
                pdf = PyPDF2.PdfReader(f)
                if len(pdf.pages) > 0:
                    text = pdf.pages[0].extract_text()
                    text = self.clean_extracted_text(text)
                    if len(text) > 100:  # Au moins 100 chars
                        logger.debug(f"‚úÖ PyPDF2: {file_path.name}")
                        return text[:2000]
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è  PyPDF2 √©chec: {e}")
        
        # M√©thode 2 : pdfplumber (meilleur pour PDFs complexes)
        try:
            import pdfplumber
            with pdfplumber.open(file_path) as pdf:
                if len(pdf.pages) > 0:
                    text = pdf.pages[0].extract_text()
                    text = self.clean_extracted_text(text)
                    if len(text) > 100:
                        logger.debug(f"‚úÖ pdfplumber: {file_path.name}")
                        return text[:2000]
        except ImportError:
            logger.debug(f"‚ö†Ô∏è  pdfplumber non install√© (pip install pdfplumber)")
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è  pdfplumber √©chec: {e}")
        
        # M√©thode 3 : PyMuPDF/fitz (tr√®s robuste)
        try:
            import fitz  # PyMuPDF
            doc = fitz.open(file_path)
            if len(doc) > 0:
                text = doc[0].get_text()
                text = self.clean_extracted_text(text)
                if len(text) > 100:
                    logger.debug(f"‚úÖ PyMuPDF: {file_path.name}")
                    doc.close()
                    return text[:2000]
                doc.close()
        except ImportError:
            logger.debug(f"‚ö†Ô∏è  PyMuPDF non install√© (pip install pymupdf)")
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è  PyMuPDF √©chec: {e}")
        
        # M√©thode 4 : OCR avec Tesseract (dernier recours)
        try:
            import fitz
            from PIL import Image
            import pytesseract
            import io
            
            doc = fitz.open(file_path)
            if len(doc) > 0:
                # Convertir premi√®re page en image
                page = doc[0]
                pix = page.get_pixmap(dpi=150)
                img_data = pix.tobytes("png")
                img = Image.open(io.BytesIO(img_data))
                
                # OCR
                text = pytesseract.image_to_string(img, lang='fra')
                text = self.clean_extracted_text(text)
                
                if len(text) > 100:
                    logger.debug(f"‚úÖ OCR Tesseract: {file_path.name}")
                    doc.close()
                    return text[:2000]
                doc.close()
        except ImportError:
            logger.debug(f"‚ö†Ô∏è  Tesseract non install√©")
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è  OCR √©chec: {e}")
        
        # √âchec total
        logger.warning(f"‚ö†Ô∏è  Impossible d'extraire texte de {file_path.name}")
        return "[PDF - Extraction impossible]"
    
    def extract_text_from_document(self, file_path: Path) -> str:
        """Extrait texte d'un PDF/ODT/XLSX (preview)"""
        ext = file_path.suffix.lower()
        
        try:
            if ext == '.pdf':
                return self.extract_text_from_pdf(file_path)
            
            elif ext in ['.odt', '.docx']:
                if ext == '.odt':
                    from odf import text as odf_text
                    from odf.opendocument import load
                    doc = load(str(file_path))
                    paragraphs = doc.getElementsByType(odf_text.P)
                    text = '\n'.join([str(p) for p in paragraphs[:20]])
                    text = self.clean_extracted_text(text)
                    return text[:2000]
                else:  # .docx
                    from docx import Document
                    doc = Document(file_path)
                    paragraphs = [p.text for p in doc.paragraphs[:20]]
                    text = '\n'.join(paragraphs)
                    text = self.clean_extracted_text(text)
                    return text[:2000]
            
            elif ext in ['.ods', '.xlsx', '.xls']:
                # ODS (LibreOffice Calc)
                if ext == '.ods':
                    from odf import table as odf_table
                    from odf import text as odf_text
                    from odf.opendocument import load
                    doc = load(str(file_path))
                    sheets = doc.spreadsheet.getElementsByType(odf_table.Table)
                    
                    if sheets:
                        sheet = sheets[0]  # Premi√®re feuille
                        rows = sheet.getElementsByType(odf_table.TableRow)
                        text_rows = []
                        
                        for i, row in enumerate(rows[:10]):  # 10 premi√®res lignes
                            cells = row.getElementsByType(odf_table.TableCell)
                            cell_values = []
                            for cell in cells:
                                # Extraire texte de chaque cellule
                                paragraphs = cell.getElementsByType(odf_text.P)
                                cell_text = ' '.join([str(p) for p in paragraphs])
                                if cell_text.strip():
                                    cell_values.append(cell_text.strip())
                            
                            if cell_values:
                                text_rows.append(' | '.join(cell_values))
                        
                        text = '\n'.join(text_rows)
                        text = self.clean_extracted_text(text)
                        return text[:2000]
                
                # XLSX/XLS (Excel)
                else:
                    import openpyxl
                    wb = openpyxl.load_workbook(file_path, read_only=True)
                    sheet = wb.active
                    rows = []
                    for i, row in enumerate(sheet.iter_rows(values_only=True)):
                        if i >= 10:
                            break
                        row_text = ' | '.join([str(cell) for cell in row if cell])
                        rows.append(row_text)
                    text = '\n'.join(rows)
                    text = self.clean_extracted_text(text)
                    return text[:2000]
        
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è  Erreur extraction {file_path.name}: {e}")
            return f"[Fichier {ext[1:].upper()} - Extraction √©chou√©e]"
        
        return f"[Fichier {ext[1:].upper()}]"
    
    def analyze_document(self, file_path: Path, url: str, parent_url: str) -> Dict:
        """Analyse un document (PDF, ODT, XLSX)"""
        
        # Cache
        cache_key = f"{url}"
        if cache_key in self.cache:
            result = self.cache[cache_key].copy()
            result['cached'] = True
            self.stats['cached'] += 1
            return result
        
        # Extraire preview
        text_preview = self.extract_text_from_document(file_path)
        
        # Construire prompt
        user_prompt = f"""Document attach√© : {file_path.name}
Page HTML parente : {parent_url}

Aper√ßu du contenu :
{text_preview}

√âvalue la pertinence de ce document."""

        full_prompt = f"{self.DOCUMENT_PROMPT}\n\n{user_prompt}"
        
        try:
            response = self.llm.generate(full_prompt, temperature=0.1, max_tokens=300)
            
            # Parser JSON
            response_clean = response.strip()
            if response_clean.startswith('```json'):
                response_clean = response_clean[7:]
            elif response_clean.startswith('```'):
                response_clean = response_clean[3:]
            if response_clean.endswith('```'):
                response_clean = response_clean[:-3]
            response_clean = response_clean.strip()
            
            # Extraction JSON si texte autour
            if not response_clean.startswith('{'):
                start_idx = response_clean.find('{')
                if start_idx != -1:
                    response_clean = response_clean[start_idx:]
            if not response_clean.endswith('}'):
                end_idx = response_clean.rfind('}')
                if end_idx != -1:
                    response_clean = response_clean[:end_idx+1]
            
            result = json.loads(response_clean)
            result['cached'] = False
            
            logger.info(f"‚úÖ {result.get('categorie', 'N/A'):12s} ({result.get('score', 0):4.1f}/10) - {file_path.name[:40]}...")
            
            # Cache
            self.cache[cache_key] = result
            
            time.sleep(0.5 if self.mode == 'local' else 1.0)
            
            return result
        
        except json.JSONDecodeError as e:
            logger.error(f"‚ùå JSON invalide pour {file_path.name}: {e}")
            return {
                "pertinent": True,  # Garder par d√©faut
                "score": 5.0,
                "categorie": "useful",
                "raison": f"Erreur parsing: {str(e)}",
                "tags": [],
                "cached": False,
                "error": str(e)
            }
        except Exception as e:
            logger.error(f"‚ùå Erreur analyse {file_path.name}: {e}")
            return {
                "pertinent": True,
                "score": 5.0,
                "categorie": "useful",
                "raison": f"Erreur: {str(e)}",
                "tags": [],
                "cached": False,
                "error": str(e)
            }
    
    def analyze_image_with_llava(self, image_path: Path, parent_url: str) -> Optional[Dict]:
        """Analyse image avec LLaVA"""
        
        if not self.llava_available:
            return None
        
        try:
            import requests
            
            # Encoder image en base64
            with open(image_path, 'rb') as f:
                image_data = base64.b64encode(f.read()).decode('utf-8')
            
            # Prompt vision
            prompt = f"""Tu analyses une image provenant d'une page CNIL pour un DPO.

Page source : {parent_url}

D√©termine si cette image est utile pour un DPO dans l'exercice de ses missions :

UTILE si :
- Sch√©ma/diagramme technique (flux de donn√©es, architecture, processus)
- Infographie p√©dagogique sur concepts RGPD
- Capture d'√©cran montrant exemple concret d'interface/formulaire conforme
- Workflow ou m√©thodologie illustr√©e
- Exemple visuel de bonnes pratiques

NON UTILE si :
- Logo, ic√¥ne, pictogramme d√©coratif
- Photo de personne ou b√¢timent
- Bandeau publicitaire ou promotionnel
- Header, footer, √©l√©ment de navigation
- √âl√©ment purement graphique sans valeur informative

R√©ponds UNIQUEMENT en JSON :
{{
  "pertinent": true/false,
  "score": 0-10,
  "categorie": "diagram" | "infographic" | "example" | "decorative",
  "raison": "courte explication de ce que tu vois et pourquoi c'est pertinent/non pertinent",
  "tags": ["tag1", "tag2"]
}}"""

            # Appel LLaVA
            response = requests.post(
                'http://localhost:11434/api/generate',
                json={
                    'model': 'llava:7b',
                    'prompt': prompt,
                    'images': [image_data],
                    'stream': False,
                    'options': {
                        'temperature': 0.1,
                        'num_predict': 300
                    }
                },
                timeout=30
            )
            
            if response.status_code != 200:
                logger.debug(f"‚ö†Ô∏è  LLaVA HTTP error {response.status_code}")
                return None
            
            data = response.json()
            response_text = data.get('response', '')
            
            logger.debug(f"üì• LLaVA r√©ponse brute (100 premiers chars) : {response_text[:100]}")
            
            # Parser JSON
            response_clean = response_text.strip()
            if response_clean.startswith('```json'):
                response_clean = response_clean[7:]
            elif response_clean.startswith('```'):
                response_clean = response_clean[3:]
            if response_clean.endswith('```'):
                response_clean = response_clean[:-3]
            response_clean = response_clean.strip()
            
            # Extraction JSON
            if not response_clean.startswith('{'):
                start_idx = response_clean.find('{')
                if start_idx != -1:
                    response_clean = response_clean[start_idx:]
                    logger.debug(f"‚úÇÔ∏è  JSON extrait √† partir de position {start_idx}")
            if not response_clean.endswith('}'):
                end_idx = response_clean.rfind('}')
                if end_idx != -1:
                    response_clean = response_clean[:end_idx+1]
            
            logger.debug(f"üßπ JSON nettoy√© (100 premiers chars) : {response_clean[:100]}")
            
            result = json.loads(response_clean)
            result['method'] = 'llava'
            result['cached'] = False
            self.stats['llava_used'] += 1
            
            logger.debug(f"‚úÖ LLaVA parsing r√©ussi: {result.get('categorie')} - {result.get('score')}/10")
            
            time.sleep(1.0)  # Rate limiting pour vision
            
            return result
        
        except json.JSONDecodeError as e:
            logger.debug(f"‚ö†Ô∏è  LLaVA JSON parsing √©chec: {e}")
            logger.debug(f"   R√©ponse brute: {response_text[:200] if 'response_text' in locals() else 'N/A'}")
            return None
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è  LLaVA √©chec: {e}")
            return None
    
    def analyze_image_heuristic(self, image_path: Path) -> Dict:
        """Analyse heuristique d'image (taille + nom fichier)"""
        
        filename = image_path.name.lower()
        
        # Patterns pertinents
        if any(kw in filename for kw in ['schema', 'diagram', 'diagramme', 'process', 'workflow', 'architecture', 'flux', 'infographic', 'infographie']):
            result = {
                "pertinent": True,
                "score": 7.0,
                "categorie": "diagram",
                "raison": "Nom de fichier indique diagramme/sch√©ma",
                "tags": ["diagram", "heuristic"],
                "method": "heuristic"
            }
        
        # Patterns non pertinents
        elif any(kw in filename for kw in ['icon', 'logo', 'bandeau', 'header', 'footer', 'picto', 'avatar', 'portrait', 'thumb']):
            result = {
                "pertinent": False,
                "score": 1.0,
                "categorie": "decorative",
                "raison": "Nom de fichier indique √©l√©ment d√©coratif",
                "tags": ["decorative", "heuristic"],
                "method": "heuristic"
            }
        
        # V√©rifier taille image
        else:
            try:
                from PIL import Image
                with Image.open(image_path) as img:
                    width, height = img.size
                    
                    if width < 100 or height < 100:
                        result = {
                            "pertinent": False,
                            "score": 2.0,
                            "categorie": "decorative",
                            "raison": f"Petite image ({width}x{height}), probablement d√©corative",
                            "tags": ["small", "heuristic"],
                            "method": "heuristic"
                        }
                    else:
                        result = {
                            "pertinent": True,
                            "score": 5.0,
                            "categorie": "example",
                            "raison": f"Image de taille significative ({width}x{height})",
                            "tags": ["medium-large", "heuristic"],
                            "method": "heuristic"
                        }
            except:
                result = {
                    "pertinent": True,
                    "score": 5.0,
                    "categorie": "example",
                    "raison": "Image potentiellement utile",
                    "tags": ["uncertain", "heuristic"],
                    "method": "heuristic"
                }
        
        self.stats['heuristic_used'] += 1
        return result
    
    def analyze_image(self, image_path: Path, url: str, parent_url: str) -> Dict:
        """Analyse une image (LLaVA ou heuristique)"""
        
        # Cache
        cache_key = f"{url}"
        if cache_key in self.cache:
            result = self.cache[cache_key].copy()
            result['cached'] = True
            self.stats['cached'] += 1
            logger.debug(f"üíæ Cache hit pour image: {image_path.name}")
            return result
        
        # Essayer LLaVA d'abord si disponible
        result = None
        if self.llava_available:
            logger.debug(f"ü§ñ Tentative analyse LLaVA: {image_path.name}")
            result = self.analyze_image_with_llava(image_path, parent_url)
        
        # Fallback heuristique si LLaVA indisponible ou √©chec
        if result is None:
            if self.llava_available:
                logger.debug(f"‚ö†Ô∏è  LLaVA √©chec, fallback heuristique: {image_path.name}")
            else:
                logger.debug(f"‚ÑπÔ∏è  LLaVA indisponible, analyse heuristique: {image_path.name}")
            result = self.analyze_image_heuristic(image_path)
        
        result['cached'] = False
        
        method = result.get('method', 'unknown')
        logger.info(f"‚úÖ {result['categorie']:12s} ({result['score']:4.1f}/10) - {image_path.name[:35]}... [{method}]")
        
        # Cache
        self.cache[cache_key] = result
        
        return result
    
    def load_kept_documents(self) -> Set[str]:
        """Charge les URLs HTML gard√©es"""
        if not self.classification_file.exists():
            raise FileNotFoundError("Lancez d'abord hybrid_filter.py")
        
        with open(self.classification_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        kept_urls = set()
        for hash_id, info in data.get('llm_classified', {}).items():
            if info.get('pertinent', False):
                kept_urls.add(info['url'])
        
        logger.info(f"üìÑ {len(kept_urls)} documents HTML pertinents")
        return kept_urls
    
    def load_resource_index(self) -> Dict:
        """Charge l'index des ressources"""
        if not self.resource_index_file.exists():
            logger.warning(f"‚ö†Ô∏è  Index ressources introuvable")
            return {}
        
        with open(self.resource_index_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def get_relevant_images_from_html(self, html_file: Path, url: str) -> List[str]:
        """Extrait images r√©f√©renc√©es (hors d√©co)"""
        try:
            with open(html_file, 'r', encoding='utf-8', errors='ignore') as f:
                soup = BeautifulSoup(f, 'lxml')
            
            # Supprimer header/footer/nav
            for tag in soup(['header', 'footer', 'nav', 'aside', 'script', 'style']):
                tag.decompose()
            
            main_content = soup.find(['main', 'article', 'div']) or soup
            
            relevant_images = []
            
            for img in main_content.find_all('img', src=True):
                src = img['src']
                alt = img.get('alt', '').lower()
                
                # Filtrer ic√¥nes
                if any(skip in src.lower() for skip in ['icon', 'logo', 'picto', 'bullet', 'arrow']):
                    continue
                
                if any(skip in alt for skip in ['logo', 'ic√¥ne', 'pictogramme']):
                    continue
                
                # Filtrer petites images
                width = img.get('width', '')
                height = img.get('height', '')
                if width and height:
                    try:
                        if int(width) < 50 or int(height) < 50:
                            continue
                    except:
                        pass
                
                # Images dans figure ou avec l√©gende = pertinentes
                if img.find_parent('figure') or alt:
                    relevant_images.append(src)
            
            return relevant_images
        
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Erreur extraction images: {e}")
            return []
    
    def run(self, max_resources: Optional[int] = None):
        """Ex√©cute l'analyse compl√®te"""
        
        print("=" * 70)
        print("üìé ANALYSE DES RESSOURCES LI√âES - VERSION ROBUSTE")
        print("=" * 70)
        
        if max_resources:
            print(f"üß™ MODE TEST : {max_resources} ressources maximum")
        
        # Charger documents pertinents
        print("\nüîç Chargement des documents HTML pertinents...")
        kept_urls = self.load_kept_documents()
        
        # Charger index ressources
        print("üîç Chargement de l'index des ressources...")
        resource_index = self.load_resource_index()
        
        # Collecter ressources
        print("\nüìã Collecte des ressources li√©es...")
        
        resources_to_analyze = {
            'pdfs': {},
            'docs': {},
            'images': {}
        }
        
        for url in tqdm(kept_urls, desc="Collecte"):
            # Ressources depuis l'index
            if url in resource_index:
                for resource in resource_index[url].get('resources', []):
                    res_url = resource['url']
                    res_type = resource['file_type']
                    res_path = self.project_root / resource['file_path']
                    
                    if not res_path.exists():
                        continue
                    
                    if res_type == 'pdf' and os.path.getsize(res_path) >= 45153 :
                        resources_to_analyze['pdfs'][res_url] = {
                            'path': res_path,
                            'parent_url': url
                        }
                    elif res_type in ['odt', 'ods', 'docx', 'xlsx', 'xls']:
                        resources_to_analyze['docs'][res_url] = {
                            'path': res_path,
                            'parent_url': url
                        }
            
            # Images r√©f√©renc√©es
            html_hash = self._get_file_hash(url)
            html_file = self.html_dir / f"{html_hash}.html"
            
            if html_file.exists():
                relevant_images = self.get_relevant_images_from_html(html_file, url)
                for img_url in relevant_images:
                    img_hash = self._get_file_hash(img_url)
                    
                    for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']:
                        img_path = self.images_dir / f"{img_hash}{ext}"
                        if img_path.exists() and os.path.getsize(img_path) >= 437742:
                            resources_to_analyze['images'][img_url] = {
                                'path': img_path,
                                'parent_url': url
                            }
                            break
        
        # Limiter si mode test
        if max_resources:
            total_collected = (
                len(resources_to_analyze['pdfs']) +
                len(resources_to_analyze['docs']) +
                len(resources_to_analyze['images'])
            )
            
            if total_collected > max_resources:
                # R√©partir √©quitablement
                ratio_pdf = len(resources_to_analyze['pdfs']) / max(1, total_collected)
                ratio_doc = len(resources_to_analyze['docs']) / max(1, total_collected)
                ratio_img = len(resources_to_analyze['images']) / max(1, total_collected)
                
                max_pdf = int(max_resources * ratio_pdf)
                max_doc = int(max_resources * ratio_doc)
                max_img = max_resources - max_pdf - max_doc
                
                resources_to_analyze['pdfs'] = dict(list(resources_to_analyze['pdfs'].items())[:max_pdf])
                resources_to_analyze['docs'] = dict(list(resources_to_analyze['docs'].items())[:max_doc])
                resources_to_analyze['images'] = dict(list(resources_to_analyze['images'].items())[:max_img])
        
        self.stats['total_resources'] = (
            len(resources_to_analyze['pdfs']) +
            len(resources_to_analyze['docs']) +
            len(resources_to_analyze['images'])
        )
        
        print(f"\nüìä Ressources √† analyser :")
        print(f"   PDFs       : {len(resources_to_analyze['pdfs'])}")
        print(f"   Documents  : {len(resources_to_analyze['docs'])}")
        print(f"   Images     : {len(resources_to_analyze['images'])}")
        print(f"   TOTAL      : {self.stats['total_resources']}")
        
        if self.stats['total_resources'] == 0:
            print("\n‚úÖ Aucune ressource √† analyser.")
            return
        
        # Estimation
        est_min = self.stats['total_resources'] * 2 / 60
        print(f"\n‚è±Ô∏è  Dur√©e estim√©e : ~{est_min:.0f} minutes ({est_min/60:.1f}h)")
        
        if not max_resources:  # Seulement demander confirmation si pas en mode test
            input("\n   Appuyez sur Entr√©e pour continuer...")
        
        # Analyse
        results = {
            'pdfs': {},
            'docs': {},
            'images': {}
        }
        
        save_counter = 0
        
        # PDFs
        if resources_to_analyze['pdfs']:
            print(f"\nüìÑ Analyse des PDFs...")
            for url, info in tqdm(resources_to_analyze['pdfs'].items(), desc="PDFs"):
                analysis = self.analyze_document(info['path'], url, info['parent_url'])
                results['pdfs'][url] = {
                    'file_path': str(info['path'].relative_to(self.project_root)),
                    'parent_url': info['parent_url'],
                    'analysis': analysis
                }
                if analysis.get('pertinent'):
                    self.stats['pdfs_kept'] += 1
                self.stats['pdfs_analyzed'] += 1
                
                save_counter += 1
                if save_counter % 10 == 0:
                    self._save_cache()
        
        # Documents
        if resources_to_analyze['docs']:
            print(f"\nüìù Analyse des documents...")
            for url, info in tqdm(resources_to_analyze['docs'].items(), desc="Docs"):
                analysis = self.analyze_document(info['path'], url, info['parent_url'])
                results['docs'][url] = {
                    'file_path': str(info['path'].relative_to(self.project_root)),
                    'parent_url': info['parent_url'],
                    'analysis': analysis
                }
                if analysis.get('pertinent'):
                    self.stats['docs_kept'] += 1
                self.stats['docs_analyzed'] += 1
                
                save_counter += 1
                if save_counter % 10 == 0:
                    self._save_cache()
        
        # Images
        if resources_to_analyze['images']:
            print(f"\nüñºÔ∏è  Analyse des images...")
            for url, info in tqdm(resources_to_analyze['images'].items(), desc="Images"):
                analysis = self.analyze_image(info['path'], url, info['parent_url'])
                results['images'][url] = {
                    'file_path': str(info['path'].relative_to(self.project_root)),
                    'parent_url': info['parent_url'],
                    'analysis': analysis
                }
                if analysis.get('pertinent'):
                    self.stats['images_kept'] += 1
                self.stats['images_analyzed'] += 1
        
        # Sauvegarde finale
        self._save_cache()
        self._save_results(results)
        
        # R√©sum√©
        self._print_summary()
    
    def _save_results(self, results: Dict):
        """Sauvegarde les r√©sultats"""
        output = {
            'pdfs': results['pdfs'],
            'docs': results['docs'],
            'images': results['images'],
            'stats': self.stats,
            'timestamp': time.strftime('%Y-%m-%dT%H:%M:%S')
        }
        
        with open(self.results_file, 'w', encoding='utf-8') as f:
            json.dump(output, f, indent=2, ensure_ascii=False)
        
        logger.info(f"üíæ R√©sultats : {self.results_file}")
    
    def _print_summary(self):
        """Affiche le r√©sum√©"""
        print("\n" + "=" * 70)
        print("üìä R√âSUM√â - ANALYSE DES RESSOURCES")
        print("=" * 70)
        
        print(f"\nüìÑ PDFs :")
        print(f"   Analys√©s : {self.stats['pdfs_analyzed']}")
        print(f"   Gard√©s   : {self.stats['pdfs_kept']} ({self.stats['pdfs_kept']/max(1,self.stats['pdfs_analyzed'])*100:.1f}%)")
        
        print(f"\nüìù Documents :")
        print(f"   Analys√©s : {self.stats['docs_analyzed']}")
        print(f"   Gard√©s   : {self.stats['docs_kept']} ({self.stats['docs_kept']/max(1,self.stats['docs_analyzed'])*100:.1f}%)")
        
        print(f"\nüñºÔ∏è  Images :")
        print(f"   Analys√©es : {self.stats['images_analyzed']}")
        print(f"   Gard√©es   : {self.stats['images_kept']} ({self.stats['images_kept']/max(1,self.stats['images_analyzed'])*100:.1f}%)")
        print(f"   LLaVA     : {self.stats['llava_used']}")
        print(f"   Heuristique: {self.stats['heuristic_used']}")
        
        print(f"\nüíæ Cache : {self.stats['cached']}/{self.stats['total_resources']} ({self.stats['cached']/max(1,self.stats['total_resources'])*100:.1f}%)")
        
        print("\n" + "=" * 70)
        print(f"üíæ R√©sultats : {self.results_file}")
        print(f"üíæ Cache : {self.cache_file}")
        print("=" * 70)


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='Analyse ressources li√©es - Version robuste')
    parser.add_argument('--project-root', type=str, default='.', help='Racine du projet')
    parser.add_argument('--test', type=int, help='Tester sur N ressources (ex: --test 10)')
    parser.add_argument('--verbose', '-v', action='store_true', help='Mode verbose')
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger("__main__").setLevel(logging.DEBUG)
    
    analyzer = ResourceAnalyzer(args.project_root)
    analyzer.run(max_resources=args.test)


if __name__ == "__main__":
    main()