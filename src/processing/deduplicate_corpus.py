"""
ðŸ”„ Phase 4C : DÃ©duplication du corpus
Ã‰limine les documents dont le contenu est identique Ã  un autre.

StratÃ©gie :
  - HTML  : hash MD5 du texte region-content (contenu utile)
  - PDF   : hash MD5 du fichier binaire
  - Docs  : hash MD5 du fichier binaire
  - Images: hash MD5 du fichier binaire

Pour chaque groupe de doublons, on garde 1 "canonical" et on retire les autres.
CritÃ¨re canonical : URL la plus courte en https, sinon la premiÃ¨re.

Le manifest est mis Ã  jour (doublons retirÃ©s).
Les fichiers doublons sont dÃ©placÃ©s dans keep/dedup_archive/ (pas supprimÃ©s).
Un rapport JSON est sauvegardÃ© pour traÃ§abilitÃ©.

Usage :
  python src/processing/deduplicate_corpus.py              # ExÃ©cuter
  python src/processing/deduplicate_corpus.py --dry-run    # Simuler
  python src/processing/deduplicate_corpus.py --fresh      # Ignorer cache
"""

import json
import hashlib
import shutil
import logging
import argparse
import time
from pathlib import Path
from collections import defaultdict
from typing import Dict, List, Tuple, Optional

from bs4 import BeautifulSoup

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')

PROJECT_ROOT = Path(__file__).parent.parent.parent
CNIL_PATH = PROJECT_ROOT / "data" / "raw" / "cnil"
KEEP_PATH = PROJECT_ROOT / "data" / "keep" / "cnil"


class CorpusDeduplicator:
    """DÃ©duplique le corpus en Ã©liminant les contenus identiques."""

    def __init__(self):
        self.manifest_file = CNIL_PATH / "keep_manifest.json"
        self.report_file = CNIL_PATH / "dedup_report.json"
        self.archive_dir = KEEP_PATH / "dedup_archive"

        self.stats = {
            'html_before': 0, 'html_after': 0, 'html_removed': 0,
            'pdf_before': 0, 'pdf_after': 0, 'pdf_removed': 0,
            'docs_before': 0, 'docs_after': 0, 'docs_removed': 0,
            'images_before': 0, 'images_after': 0, 'images_removed': 0,
            'relationships_cleaned': 0,
        }
        self.report = {
            'timestamp': '',
            'groups': [],   # groupes de doublons dÃ©tectÃ©s
            'removed': [],  # fichiers retirÃ©s
            'stats': {},
        }

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Hashing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _hash_html_content(self, file_path: Path) -> Optional[str]:
        """Hash le texte region-content d'un HTML (contenu utile uniquement)."""
        try:
            html = file_path.read_text(encoding='utf-8', errors='ignore')
            soup = BeautifulSoup(html, 'html.parser')
            region = soup.find(class_='region-content')
            if region:
                text = region.get_text(separator=' ', strip=True)
                if len(text) >= 20:
                    return hashlib.md5(text.encode('utf-8')).hexdigest()
            # Fallback : hash du body entier si pas de region-content
            body = soup.find('body')
            if body:
                text = body.get_text(separator=' ', strip=True)
                if len(text) >= 20:
                    return hashlib.md5(text.encode('utf-8')).hexdigest()
            return None
        except Exception as e:
            logger.warning(f"Erreur hash HTML {file_path.name}: {e}")
            return None

    def _hash_binary(self, file_path: Path) -> Optional[str]:
        """Hash MD5 d'un fichier binaire."""
        try:
            return hashlib.md5(file_path.read_bytes()).hexdigest()
        except Exception as e:
            logger.warning(f"Erreur hash binaire {file_path.name}: {e}")
            return None

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Canonical selection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _pick_canonical(self, items: List[Dict]) -> Tuple[Dict, List[Dict]]:
        """Choisit le document canonical dans un groupe de doublons.
        
        CritÃ¨res (par ordre) :
        1. URL https > http
        2. URL la plus courte (souvent la plus propre)
        3. Premier dans la liste (stable)
        """
        def score(item):
            url = item.get('url', '')
            is_https = 1 if url.startswith('https://') else 0
            # PÃ©naliser les URLs avec ?page=, les tags en doublon, etc.
            has_params = 1 if '?' in url else 0
            return (-is_https, has_params, len(url))

        sorted_items = sorted(items, key=score)
        canonical = sorted_items[0]
        duplicates = sorted_items[1:]
        return canonical, duplicates

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ DÃ©dup par type â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _dedup_list(self, items: List[Dict], item_type: str,
                    hash_fn) -> Tuple[List[Dict], List[Dict]]:
        """DÃ©duplique une liste de documents du manifest.
        
        Returns:
            (kept, removed) â€” listes de documents gardÃ©s et retirÃ©s
        """
        # Grouper par hash contenu
        groups = defaultdict(list)
        no_hash = []

        for item in items:
            fp = PROJECT_ROOT / item.get('metadata', {}).get('file_path', '')
            if fp.exists():
                h = hash_fn(fp)
                if h:
                    groups[h].append(item)
                else:
                    no_hash.append(item)
            else:
                no_hash.append(item)

        kept = []
        removed = []

        for h, group_items in groups.items():
            canonical, duplicates = self._pick_canonical(group_items)
            kept.append(canonical)

            if duplicates:
                # Enregistrer le groupe pour le rapport
                self.report['groups'].append({
                    'type': item_type,
                    'hash': h,
                    'count': len(group_items),
                    'canonical': {
                        'file': canonical.get('file', ''),
                        'url': canonical.get('url', ''),
                    },
                    'duplicates': [
                        {'file': d.get('file', ''), 'url': d.get('url', '')}
                        for d in duplicates
                    ],
                })

                for d in duplicates:
                    removed.append(d)
                    self.report['removed'].append({
                        'type': item_type,
                        'file': d.get('file', ''),
                        'url': d.get('url', ''),
                        'canonical_url': canonical.get('url', ''),
                    })

        # Les docs sans hash sont gardÃ©s (prudence)
        kept.extend(no_hash)

        return kept, removed

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Nettoyage relationships â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _clean_relationships(self, relationships: Dict,
                             kept_html_urls: set) -> Dict:
        """Nettoie les relationships pour ne garder que les HTML canonical."""
        cleaned = {}
        removed_count = 0
        for url, resources in relationships.items():
            if url in kept_html_urls:
                cleaned[url] = resources
            else:
                removed_count += 1
        self.stats['relationships_cleaned'] = removed_count
        return cleaned

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Archivage fichiers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _archive_removed_files(self, removed: List[Dict], dry_run: bool):
        """DÃ©place les fichiers doublons dans dedup_archive/."""
        if dry_run:
            return

        self.archive_dir.mkdir(parents=True, exist_ok=True)

        for item in removed:
            rel_file = item.get('file', '')
            if not rel_file:
                continue

            src = KEEP_PATH / rel_file
            if src.exists():
                # Garder la structure de sous-dossiers
                dest_dir = self.archive_dir / Path(rel_file).parent
                dest_dir.mkdir(parents=True, exist_ok=True)
                dest = dest_dir / src.name

                try:
                    shutil.move(str(src), str(dest))
                except Exception as e:
                    logger.warning(f"Impossible de dÃ©placer {src.name}: {e}")

            # Aussi dÃ©placer le metadata associÃ©
            stem = Path(rel_file).stem
            meta_src = KEEP_PATH / 'metadata' / f"{stem}.json"
            if meta_src.exists():
                meta_dest_dir = self.archive_dir / 'metadata'
                meta_dest_dir.mkdir(parents=True, exist_ok=True)
                try:
                    shutil.move(str(meta_src), str(meta_dest_dir / meta_src.name))
                except Exception:
                    pass

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Run principal â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def run(self, dry_run: bool = False, fresh: bool = False):
        """ExÃ©cute la dÃ©duplication complÃ¨te."""
        start = time.time()

        print("=" * 70)
        print("ðŸ§¹ PHASE 4C : DÃ‰DUPLICATION DU CORPUS")
        print("=" * 70)

        if dry_run:
            print("   MODE SIMULATION â€” aucune modification")

        # Charger manifest
        if not self.manifest_file.exists():
            print("\nâŒ keep_manifest.json introuvable")
            return

        with open(self.manifest_file, 'r', encoding='utf-8') as f:
            manifest = json.load(f)

        # Rapport existant ?
        if not fresh and self.report_file.exists():
            print("\nâš ï¸  dedup_report.json existe dÃ©jÃ .")
            print("   Utilisez --fresh pour re-dÃ©dupliquer (depuis le backup manifest).")
            # Afficher stats du rapport existant
            with open(self.report_file, 'r', encoding='utf-8') as f:
                old_report = json.load(f)
            old_stats = old_report.get('stats', {})
            for k, v in old_stats.items():
                print(f"   {k}: {v}")
            return

        # En mode --fresh, restaurer le backup du manifest si disponible
        backup = self.manifest_file.with_suffix('.json.pre_dedup')
        if fresh and backup.exists():
            shutil.copy2(str(backup), str(self.manifest_file))
            print(f"\nâ™»ï¸  Manifest restaurÃ© depuis backup ({backup.name})")
            with open(self.manifest_file, 'r', encoding='utf-8') as f:
                manifest = json.load(f)

        # â”€â”€ Stats avant â”€â”€
        self.stats['html_before'] = len(manifest.get('html', []))
        self.stats['pdf_before'] = len(manifest.get('pdfs', []))
        self.stats['docs_before'] = len(manifest.get('docs', []))
        self.stats['images_before'] = len(manifest.get('images', []))
        total_before = sum(self.stats[f'{t}_before'] for t in ['html', 'pdf', 'docs', 'images'])

        print(f"\nðŸ“Š Corpus avant dÃ©duplication :")
        print(f"   HTML   : {self.stats['html_before']}")
        print(f"   PDFs   : {self.stats['pdf_before']}")
        print(f"   Docs   : {self.stats['docs_before']}")
        print(f"   Images : {self.stats['images_before']}")
        print(f"   TOTAL  : {total_before}")

        # â”€â”€ DÃ©dup HTML (par region-content) â”€â”€
        print("\nðŸ“„ DÃ©duplication HTML (hash region-content)...")
        html_kept, html_removed = self._dedup_list(
            manifest.get('html', []), 'html', self._hash_html_content
        )
        self.stats['html_after'] = len(html_kept)
        self.stats['html_removed'] = len(html_removed)
        print(f"   {self.stats['html_before']} â†’ {self.stats['html_after']} "
              f"(-{self.stats['html_removed']})")

        # â”€â”€ DÃ©dup PDFs (hash binaire) â”€â”€
        print("\nðŸ“‘ DÃ©duplication PDFs (hash binaire)...")
        pdf_kept, pdf_removed = self._dedup_list(
            manifest.get('pdfs', []), 'pdf', self._hash_binary
        )
        self.stats['pdf_after'] = len(pdf_kept)
        self.stats['pdf_removed'] = len(pdf_removed)
        print(f"   {self.stats['pdf_before']} â†’ {self.stats['pdf_after']} "
              f"(-{self.stats['pdf_removed']})")

        # â”€â”€ DÃ©dup Docs (hash binaire) â”€â”€
        print("\nðŸ“ DÃ©duplication Docs (hash binaire)...")
        docs_kept, docs_removed = self._dedup_list(
            manifest.get('docs', []), 'doc', self._hash_binary
        )
        self.stats['docs_after'] = len(docs_kept)
        self.stats['docs_removed'] = len(docs_removed)
        print(f"   {self.stats['docs_before']} â†’ {self.stats['docs_after']} "
              f"(-{self.stats['docs_removed']})")

        # â”€â”€ DÃ©dup Images (hash binaire) â”€â”€
        print("\nðŸ–¼ï¸  DÃ©duplication Images (hash binaire)...")
        images_kept, images_removed = self._dedup_list(
            manifest.get('images', []), 'image', self._hash_binary
        )
        self.stats['images_after'] = len(images_kept)
        self.stats['images_removed'] = len(images_removed)
        print(f"   {self.stats['images_before']} â†’ {self.stats['images_after']} "
              f"(-{self.stats['images_removed']})")

        # â”€â”€ Nettoyer relationships â”€â”€
        kept_html_urls = {item['url'] for item in html_kept}
        relationships = self._clean_relationships(
            manifest.get('relationships', {}), kept_html_urls
        )

        # â”€â”€ Bilan â”€â”€
        total_after = (self.stats['html_after'] + self.stats['pdf_after'] +
                       self.stats['docs_after'] + self.stats['images_after'])
        total_removed = total_before - total_after

        print("\n" + "=" * 70)
        print("ðŸ“Š BILAN DÃ‰DUPLICATION")
        print("=" * 70)
        print(f"""
           AVANT      APRÃˆS      RETIRÃ‰S
  HTML   : {self.stats['html_before']:>5}      {self.stats['html_after']:>5}      -{self.stats['html_removed']}
  PDF    : {self.stats['pdf_before']:>5}      {self.stats['pdf_after']:>5}      -{self.stats['pdf_removed']}
  Docs   : {self.stats['docs_before']:>5}      {self.stats['docs_after']:>5}      -{self.stats['docs_removed']}
  Images : {self.stats['images_before']:>5}      {self.stats['images_after']:>5}      -{self.stats['images_removed']}
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  TOTAL  : {total_before:>5}      {total_after:>5}      -{total_removed} ({total_removed/max(1,total_before)*100:.0f}%)
  Relations nettoyÃ©es : {self.stats['relationships_cleaned']}
""")

        if dry_run:
            print("   MODE SIMULATION â€” rien n'a Ã©tÃ© modifiÃ©")
            return

        # â”€â”€ Sauvegarder â”€â”€

        # 1. Archiver fichiers doublons
        all_removed = html_removed + pdf_removed + docs_removed + images_removed
        print(f"ðŸ“¦ Archivage de {len(all_removed)} fichiers dans keep/dedup_archive/...")
        self._archive_removed_files(all_removed, dry_run)

        # 2. Mettre Ã  jour manifest
        new_manifest = {
            'html': html_kept,
            'pdfs': pdf_kept,
            'docs': docs_kept,
            'images': images_kept,
            'relationships': relationships,
        }

        # Backup du manifest original (seulement si pas dÃ©jÃ  existant)
        backup = self.manifest_file.with_suffix('.json.pre_dedup')
        if not backup.exists():
            shutil.copy2(str(self.manifest_file), str(backup))
            print(f"ðŸ’¾ Backup manifest : {backup.name}")
        else:
            print(f"ðŸ’¾ Backup manifest existant conservÃ© : {backup.name}")

        with open(self.manifest_file, 'w', encoding='utf-8') as f:
            json.dump(new_manifest, f, indent=2, ensure_ascii=False)
        print(f"ðŸ’¾ Manifest mis Ã  jour : {self.manifest_file.name}")

        # 3. Sauvegarder rapport
        self.report['timestamp'] = time.strftime('%Y-%m-%d %H:%M:%S')
        self.report['stats'] = self.stats
        with open(self.report_file, 'w', encoding='utf-8') as f:
            json.dump(self.report, f, indent=2, ensure_ascii=False)
        print(f"ðŸ’¾ Rapport sauvÃ© : {self.report_file.name}")

        elapsed = time.time() - start
        print(f"\nâœ… DÃ©duplication terminÃ©e en {elapsed:.1f}s")
        print(f"   Le corpus est prÃªt pour Phase 5A â†’ 6B")


def main():
    parser = argparse.ArgumentParser(description='Phase 4C : DÃ©duplication corpus')
    parser.add_argument('--dry-run', action='store_true',
                        help='Simuler sans modifier les fichiers')
    parser.add_argument('--fresh', action='store_true',
                        help='Ignorer le rapport existant, re-dÃ©dupliquer')
    args = parser.parse_args()

    dedup = CorpusDeduplicator()
    dedup.run(dry_run=args.dry_run, fresh=args.fresh)


if __name__ == "__main__":
    main()
