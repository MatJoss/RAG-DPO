"""
Classification Hybride Optimis√©e : Keywords + LLM
Compatible avec cnil_scraper_final.py
"""

import json
from pathlib import Path
import re
import logging
import signal
import sys
from typing import Dict, List, Optional, Tuple
from bs4 import BeautifulSoup
from tqdm import tqdm
import time

# Ajouter le chemin utils pour llm_provider
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root / 'src' / 'utils'))

from llm_provider import LLMFactory, RAGConfig

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)

# R√©duire verbosit√© de httpx
logging.getLogger("httpx").setLevel(logging.WARNING)


class HybridClassifier:
    """Classification hybride keywords + LLM optimis√©e"""
    
    # Patterns d'exclusion √âVIDENTS (pas besoin de LLM)
    OBVIOUS_EXCLUDE_PATTERNS = [
        r'/emploi',
        r'/recrutement',
        r'/presse',
        r'/communique',
        r'/contact',
        r'/mentions-legales',
        r'/plan-du-site',
        r'/accessibilite',
        r'/newsletter-inscription',
    ]
    
    # Mots-cl√©s d'exclusion forte
    STRONG_EXCLUDE_KEYWORDS = [
        'exercer vos droits',
        'porter plainte en ligne',
        'vos droits en tant que citoyen',
        'plainte particulier',
    ]
    
    # Mots-cl√©s par cat√©gorie avec poids (pour pr√©-filtrage)
    KEYWORDS = {
        'dpo_core': {
            'poids': 3,
            'mots': [
                'dpo', 'd√©l√©gu√© protection donn√©es', 'd√©l√©gu√© √† la protection',
                'responsable traitement', 'sous-traitant', 'aipd', 'pia',
                'registre traitement', 'analyse impact', 'violation donn√©es',
            ]
        },
        'rgpd_pro': {
            'poids': 2,
            'mots': [
                'rgpd', 'gdpr', 'protection donn√©es', 'donn√©es personnelles',
                'traitement donn√©es', 'base l√©gale', 'consentement',
                'entreprise', 'organisme', 'professionnel',
            ]
        },
        'particulier': {
            'poids': -2,
            'mots': [
                'citoyen', 'consommateur', 'usager', 'internaute',
                'particulier', 'plainte en ligne',
            ]
        },
        'institutionnel': {
            'poids': -1,
            'mots': [
                'qui sommes-nous', 'recrutement', 'offre emploi',
                'communiqu√© presse', 'contact cnil',
            ]
        }
    }
    
    # Prompt LLM optimis√©
    SYSTEM_PROMPT = """Tu es un DPO senior (15 ans d'exp√©rience) √©valuant des documents pour constituer ta base de connaissances professionnelle.

Ta mission : d√©terminer si ce document t'est UTILE dans l'exercice QUOTIDIEN de tes fonctions de DPO.

Tu cherches des documents qui t'aident concr√®tement √† :
1. PILOTER la conformit√© (registre, AIPD/PIA, bases l√©gales, dur√©es de conservation)
2. G√âRER les incidents (violations de donn√©es, notifications, proc√©dures de crise)
3. CONSEILLER l'organisation (avis sur traitements, privacy by design, sous-traitance)
4. FORMER et sensibiliser (supports p√©dagogiques, bonnes pratiques)
5. R√âPONDRE aux contr√¥les (pr√©paration, documentation, jurisprudence CNIL)
6. APPLIQUER les r√©f√©rentiels sectoriels (sant√©, RH, marketing, vid√©osurveillance)

Sont PERTINENTS (score >= 6) :
- Guides pratiques, m√©thodologies, checklists, mod√®les de documents
- D√©lib√©rations CNIL (sanctions, mises en demeure, avertissements)
- Lignes directrices, recommandations, r√©f√©rentiels
- FAQ et analyses juridiques sur le RGPD, la loi Informatique et Libert√©s
- Fiches th√©matiques (cookies, vid√©osurveillance, donn√©es RH, sous-traitance...)
- Mod√®les de registre, clauses contractuelles, mentions d'information
- Tout document technique avec impact conformit√© (s√©curit√©, pseudonymisation...)

Ne sont PAS pertinents (score < 4) :
- Pages destin√©es AUX PARTICULIERS (exercice des droits, plainte en ligne)
- Communication institutionnelle (presse, recrutement, organigramme CNIL)
- Pages de navigation, index, listes de liens sans contenu propre
- Actualit√©s purement √©v√©nementielles sans valeur op√©rationnelle durable
- Contenus sans rapport avec la protection des donn√©es

R√®gle d'or : EN CAS DE DOUTE, GARDE LE DOCUMENT (score 5-6). Il vaut mieux un document en trop qu'un document utile manquant.

R√©ponds UNIQUEMENT au format JSON suivant :
{
  "pertinent": true/false,
  "score": 0-10,
  "categorie": "essential" | "relevant" | "useful" | "neutral" | "irrelevant",
  "raison": "explication courte (max 100 mots)",
  "tags": ["tag1", "tag2", "tag3"]
}

Cat√©gories :
- essential (8-10) : indispensable pour un DPO
- relevant (6-7.9) : clairement pertinent
- useful (4-5.9) : potentiellement utile
- neutral (2-3.9) : information g√©n√©rale sans valeur op√©rationnelle
- irrelevant (0-1.9) : hors sujet pour un DPO
"""
    
    def __init__(self, project_root: str = '.'):
        self.project_root = Path(project_root)
        self.html_dir = self.project_root / 'data' / 'raw' / 'cnil' / 'html'
        self.metadata_dir = self.project_root / 'data' / 'metadata'
        self.cache_file = self.project_root / 'data' / 'raw' / 'cnil' / 'llm_classification_cache.json'
        self.results_file = self.project_root / 'data' / 'raw' / 'cnil' / 'hybrid_classification.json'
        
        # Charger cache LLM
        self.llm_cache = self._load_cache()
        
        # Charger r√©sultats existants pour resume
        self._existing_results = self._load_existing_results()
        
        # Flag d'interruption gracieuse (Ctrl+C)
        self._interrupted = False
        self._original_sigint = signal.getsignal(signal.SIGINT)
        
        # Initialiser LLM
        try:
            config = RAGConfig()
            self.llm = config.llm_provider
            self.mode = config.mode
            logger.info(f"ü§ñ LLM initialis√© en mode : {self.mode}")
        except Exception as e:
            logger.error(f"‚ùå Erreur init LLM : {e}")
            raise
        
        # Stats
        self.stats = {
            'total': 0,
            'obvious_exclude': 0,
            'keyword_exclude': 0,
            'llm_needed': 0,
            'llm_kept': 0,
            'llm_cached': 0,
            'resumed_skip': 0,
        }
    
    def _load_cache(self) -> Dict:
        """Charge le cache LLM"""
        if self.cache_file.exists():
            try:
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    cache = json.load(f)
                logger.info(f"üì¶ Cache charg√© : {len(cache)} classifications")
                return cache
            except:
                return {}
        return {}
    
    def _save_cache(self):
        """Sauvegarde le cache LLM"""
        try:
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.llm_cache, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logger.error(f"‚ùå Erreur sauvegarde cache : {e}")
    
    def _load_existing_results(self) -> Dict:
        """Charge les r√©sultats existants pour reprendre apr√®s interruption."""
        if self.results_file.exists():
            try:
                with open(self.results_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # Construire un set de tous les hashes d√©j√† trait√©s
                already_done = set()
                
                # Docs classifi√©s par LLM
                for h in data.get('llm_classified', {}).keys():
                    already_done.add(h)
                
                # Docs exclus (obvious)
                for item in data.get('excluded_obvious', []):
                    already_done.add(item.get('hash', ''))
                
                # Docs exclus (keywords)
                for item in data.get('excluded_keywords', []):
                    already_done.add(item.get('hash', ''))
                
                already_done.discard('')
                
                if already_done:
                    logger.info(f"‚ôªÔ∏è  R√©sultats existants charg√©s : {len(already_done)} docs d√©j√† trait√©s")
                
                return data
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  Impossible de charger les r√©sultats existants : {e}")
                return {}
        return {}
    
    def _get_already_done_hashes(self) -> set:
        """Retourne le set des hashes d√©j√† trait√©s."""
        done = set()
        for h in self._existing_results.get('llm_classified', {}).keys():
            done.add(h)
        for item in self._existing_results.get('excluded_obvious', []):
            done.add(item.get('hash', ''))
        for item in self._existing_results.get('excluded_keywords', []):
            done.add(item.get('hash', ''))
        done.discard('')
        return done
    
    def _handle_interrupt(self, signum, frame):
        """Gestionnaire Ctrl+C : flag interruption pour sauvegarde propre."""
        if self._interrupted:
            # Deuxi√®me Ctrl+C : quitter imm√©diatement
            print("\n\n‚ö†Ô∏è  Deuxi√®me Ctrl+C ‚Äî arr√™t imm√©diat !")
            sys.exit(1)
        
        self._interrupted = True
        print("\n\nüõë Ctrl+C d√©tect√© ‚Äî arr√™t gracieux en cours...")
        print("   (sauvegarde des r√©sultats en cours, patientez...)")
        print("   (Ctrl+C √† nouveau pour arr√™t imm√©diat)")
    
    def _extract_clean_text(self, html_file: Path, max_length: int = 4000) -> str:
        """Extrait le texte propre d'un HTML en ciblant le contenu principal.
        
        Strat√©gie : 
        1. Cherche le bloc region-content (structure CNIL)
        2. Supprime les √©l√©ments de navigation internes
        3. Fallback sur le body entier si region-content absent
        """
        try:
            with open(html_file, 'r', encoding='utf-8', errors='ignore') as f:
                soup = BeautifulSoup(f, 'lxml')
            
            # Strat√©gie 1 : Cibler region-content (100% des pages CNIL)
            content_block = (
                soup.find(class_='region-content')
                or soup.find('main')
                or soup.find('article')
                or soup.find(class_='field-name-body')
            )
            
            if content_block:
                # Supprimer les blocs de navigation internes
                for tag in content_block(['script', 'style', 'nav', 'aside', 
                                          'iframe', 'noscript', 'svg']):
                    tag.decompose()
                
                # Supprimer menus, breadcrumbs, pagination
                for nav_block in content_block.find_all(class_=lambda c: c and any(
                    x in str(c).lower() for x in [
                        'menu-push', 'breadcrumb', 'pager', 'pagination',
                        'nav-', 'share-', 'social', 'cookie', 'back-to-top'
                    ]
                )):
                    nav_block.decompose()
                
                text = content_block.get_text(separator=' ', strip=True)
            else:
                # Fallback : full page nettoy√©e
                for tag in soup(['script', 'style', 'nav', 'footer', 'header',
                                'aside', 'iframe', 'noscript', 'svg']):
                    tag.decompose()
                text = soup.get_text(separator=' ', strip=True)
            
            # Nettoyage whitespace
            text = ' '.join(text.split())
            
            # Tronquer si trop long
            if len(text) > max_length:
                half = max_length // 2
                text = text[:half] + "\n[...]\n" + text[-half:]
            
            return text
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Erreur extraction texte : {e}")
            return ""
    
    def is_obvious_exclude(self, url: str, text: str) -> Tuple[bool, str]:
        """V√©rifie si le document est √©videmment hors-sujet"""
        
        # 1. Patterns URL √©vidents
        for pattern in self.OBVIOUS_EXCLUDE_PATTERNS:
            if re.search(pattern, url, re.IGNORECASE):
                return True, f"URL pattern: {pattern}"
        
        # 2. Mots-cl√©s d'exclusion forte
        text_lower = text.lower()
        for keyword in self.STRONG_EXCLUDE_KEYWORDS:
            if keyword in text_lower and text_lower.count(keyword) >= 2:
                return True, f"Strong keyword: {keyword}"
        
        return False, ""
    
    def calculate_keyword_score(self, text: str, url: str) -> float:
        """Calcule un score rapide bas√© sur keywords"""
        text_lower = text.lower()
        
        # Score texte
        text_score = 0
        for category, config in self.KEYWORDS.items():
            count = sum(text_lower.count(mot) for mot in config['mots'])
            text_score += count * config['poids']
        
        # Score URL
        url_score = 0
        if '/guide' in url or '/modele' in url or '/outil' in url:
            url_score += 2
        if '/professionnel' in url:
            url_score += 1
        if '/particulier' in url:
            url_score -= 3
        
        return (text_score * 0.7) + (url_score * 0.3)
    
    def classify_with_llm(self, text: str, url: str) -> Dict:
        """Classifie un document avec le LLM"""
        
        # V√©rifier cache
        cache_key = f"{url}_{len(text)}"
        if cache_key in self.llm_cache:
            result = self.llm_cache[cache_key].copy()
            result['cached'] = True
            self.stats['llm_cached'] += 1
            logger.debug(f"üíæ Cache hit pour {url[:60]}...")
            return result
        
        # Construire prompt
        user_prompt = f"""URL : {url}

Extrait du document :
{text}

√âvalue la pertinence de ce document pour un DPO."""
        
        full_prompt = f"{self.SYSTEM_PROMPT}\n\n{user_prompt}"
        
        try:
            logger.debug(f"ü§ñ Appel LLM pour {url[:60]}...")
            
            # Appeler LLM
            response = self.llm.generate(
                full_prompt,
                temperature=0.1,
                max_tokens=500
            )
            
            logger.debug(f"üì• R√©ponse brute (100 premiers chars) : {response[:100]}")
            
            # Parser JSON
            response_clean = response.strip()
            
            # Nettoyer markdown si pr√©sent
            if response_clean.startswith('```json'):
                response_clean = response_clean[7:]
            elif response_clean.startswith('```'):
                response_clean = response_clean[3:]
            
            if response_clean.endswith('```'):
                response_clean = response_clean[:-3]
            
            response_clean = response_clean.strip()
            
            # Essayer de trouver le JSON s'il y a du texte avant/apr√®s
            if not response_clean.startswith('{'):
                # Chercher le premier {
                start_idx = response_clean.find('{')
                if start_idx != -1:
                    response_clean = response_clean[start_idx:]
                    logger.debug(f"‚úÇÔ∏è  JSON extrait apr√®s nettoyage")
            
            if not response_clean.endswith('}'):
                # Chercher le dernier }
                end_idx = response_clean.rfind('}')
                if end_idx != -1:
                    response_clean = response_clean[:end_idx+1]
            
            logger.debug(f"üßπ JSON nettoy√© (100 premiers chars) : {response_clean[:100]}")
            
            # Parser
            result = json.loads(response_clean)
            
            # Valider structure
            required_fields = ['pertinent', 'score', 'categorie', 'raison', 'tags']
            for field in required_fields:
                if field not in result:
                    raise ValueError(f"Champ manquant dans r√©ponse LLM : {field}")
            
            result['cached'] = False
            
            # Log succ√®s
            logger.info(f"‚úÖ {result['categorie']:12s} ({result['score']:4.1f}/10) - {url[:50]}...")
            
            # Mettre en cache
            self.llm_cache[cache_key] = result
            
            # Rate limiting
            time.sleep(0.5 if self.mode == 'local' else 1.0)
            
            return result
        
        except json.JSONDecodeError as e:
            logger.error(f"‚ùå JSON invalide pour {url[:60]}...")
            logger.error(f"   Erreur : {e}")
            logger.error(f"   R√©ponse brute : {response[:200] if 'response' in locals() else 'N/A'}")
            logger.error(f"   Apr√®s nettoyage : {response_clean[:200] if 'response_clean' in locals() else 'N/A'}")
            
            # Fallback : garder par d√©faut
            return {
                "pertinent": True,
                "score": 5.0,
                "categorie": "useful",
                "raison": f"Erreur parsing JSON: {str(e)}",
                "tags": [],
                "cached": False,
                "error": f"JSONDecodeError: {str(e)}"
            }
        
        except Exception as e:
            logger.error(f"‚ùå Erreur LLM pour {url[:60]}...")
            logger.error(f"   Erreur : {type(e).__name__}: {str(e)}")
            
            # Fallback : garder par d√©faut
            return {
                "pertinent": True,
                "score": 5.0,
                "categorie": "useful",
                "raison": f"Erreur classification: {str(e)}",
                "tags": [],
                "cached": False,
                "error": f"{type(e).__name__}: {str(e)}"
            }
    
    def run(self, max_docs: Optional[int] = None, fresh: bool = False):
        """Ex√©cute la classification hybride compl√®te.
        
        Args:
            max_docs: Limiter √† N documents (mode test)
            fresh: Si True, ignore les r√©sultats existants et recommence √† z√©ro
        """
        
        print("=" * 70)
        print("‚ö°üß† CLASSIFICATION HYBRIDE : Keywords + LLM")
        print("=" * 70)
        
        # Installer gestionnaire Ctrl+C
        signal.signal(signal.SIGINT, self._handle_interrupt)
        
        # R√©sultats existants pour resume
        already_done = set() if fresh else self._get_already_done_hashes()
        if already_done and not fresh:
            print(f"\n‚ôªÔ∏è  MODE RESUME : {len(already_done)} documents d√©j√† trait√©s (seront skipp√©s)")
            print(f"   (utiliser --fresh pour recommencer √† z√©ro)")
        
        # R√©cup√©rer listes existantes pour merge incr√©mental
        llm_results = dict(self._existing_results.get('llm_classified', {})) if not fresh else {}
        excluded_obvious = list(self._existing_results.get('excluded_obvious', [])) if not fresh else []
        excluded_keywords = list(self._existing_results.get('excluded_keywords', [])) if not fresh else []
        
        # Lister les HTML
        html_files = list(self.html_dir.glob('*.html'))
        
        if max_docs:
            html_files = html_files[:max_docs]
            print(f"\nüß™ MODE TEST : {max_docs} documents")
        
        self.stats['total'] = len(html_files)
        print(f"\nüìÑ {len(html_files)} documents √† analyser\n")
        
        # PHASE 1 : Pr√©-filtrage Keywords
        print("‚ö° Phase 1 : Pr√©-filtrage par keywords...")
        
        to_llm_classify = []
        
        for html_file in tqdm(html_files, desc="Pr√©-filtrage"):
            if self._interrupted:
                break
            
            try:
                # Skip si d√©j√† trait√© (resume)
                if html_file.stem in already_done:
                    self.stats['resumed_skip'] += 1
                    continue
                
                # Charger m√©tadonn√©es
                metadata_file = self.metadata_dir / f"{html_file.stem}.json"
                if not metadata_file.exists():
                    continue
                
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                    url = metadata.get('url', '')
                
                # Extraire texte
                text = self._extract_clean_text(html_file, max_length=2000)
                
                # V√©rifier exclusions √©videntes
                is_obvious, reason = self.is_obvious_exclude(url, text)
                if is_obvious:
                    excluded_obvious.append({
                        'hash': html_file.stem,
                        'url': url,
                        'reason': reason
                    })
                    self.stats['obvious_exclude'] += 1
                    continue
                
                # Score keywords rapide
                keyword_score = self.calculate_keyword_score(text, url)
                
                # Si score tr√®s n√©gatif, exclure
                if keyword_score < -3:
                    excluded_keywords.append({
                        'hash': html_file.stem,
                        'url': url,
                        'score': keyword_score
                    })
                    self.stats['keyword_exclude'] += 1
                    continue
                
                # Sinon, passer au LLM
                to_llm_classify.append({
                    'hash': html_file.stem,
                    'url': url,
                    'file': html_file,
                    'keyword_score': keyword_score
                })
            
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  Erreur {html_file.name}: {e}")
                continue
        
        self.stats['llm_needed'] = len(to_llm_classify)
        
        # R√©sum√© Phase 1
        total_for_pct = max(1, self.stats['total'])
        print(f"\nüìä R√©sultats Phase 1 :")
        if self.stats['resumed_skip'] > 0:
            print(f"   D√©j√† trait√©s (skip) : {self.stats['resumed_skip']:5d} ({self.stats['resumed_skip']/total_for_pct*100:5.1f}%)")
        print(f"   Exclus (URL √©vidente)   : {self.stats['obvious_exclude']:5d} ({self.stats['obvious_exclude']/total_for_pct*100:5.1f}%)")
        print(f"   Exclus (keywords < -3)  : {self.stats['keyword_exclude']:5d} ({self.stats['keyword_exclude']/total_for_pct*100:5.1f}%)")
        print(f"   √Ä classifier par LLM    : {self.stats['llm_needed']:5d} ({self.stats['llm_needed']/total_for_pct*100:5.1f}%)")
        
        new_excluded = self.stats['obvious_exclude'] + self.stats['keyword_exclude']
        if new_excluded > 0:
            gain_pct = new_excluded / total_for_pct * 100
            print(f"   Gain de temps           : {gain_pct:.1f}%")
        
        # Sauvegarde interm√©diaire apr√®s Phase 1 (les exclusions sont d√©j√† d√©cid√©es)
        self._save_results(llm_results, excluded_obvious, excluded_keywords)
        
        # PHASE 2 : Classification LLM
        if self._interrupted:
            print("\nüõë Interrompu apr√®s Phase 1. R√©sultats partiels sauvegard√©s.")
            self._save_cache()
            signal.signal(signal.SIGINT, self._original_sigint)
            return
        
        if len(to_llm_classify) == 0:
            print("\n‚úÖ Tous les documents trait√©s (exclus ou d√©j√† classifi√©s) !")
            self._save_results(llm_results, excluded_obvious, excluded_keywords)
            signal.signal(signal.SIGINT, self._original_sigint)
            return
        
        estimated_min = len(to_llm_classify) * 3 / 60
        print(f"\nüß† Phase 2 : Classification LLM de {len(to_llm_classify)} documents...")
        print(f"   Dur√©e estim√©e : ~{estimated_min:.0f} minutes ({estimated_min/60:.1f}h)")
        print(f"   üí° Ctrl+C pour interrompre proprement (reprise possible)\n")
        
        save_counter = 0
        phase2_start = time.time()
        
        for item in tqdm(to_llm_classify, desc="Classification LLM"):
            if self._interrupted:
                break
            
            try:
                # Extraire texte complet
                text = self._extract_clean_text(item['file'])
                
                # Classifier
                classification = self.classify_with_llm(text, item['url'])
                
                llm_results[item['hash']] = {
                    'url': item['url'],
                    'keyword_score': item['keyword_score'],
                    'pertinent': classification['pertinent'],
                    'score': classification['score'],
                    'categorie': classification['categorie'],
                    'raison': classification['raison'],
                    'tags': classification.get('tags', []),
                    'cached': classification.get('cached', False),
                }
                
                if classification['pertinent']:
                    self.stats['llm_kept'] += 1
                
                # Sauvegarde r√©guli√®re (cache + r√©sultats)
                save_counter += 1
                if save_counter % 10 == 0:
                    self._save_cache()
                    self._save_results(llm_results, excluded_obvious, excluded_keywords)
                    
                    # Afficher progression temps
                    elapsed = time.time() - phase2_start
                    done_count = save_counter
                    remaining = len(to_llm_classify) - done_count
                    if done_count > 0:
                        eta_sec = (elapsed / done_count) * remaining
                        logger.info(f"üíæ Checkpoint ({done_count}/{len(to_llm_classify)}) ‚Äî ETA: {eta_sec/60:.0f}min")
            
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  Erreur LLM {item['url']}: {e}")
                # En cas d'erreur, garder par d√©faut
                llm_results[item['hash']] = {
                    'url': item['url'],
                    'pertinent': True,
                    'score': 5.0,
                    'categorie': 'useful',
                    'raison': f'Erreur: {str(e)}',
                    'tags': [],
                    'error': str(e)
                }
                self.stats['llm_kept'] += 1
        
        # Sauvegarde finale (ou post-interruption)
        self._save_cache()
        self._save_results(llm_results, excluded_obvious, excluded_keywords)
        
        # Restaurer handler Ctrl+C
        signal.signal(signal.SIGINT, self._original_sigint)
        
        if self._interrupted:
            print(f"\nüõë Interrompu apr√®s {save_counter} docs LLM. R√©sultats sauvegard√©s.")
            print(f"   Relancez la commande pour reprendre automatiquement.")
        else:
            # R√©sum√© final
            self._print_final_summary()
    
    def _save_results(self, llm_results: Dict, excluded_obvious: List, excluded_keywords: List):
        """Sauvegarde les r√©sultats complets"""
        results = {
            'llm_classified': llm_results,
            'excluded_obvious': excluded_obvious,
            'excluded_keywords': excluded_keywords,
            'stats': self.stats,
            'metadata': {
                'mode': self.mode,
                'total_documents': self.stats['total'],
                'timestamp': time.strftime('%Y-%m-%dT%H:%M:%S'),
            }
        }
        
        with open(self.results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"üíæ R√©sultats sauvegard√©s : {self.results_file}")
    
    def _print_final_summary(self):
        """Affiche le r√©sum√© final"""
        print("\n" + "=" * 70)
        print("üìä R√âSUM√â FINAL - CLASSIFICATION HYBRIDE")
        print("=" * 70)
        
        total = self.stats['total']
        
        print(f"\nüìÑ Documents analys√©s : {total}")
        
        if self.stats['resumed_skip'] > 0:
            print(f"\n‚ôªÔ∏è  Resume :")
            print(f"   D√©j√† trait√©s (skip) : {self.stats['resumed_skip']:5d} ({self.stats['resumed_skip']/total*100:5.1f}%)")
        
        print(f"\n‚ö° Phase 1 - Pr√©-filtrage :")
        print(f"   Exclus (URL)      : {self.stats['obvious_exclude']:5d} ({self.stats['obvious_exclude']/total*100:5.1f}%)")
        print(f"   Exclus (keywords) : {self.stats['keyword_exclude']:5d} ({self.stats['keyword_exclude']/total*100:5.1f}%)")
        
        print(f"\nüß† Phase 2 - LLM :")
        print(f"   Analys√©s par LLM  : {self.stats['llm_needed']:5d} ({self.stats['llm_needed']/total*100:5.1f}%)")
        print(f"   Gard√©s par LLM    : {self.stats['llm_kept']:5d} ({self.stats['llm_kept']/self.stats['llm_needed']*100:5.1f}% des analys√©s)")
        print(f"   Cache utilis√©     : {self.stats['llm_cached']:5d} ({self.stats['llm_cached']/self.stats['llm_needed']*100:5.1f}%)")
        
        print(f"\n‚úÖ R√©sultat final :")
        kept = self.stats['llm_kept']
        excluded = total - kept
        print(f"   Documents gard√©s  : {kept:5d} ({kept/total*100:5.1f}%)")
        print(f"   Documents exclus  : {excluded:5d} ({excluded/total*100:5.1f}%)")
        
        # Optimisation
        time_saved_pct = (self.stats['obvious_exclude'] + self.stats['keyword_exclude']) / total * 100
        time_saved_min = (self.stats['obvious_exclude'] + self.stats['keyword_exclude']) * 3 / 60
        
        print(f"\n‚è±Ô∏è  Optimisation :")
        print(f"   Gain de temps LLM : ~{time_saved_pct:.0f}%")
        print(f"   Dur√©e √©conomis√©e  : ~{time_saved_min:.0f} minutes ({time_saved_min/60:.1f}h)")
        
        print("\n" + "=" * 70)
        print(f"üíæ R√©sultats : {self.results_file}")
        print(f"üíæ Cache LLM : {self.cache_file}")
        print("=" * 70)


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='Classification hybride Keywords + LLM')
    parser.add_argument('--project-root', type=str, default='.', help='Racine du projet')
    parser.add_argument('--test', type=int, help='Tester sur N documents')
    parser.add_argument('--fresh', action='store_true', help='Ignorer les r√©sultats existants, recommencer √† z√©ro')
    parser.add_argument('--verbose', '-v', action='store_true', help='Mode verbose (debug)')
    
    args = parser.parse_args()
    
    # Activer debug si demand√©
    if args.verbose:
        logger.setLevel(logging.DEBUG)
        logging.getLogger("__main__").setLevel(logging.DEBUG)
        logger.info("üîç Mode verbose activ√©")
    
    classifier = HybridClassifier(args.project_root)
    classifier.run(max_docs=args.test, fresh=args.fresh)


if __name__ == "__main__":
    main()
