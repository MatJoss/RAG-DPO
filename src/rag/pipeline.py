"""
RAG Pipeline - Orchestration compl√®te du syst√®me RAG

Architecture :
1. Query Expansion : LLM reformule en 3 variantes (multi-query retrieval)
2. Hybrid Retrieval : BM25 (sparse) + ChromaDB (dense) + Summary pre-filter
3. RRF Fusion : Reciprocal Rank Fusion des r√©sultats sparse + dense + multi-query
4. Cross-Encoder Reranking : Jina reranker v2 multilingue (40 ‚Üí 10 chunks)
5. Dual Generation : 2 passes (ordre naturel + invers√©) pour self-consistency
6. Stance Comparison : d√©tection concordance/contradiction entre les 2 r√©ponses
7. Synthesis : si contradiction ‚Üí 3√®me appel LLM pour r√©ponse nuanc√©e
8. Grounding Validation : v√©rification citations sources
"""
import logging
from typing import Dict, Optional, List, Any
from dataclasses import dataclass, field
from datetime import datetime

from .retriever import RAGRetriever, RetrievedDocument, RetrievedChunk
from .context_builder import ContextBuilder
from .generator import Generator
from .validators import RelevanceValidator, GroundingValidator
from .reranker import CrossEncoderReranker, RankedChunk

logger = logging.getLogger(__name__)


@dataclass
class RAGResponse:
    """R√©ponse compl√®te du syst√®me RAG"""
    # R√©ponse
    answer: str
    
    # Sources
    sources: List[Dict[str, Any]] = field(default_factory=list)
    cited_sources: List[int] = field(default_factory=list)
    
    # M√©tadonn√©es
    question: str = ""
    model: str = ""
    retrieval_time: float = 0.0
    generation_time: float = 0.0
    total_time: float = 0.0
    
    # Contexte (optionnel, pour debug)
    retrieved_documents: Optional[List] = None
    context_used: Optional[str] = None
    
    # Erreur √©ventuelle
    error: Optional[str] = None


class RAGPipeline:
    """
    Pipeline RAG complet
    
    Flux :
    1. Query ‚Üí Retriever ‚Üí Documents pertinents
    2. Documents ‚Üí Context Builder ‚Üí Prompt format√©
    3. Prompt ‚Üí Generator ‚Üí R√©ponse LLM
    4. Post-traitement ‚Üí RAGResponse finale
    """
    
    def __init__(
        self,
        retriever: RAGRetriever,
        context_builder: ContextBuilder,
        generator: Generator,
        llm_provider = None,  # Pour validators
        reranker: Optional[CrossEncoderReranker] = None,
        debug_mode: bool = False,
        enable_validation: bool = True,  # Validation ON par d√©faut
        rerank_candidates: int = 40,     # Candidats bruts pass√©s au reranker (semantic+BM25, pas d√©dupliqu√©s)
        rerank_top_k: int = 10,          # Nombre de chunks √† garder apr√®s reranking
    ):
        """
        Args:
            retriever: Retriever configur√©
            context_builder: Context builder configur√©
            generator: Generator configur√©
            llm_provider: Provider LLM pour validators
            reranker: Cross-encoder reranker (optionnel)
            debug_mode: Si True, inclut contexte et documents dans la r√©ponse
            enable_validation: Active validation pertinence + grounding
            rerank_candidates: Chunks √† envoyer au reranker
            rerank_top_k: Chunks √† garder apr√®s reranking
        """
        self.retriever = retriever
        self.context_builder = context_builder
        self.generator = generator
        self.reranker = reranker
        self.debug_mode = debug_mode
        self.enable_validation = enable_validation
        self.rerank_candidates = rerank_candidates
        self.rerank_top_k = rerank_top_k
        
        # Validators
        if enable_validation and llm_provider:
            self.relevance_validator = RelevanceValidator(llm_provider, threshold=0.80)
            self.grounding_validator = GroundingValidator(llm_provider)
        else:
            self.relevance_validator = None
            self.grounding_validator = None
    
    def query(
        self,
        question: str,
        where_filter: Optional[Dict] = None,
        conversation_history: Optional[List[Dict[str, str]]] = None,
        n_documents: Optional[int] = None,
        n_chunks_per_doc: Optional[int] = None,
        temperature: Optional[float] = None,
        _retry_count: int = 0  # Compteur interne pour fallback
    ) -> RAGResponse:
        """
        Query principale du syst√®me RAG
        
        Args:
            question: Question de l'utilisateur
            where_filter: Filtre ChromaDB optionnel (ex: {"chunk_nature": "GUIDE"})
            conversation_history: Historique optionnel [{role: str, content: str}]
            n_documents: Override nombre de documents
            n_chunks_per_doc: Override chunks par document
            temperature: Override temperature LLM
        
        Returns:
            RAGResponse compl√®te
        """
        import time
        start_time = time.time()
        
        logger.info(f"\n{'='*80}")
        logger.info(f"üéØ RAG Query: {question[:100]}...")
        logger.info(f"{'='*80}\n")
        
        try:
            # 1. RETRIEVAL
            logger.info("üì• Phase 1 : Retrieval")
            retrieval_start = time.time()
            
            # Si reranker actif, r√©cup√©rer un large pool de candidats bruts
            # SANS d√©duplication par document ‚Äî le reranker triera
            if self.reranker is not None:
                raw_candidates = self.retriever.retrieve_candidates(
                    query=question,
                    n_candidates=self.rerank_candidates,
                    where_filter=where_filter,
                )
                
                retrieval_time = time.time() - retrieval_start
                logger.info(f"‚úÖ Retrieval candidats: {len(raw_candidates)} chunks en {retrieval_time:.2f}s")
                
                if not raw_candidates:
                    logger.warning("‚ö†Ô∏è  Aucun chunk candidat trouv√©")
                    return RAGResponse(
                        answer="Je n'ai pas trouv√© d'information pertinente dans ma base de connaissance pour r√©pondre √† cette question.",
                        question=question,
                        retrieval_time=retrieval_time,
                        total_time=time.time() - start_time,
                        error="No candidates found"
                    )
                
                # 1.5 RERANKING (cross-encoder) sur TOUS les candidats bruts
                logger.info(f"üîÑ Phase 1.5 : Cross-Encoder Reranking ({len(raw_candidates)} candidats)")
                import time as _time
                rerank_start = _time.time()
                
                ranked_chunks = self.reranker.rerank(
                    query=question,
                    chunks=raw_candidates,
                    top_k=self.rerank_top_k,
                )
                
                # Reconstruire les documents √† partir des chunks reranked
                # On passe une liste vide car on n'a pas de documents originaux
                documents = self._rebuild_documents_from_ranked_chunks(
                    ranked_chunks, n_chunks_per_doc
                )
                
                rerank_time = _time.time() - rerank_start
                logger.info(
                    f"‚úÖ Reranking termin√© en {rerank_time:.2f}s "
                    f"({len(raw_candidates)} ‚Üí {sum(len(d.chunks) for d in documents)} chunks, "
                    f"{len(documents)} documents)"
                )
            else:
                # Sans reranker : flow classique avec d√©dup par document
                documents = self.retriever.retrieve(
                    query=question,
                    where_filter=where_filter,
                    n_documents=n_documents,
                    n_chunks_per_doc=n_chunks_per_doc
                )
                
                retrieval_time = time.time() - retrieval_start
                logger.info(f"‚úÖ Retrieval termin√© en {retrieval_time:.2f}s")
            
            if not documents:
                logger.warning("‚ö†Ô∏è  Aucun document trouv√©")
                return RAGResponse(
                    answer="Je n'ai pas trouv√© d'information pertinente dans ma base de connaissance pour r√©pondre √† cette question.",
                    question=question,
                    retrieval_time=retrieval_time,
                    total_time=time.time() - start_time,
                    error="No documents found"
                )
            
            # Debug: affiche documents r√©cup√©r√©s
            if self.debug_mode:
                logger.debug(self.retriever.format_results_debug(documents))
            
            # 2. VALIDATION PERTINENCE
            # Skip quand le reranker est actif : le cross-encoder EST la validation de pertinence.
            # Le seuil distance <= 0.80 est calibr√© pour distances cosine brutes (~0.20-0.27),
            # pas pour distances post-reranking (1 - rerank_score). Le validator rejetterait
            # les chunks que le reranker a correctement scor√©s bas (mais non-nuls).
            if self.enable_validation and self.relevance_validator and self.reranker is None:
                logger.info("üîç Phase 2 : Validation pertinence des chunks...")
                
                # Extraire tous les chunks pour validation
                all_chunks = []
                for doc in documents:
                    all_chunks.extend(doc.chunks)
                
                total_chunks = len(all_chunks)
                
                # Valider
                valid_chunks = self.relevance_validator.validate_chunks(
                    query=question,
                    chunks=all_chunks,
                    conversation_history=conversation_history
                )
                
                logger.info(f"   ‚úÖ Pertinence: {len(valid_chunks)}/{total_chunks} chunks valid√©s")
                
                # IMPORTANT: Filtrer les chunks non pertinents des documents
                valid_chunk_ids = {chunk.chunk_id for chunk in valid_chunks}
                for doc in documents:
                    doc.chunks = [c for c in doc.chunks if c.chunk_id in valid_chunk_ids]
                
                # Si trop de chunks rejet√©s ET pas de reranker actif, fallback vers plus de docs
                # Quand le reranker est actif, les chunks bas-score sont l√©gitimement non-pertinents
                # Le fallback classique (retrieve sans reranker) ne ferait qu'ajouter du bruit
                if (len(valid_chunks) < total_chunks * 0.5 
                    and len(valid_chunks) < 3 
                    and self.reranker is None):
                    logger.warning(f"‚ö†Ô∏è  Seulement {len(valid_chunks)} chunks pertinents, r√©cup√©ration de documents additionnels...")
                    # R√©cup√©rer plus de docs en fallback
                    documents = self.retriever.retrieve(
                        query=question,
                        where_filter=where_filter,
                        n_documents=(n_documents or 3) + 2,  # +2 docs suppl√©mentaires
                        n_chunks_per_doc=n_chunks_per_doc
                    )
                elif len(valid_chunks) < 3 and self.reranker is not None:
                    logger.info(f"‚ÑπÔ∏è  {len(valid_chunks)} chunks valides apr√®s reranking ‚Äî normal pour questions sp√©cifiques")
            
            # 3. CONTEXT BUILDING + 4. GENERATION
            # Dual-generation quand le reranker est actif : on g√©n√®re avec les
            # deux ordres de pr√©sentation (normal + invers√©) et on compare.
            # Si les r√©ponses se contredisent, le contexte est ambigu.
            logger.info("üèóÔ∏è  Phase 3 : Context Building")
            
            if self.reranker is not None:
                # ‚îÄ‚îÄ DUAL-GENERATION (self-consistency via context order) ‚îÄ‚îÄ
                # Pass A : ordre naturel (Source 1 = plus pertinent en premier)
                context_a = self.context_builder.build_context(
                    documents=documents,
                    question=question,
                    conversation_history=conversation_history,
                    reverse_packing_override=False,
                )
                # Pass B : ordre invers√© (Source 1 en dernier, recency bias)
                context_b = self.context_builder.build_context(
                    documents=documents,
                    question=question,
                    conversation_history=conversation_history,
                    reverse_packing_override=True,
                )
                
                logger.info(f"‚úÖ Dual context construit (A={len(context_a['user'])} chars, B={len(context_b['user'])} chars)")
                
                # G√©n√©ration des deux passes
                logger.info("ü§ñ Phase 4 : Dual Generation (self-consistency)")
                generation_start = time.time()
                
                generated_a = self.generator.generate(
                    system_prompt=context_a['system'],
                    user_prompt=context_a['user'],
                    temperature=temperature
                )
                generated_b = self.generator.generate(
                    system_prompt=context_b['system'],
                    user_prompt=context_b['user'],
                    temperature=temperature
                )
                
                generation_time = time.time() - generation_start
                
                if generated_a.error and generated_b.error:
                    logger.error(f"‚ùå Erreur double g√©n√©ration: A={generated_a.error}, B={generated_b.error}")
                    return RAGResponse(
                        answer="Une erreur s'est produite lors de la g√©n√©ration de la r√©ponse.",
                        question=question,
                        retrieval_time=retrieval_time,
                        generation_time=generation_time,
                        total_time=time.time() - start_time,
                        error=generated_a.error
                    )
                
                # Choisir la r√©ponse ou d√©tecter l'incoh√©rence
                generated, context = self._select_dual_response(
                    generated_a, generated_b,
                    context_a, context_b,
                    question, documents,
                )
                
                logger.info(f"‚úÖ Dual generation termin√©e en {generation_time:.2f}s")
            else:
                # ‚îÄ‚îÄ SINGLE GENERATION (pas de reranker) ‚îÄ‚îÄ
                context = self.context_builder.build_context(
                    documents=documents,
                    question=question,
                    conversation_history=conversation_history
                )
                
                logger.info(f"‚úÖ Context construit ({len(context['user'])} chars)")
                
                # Debug: affiche le prompt
                if self.debug_mode:
                    logger.debug(f"\n{'='*80}")
                    logger.debug("SYSTEM PROMPT:")
                    logger.debug(context['system'][:500] + "...")
                    logger.debug(f"\n{'='*80}")
                    logger.debug("USER PROMPT:")
                    logger.debug(context['user'][:1000] + "...")
                    logger.debug(f"{'='*80}\n")
                
                logger.info("ü§ñ Phase 4 : Generation")
                generation_start = time.time()
                
                generated = self.generator.generate(
                    system_prompt=context['system'],
                    user_prompt=context['user'],
                    temperature=temperature
                )
                
                generation_time = time.time() - generation_start
                
                if generated.error:
                    logger.error(f"‚ùå Erreur g√©n√©ration: {generated.error}")
                    return RAGResponse(
                        answer="Une erreur s'est produite lors de la g√©n√©ration de la r√©ponse.",
                        question=question,
                        retrieval_time=retrieval_time,
                        generation_time=generation_time,
                        total_time=time.time() - start_time,
                        error=generated.error
                    )
                
                logger.info(f"‚úÖ G√©n√©ration termin√©e en {generation_time:.2f}s")
            
            # 4.5 VALIDATION GROUNDING
            if self.enable_validation and self.grounding_validator:
                logger.info("üîç Validation grounding de la r√©ponse...")
                
                available_sources = [s['id'] for s in context['sources_metadata']]
                validation = self.grounding_validator.validate_response(
                    response=generated.text,
                    available_sources=available_sources,
                    context=context['user']
                )
                
                if validation.is_valid:
                    logger.info(f"   ‚úÖ Grounding valid√© (score: {validation.score:.2f})")
                else:
                    logger.error(f"   ‚ùå Grounding √©chou√©: {validation.reason}")
                
                # Grounding issues : traitement gradu√©
                if not validation.is_valid:
                    if "hallucination" in validation.reason.lower():
                        # Compter les probl√®mes concrets (montants, articles, dates)
                        n_issues = validation.reason.count(" ; ") + 1
                        if n_issues >= 3:
                            # Trop de faits invent√©s ‚Üí rejeter
                            logger.error(f"‚ùå HALLUCINATION S√âV√àRE ({n_issues} probl√®mes) - R√©ponse rejet√©e")
                            return RAGResponse(
                                answer="Je n'ai pas trouv√© suffisamment d'informations fiables dans mes sources pour r√©pondre pr√©cis√©ment √† cette question. Les documents disponibles ne couvrent peut-√™tre pas ce sujet en d√©tail.",
                                question=question,
                                retrieval_time=retrieval_time,
                                generation_time=generation_time,
                                total_time=time.time() - start_time,
                                error=f"Hallucination detected: {validation.reason}"
                            )
                        else:
                            # Warnings mineurs ‚Üí garder la r√©ponse mais logger
                            logger.warning(f"‚ö†Ô∏è  Grounding partiel ({n_issues} warning(s)): {validation.reason}")
                    
                    # Phrases interdites : WARNING seulement (few-shot prompt devrait g√©rer)
                    if "phrases interdites" in validation.reason.lower():
                        logger.warning(f"‚ö†Ô∏è  Phrases 'consultez' d√©tect√©es (few-shot prompt pas respect√©): {validation.reason}")
                    
                    # Si sources invent√©es, les supprimer
                    if "invent√©es" in validation.reason.lower():
                        logger.warning("üßπ Nettoyage des sources invent√©es...")
                        generated.text = self.grounding_validator.fix_invented_sources(
                            response=generated.text,
                            available_sources=available_sources
                        )
                    
                    # Si r√©ponse √©vasive, warning seulement
                    if "√©vasive" in validation.reason.lower():
                        logger.warning("‚ö†Ô∏è  R√©ponse √©vasive d√©tect√©e - le LLM ne trouve pas la r√©ponse dans le contexte")
            
            # 5. POST-PROCESSING
            logger.info("üìù Phase 5 : Post-processing")
            
            # Validation qualit√© : v√©rifier que r√©ponse contient du contenu substantiel
            response_text = generated.text.strip()
            lines = [l.strip() for l in response_text.split('\n') if l.strip()]
            
            # R√©ponse trop courte (< 50 chars ou vide) ‚Üí retry imm√©diat
            if len(response_text) < 50 and _retry_count == 0:
                logger.warning(f"‚ö†Ô∏è  R√©ponse trop courte ({len(response_text)} chars)")
                logger.info("üîÑ FALLBACK : Retry avec plus de documents...")
                return self.query(
                    question=question,
                    where_filter=where_filter,
                    conversation_history=conversation_history,
                    n_documents=(n_documents or 5) + 3,
                    n_chunks_per_doc=(n_chunks_per_doc or 3) + 2,
                    temperature=temperature,
                    _retry_count=1
                )
            
            # Compter lignes avec contenu actionnable (listes, crit√®res, √©tapes concr√®tes)
            actionable_keywords = [':', '-', '‚Ä¢', '1.', '2.', '3.', 'selon', 'exemple', 'notamment', 'sont', 'doit']
            actionable_lines = sum(1 for line in lines if any(kw in line.lower() for kw in actionable_keywords))
            
            # Si <30% de contenu actionnable ET r√©ponse longue (>3 lignes) ET pas encore retry ‚Üí FALLBACK
            # Note: r√©ponses courtes (1-3 lignes) sont souvent concises et correctes, pas insuffisantes
            if len(lines) > 3 and actionable_lines / len(lines) < 0.3 and _retry_count == 0:
                logger.warning(f"‚ö†Ô∏è  R√©ponse insuffisante ({actionable_lines}/{len(lines)} lignes actionnables)")
                logger.info("üîÑ FALLBACK : R√©cup√©ration de plus de documents...")
                
                # Retry avec plus de docs et chunks
                return self.query(
                    question=question,
                    where_filter=where_filter,
                    conversation_history=conversation_history,
                    n_documents=(n_documents or 5) + 3,  # +3 docs
                    n_chunks_per_doc=(n_chunks_per_doc or 3) + 2,  # +2 chunks
                    temperature=temperature,
                    _retry_count=1  # Marquer comme retry
                )
            elif len(lines) > 3 and actionable_lines / len(lines) < 0.3 and _retry_count > 0:
                logger.error("‚ùå R√©ponse insuffisante m√™me apr√®s fallback")
                # Garder la r√©ponse mais ajouter disclaimer
                generated.text += f"\n\n‚ö†Ô∏è Les sources disponibles ne contiennent pas suffisamment de d√©tails. Pour plus d'informations, consultez directement [Source 1]."
            
            # Post-processing Markdown : am√©liorer le formatage des listes
            generated.text = self._fix_markdown_formatting(generated.text)
            
            formatted = self.context_builder.format_response_with_sources(
                response=generated.text,
                sources_metadata=context['sources_metadata']
            )
            
            # Construction r√©ponse finale
            total_time = time.time() - start_time
            
            response = RAGResponse(
                answer=formatted['response'],
                sources=formatted['sources'],
                cited_sources=formatted['cited_sources'],
                question=question,
                model=generated.model,
                retrieval_time=retrieval_time,
                generation_time=generation_time,
                total_time=total_time
            )
            
            # Debug info
            if self.debug_mode:
                response.retrieved_documents = documents
                response.context_used = context['user']
            
            logger.info(f"\n{'='*80}")
            logger.info(f"‚úÖ Pipeline termin√© en {total_time:.2f}s")
            logger.info(f"   - Retrieval: {retrieval_time:.2f}s")
            logger.info(f"   - Generation: {generation_time:.2f}s")
            logger.info(f"   - Documents: {len(documents)}")
            logger.info(f"   - Sources cit√©es: {len(response.cited_sources)}/{len(response.sources)}")
            logger.info(f"{'='*80}\n")
            
            return response
        
        except Exception as e:
            logger.error(f"‚ùå Erreur pipeline: {e}", exc_info=True)
            return RAGResponse(
                answer="Une erreur inattendue s'est produite lors du traitement de votre question.",
                question=question,
                total_time=time.time() - start_time,
                error=str(e)
            )
    
    @staticmethod
    def _fix_markdown_formatting(text: str) -> str:
        """Post-processing : convertit les listes inline en listes Markdown √† puces.
        
        D√©tecte les patterns courants o√π le LLM produit des listes s√©par√©es par
        des ';' ou des num√©ros indent√©s au lieu de listes Markdown propres.
        """
        import re
        
        lines = text.split('\n')
        result = []
        
        for line in lines:
            stripped = line.strip()
            
            # Pattern 1: lignes avec items s√©par√©s par " ; " qui ressemblent √† une liste
            # Ex: "crit√®re A ; crit√®re B ; crit√®re C [Source 2]."
            # On d√©tecte si c'est une vraie liste (3+ items s√©par√©s par ;)
            if ' ; ' in stripped:
                parts = stripped.split(' ; ')
                if len(parts) >= 3:
                    # Extraire le pr√©fixe (texte avant la liste)
                    # Ex: "Les neuf crit√®res sont : crit√®re A ; crit√®re B..."
                    # Chercher " : " ou ": " comme s√©parateur intro/liste
                    intro_match = re.match(r'^(.+?(?:sont|suivants?|comprennent|incluent|suivantes?)\s*:\s*)(.*)', stripped, re.IGNORECASE)
                    if intro_match:
                        intro = intro_match.group(1).strip()
                        list_text = intro_match.group(2).strip()
                        parts = list_text.split(' ; ')
                    else:
                        intro = None
                    
                    if len(parts) >= 3:
                        # C'est une liste inline ‚Üí convertir en puces
                        if intro:
                            result.append(intro)
                            result.append('')  # ligne vide avant la liste
                        for part in parts:
                            part = part.strip().rstrip('.').rstrip(',')
                            if part:
                                # Majuscule au d√©but
                                part = part[0].upper() + part[1:] if part else part
                                result.append(f'- {part}')
                        # R√©cup√©rer la source √† la fin si pr√©sente
                        last_part = parts[-1].strip()
                        source_match = re.search(r'(\[Source\s*\d+\]\.?)$', last_part)
                        if source_match:
                            # La source est d√©j√† sur le dernier item, c'est bon
                            pass
                        result.append('')  # ligne vide apr√®s la liste
                        continue
            
            # Pattern 2: lignes indent√©es avec num√©ro mais sans format Markdown
            # Ex: "    1. D√©crire le traitement" ou "    D√©crire le traitement"
            indent_match = re.match(r'^(\s{4,})(\d+[\.\)]\s*)?(.+)$', line)
            if indent_match and not stripped.startswith('-') and not stripped.startswith('*'):
                num = indent_match.group(2)
                content = indent_match.group(3).strip()
                if num:
                    result.append(f'{num.strip()} {content}')
                else:
                    result.append(f'- {content}')
                continue
            
            result.append(line)
        
        return '\n'.join(result)
    
    def _select_dual_response(
        self,
        generated_a,  # GeneratedResponse (ordre naturel)
        generated_b,  # GeneratedResponse (ordre invers√©)
        context_a: Dict,
        context_b: Dict,
        question: str,
        documents: List,
    ):
        """
        Compare deux r√©ponses g√©n√©r√©es avec des ordres de contexte diff√©rents.
        
        Self-consistency via context order :
        - Si les deux r√©ponses concordent (m√™me position oui/non, m√™mes faits cl√©s)
          ‚Üí prendre la r√©ponse A (ordre naturel, plus fiable avec reranker)
        - Si elles se contredisent (une dit oui, l'autre non) ‚Üí le contexte contient
          des sources contradictoires pour des cas diff√©rents ‚Üí synth√®se nuanc√©e
        
        Returns:
            (generated, context) ‚Äî la r√©ponse choisie et son contexte associ√©
        """
        import re
        
        # Si une seule r√©ponse est valide, la prendre
        if generated_a.error:
            logger.warning("‚ö†Ô∏è  Dual-gen: pass A en erreur, utilisation pass B")
            return generated_b, context_b
        if generated_b.error:
            logger.warning("‚ö†Ô∏è  Dual-gen: pass B en erreur, utilisation pass A")
            return generated_a, context_a
        
        text_a = generated_a.text.strip()
        text_b = generated_b.text.strip()
        
        # D√©tecter la position (oui/non) de chaque r√©ponse
        stance_a = self._detect_stance(text_a)
        stance_b = self._detect_stance(text_b)
        
        logger.info(f"üîÑ Dual-gen: stance A={stance_a}, stance B={stance_b}")
        logger.info(f"   A ({len(text_a)} chars): {text_a[:120]}...")
        logger.info(f"   B ({len(text_b)} chars): {text_b[:120]}...")
        
        # Cas 1 : les deux r√©ponses concordent ‚Üí prendre A (ordre naturel)
        # v11b a montr√© que le reverse packing (pass B) donne 91% vs 93% avec pass A
        # Le recency bias amplifie les chunks toxiques quand ils sont bien class√©s
        if stance_a == stance_b:
            logger.info(f"‚úÖ Dual-gen: r√©ponses concordantes ({stance_a}), utilisation pass A (ordre naturel)")
            return generated_a, context_a
        
        # Cas 2 : les deux sont neutres/incertaines ‚Üí prendre A (ordre naturel)
        if stance_a == "neutral" and stance_b == "neutral":
            logger.info(f"‚úÖ Dual-gen: deux r√©ponses neutres, utilisation pass A (ordre naturel)")
            return generated_a, context_a
        
        # Cas 3 : CONTRADICTION (une dit oui, l'autre dit non)
        # ‚Üí Les sources contiennent des positions diff√©rentes pour des cas diff√©rents
        # ‚Üí G√©n√©rer une r√©ponse de synth√®se
        logger.warning(f"‚ö†Ô∏è  Dual-gen: CONTRADICTION d√©tect√©e (A={stance_a}, B={stance_b})")
        logger.warning(f"   ‚Üí Synth√®se des deux r√©ponses")
        
        # Construire un prompt de synth√®se qui force la nuance
        synthesis_prompt = f"""Voici deux analyses de la m√™me question, bas√©es sur les m√™mes sources RGPD/CNIL mais pr√©sent√©es dans un ordre diff√©rent.

ANALYSE 1 (position: {stance_a}) :
{text_a}

ANALYSE 2 (position: {stance_b}) :
{text_b}

QUESTION ORIGINALE : {question}

Les deux analyses utilisent les m√™mes sources. Si elles arrivent √† des conclusions diff√©rentes, c'est que les sources d√©crivent des CAS DIFF√âRENTS (ex: secteur public vs priv√©, type d'organisme, finalit√© du traitement).

R√©dige UNE r√©ponse de synth√®se qui :
1. R√©pond "Oui, sous conditions" ou "Cela d√©pend du cas" si applicable
2. Distingue clairement les cas o√π c'est possible et ceux o√π √ßa ne l'est pas
3. Cite les [Source X] des deux analyses
4. Donne les conditions, crit√®res ou limites mentionn√©s dans les analyses

R√©ponse de synth√®se :"""

        try:
            synthesis = self.generator.generate(
                system_prompt=context_a['system'],
                user_prompt=synthesis_prompt,
                temperature=0.0
            )
            if not synthesis.error:
                logger.info(f"‚úÖ Dual-gen: synth√®se g√©n√©r√©e ({len(synthesis.text)} chars)")
                # Utiliser context_a pour les m√©tadonn√©es sources (identiques)
                return synthesis, context_a
            else:
                logger.warning(f"‚ö†Ô∏è  Dual-gen: erreur synth√®se, fallback sur pass A")
                return generated_a, context_a
        except Exception as e:
            logger.error(f"‚ùå Dual-gen: erreur synth√®se: {e}, fallback sur pass A")
            return generated_a, context_a
    
    @staticmethod
    def _detect_stance(text: str) -> str:
        """
        D√©tecte la position globale d'une r√©ponse : 'positive', 'negative', ou 'neutral'.
        
        Analyse les premiers ~200 caract√®res (la r√©ponse directe) pour d√©tecter
        si le LLM dit oui, non, ou nuance.
        """
        # Prendre le d√©but de la r√©ponse (la position est toujours au d√©but)
        start = text[:300].lower()
        
        # Patterns n√©gatifs forts
        neg_patterns = [
            r'\b(?:non|no)\b[,.]',
            r"n'est pas (?:possible|autoris√©|permis|mobilisable|valable|applicable)",
            r"ne peut pas √™tre (?:utilis√©|invoqu√©|mobilis√©|retenu)",
            r"ne (?:peut|doit|saurait) pas",
            r"n'est pas une base l√©gale",
            r"impossible",
            r"interdit",
        ]
        
        # Patterns positifs forts
        pos_patterns = [
            r'\b(?:oui|yes)\b[,.]',
            r"(?:peut|peuvent) √™tre (?:utilis√©|invoqu√©|mobilis√©|retenu)",
            r"est possible",
            r"est autoris√©",
            r"sous (?:certaines |)conditions",
            r"sous r√©serve",
            r"peut (?:constituer|servir|fonder)",
        ]
        
        import re
        neg_score = sum(1 for p in neg_patterns if re.search(p, start))
        pos_score = sum(1 for p in pos_patterns if re.search(p, start))
        
        if neg_score > pos_score and neg_score >= 1:
            return "negative"
        elif pos_score > neg_score and pos_score >= 1:
            return "positive"
        else:
            return "neutral"
    
    def _rebuild_documents_from_ranked_chunks(
        self,
        ranked_chunks: List[RankedChunk],
        n_chunks_per_doc: Optional[int] = None,
    ) -> List[RetrievedDocument]:
        """
        Reconstruit les RetrievedDocument √† partir de chunks reranked SANS documents originaux.
        
        Utilis√© par le nouveau flow o√π le retriever retourne des chunks bruts (pas
        group√©s par document). Le reranker trie tous les candidats, puis on regroupe
        les top chunks par document_path pour construire les documents.
        
        IMPORTANT: Pas de limite n_chunks_per_doc ici ‚Äî le reranker a D√âJ√Ä s√©lectionn√©
        les top_k meilleurs chunks. Tous doivent √™tre pass√©s au context builder.
        La limitation par document ferait perdre des chunks pertinents que le cross-encoder
        a explicitement choisi de garder.
        
        Args:
            ranked_chunks: Chunks tri√©s par le cross-encoder
            n_chunks_per_doc: IGNOR√â dans ce flow (kept for API compat)
            
        Returns:
            Liste de RetrievedDocument ordonn√©s par meilleur score de reranking
        """
        from collections import defaultdict
        
        # NOTE: On ne limite PAS par n_chunks_per_doc dans le flow reranker.
        # Le reranker a d√©j√† s√©lectionn√© les meilleurs chunks tous documents confondus.
        # Limiter √† 3/doc jetterait des chunks bien class√©s.
        
        # Regrouper les ranked chunks par document
        doc_chunks = defaultdict(list)
        doc_best_score = {}
        for rc in ranked_chunks:
            doc_chunks[rc.document_path].append(rc)
            if rc.document_path not in doc_best_score or rc.rerank_score > doc_best_score[rc.document_path]:
                doc_best_score[rc.document_path] = rc.rerank_score
        
        # Construire les documents, tri√©s par meilleur score de chunk
        sorted_doc_paths = sorted(doc_best_score.keys(), key=lambda p: doc_best_score[p], reverse=True)
        
        new_documents = []
        for doc_path in sorted_doc_paths:
            chunks_ranked = doc_chunks[doc_path]
            chunks_ranked.sort(key=lambda x: x.rerank_score, reverse=True)
            # PAS de limite n_chunks_per_doc : tous les chunks reranked sont gard√©s
            
            # Reconvertir RankedChunk ‚Üí RetrievedChunk
            converted_chunks = []
            for rc in chunks_ranked:
                converted_chunks.append(RetrievedChunk(
                    chunk_id=rc.chunk_id,
                    text=rc.text,
                    document_path=rc.document_path,
                    chunk_nature=rc.metadata.get('chunk_nature', 'UNKNOWN'),
                    chunk_index=rc.metadata.get('chunk_index', 0),
                    confidence=rc.metadata.get('confidence', 'medium'),
                    distance=1.0 - rc.rerank_score,  # Convertir score ‚Üí distance
                    metadata=rc.metadata,
                    hybrid_score=rc.rerank_score,
                ))
            
            # D√©duire la nature du document depuis les chunks
            natures = [rc.metadata.get('chunk_nature', 'UNKNOWN') for rc in chunks_ranked]
            primary_nature = max(set(natures), key=natures.count) if natures else 'UNKNOWN'
            
            new_documents.append(RetrievedDocument(
                document_path=doc_path,
                chunks=converted_chunks,
                avg_similarity=doc_best_score[doc_path],
                primary_nature=primary_nature,
            ))
        
        logger.info(f"üì¶ Reranking ‚Üí {len(new_documents)} documents reconstruits")
        return new_documents
    
    def _rebuild_documents_from_ranked(
        self,
        ranked_chunks: List[RankedChunk],
        original_documents: List[RetrievedDocument],
        n_chunks_per_doc: Optional[int] = None,
    ) -> List[RetrievedDocument]:
        """
        Reconstruit les RetrievedDocument √† partir des chunks reranked.
        
        Apr√®s reranking, on regroupe les chunks par document_path
        et on reconstruit des RetrievedDocument ordonn√©s par le meilleur
        score de reranking de leurs chunks.
        """
        from collections import defaultdict
        
        n_chunks = n_chunks_per_doc or self.retriever.n_chunks_per_doc
        
        # Index des documents originaux pour r√©cup√©rer les m√©tadonn√©es
        original_doc_map = {doc.document_path: doc for doc in original_documents}
        
        # Regrouper les ranked chunks par document
        doc_chunks = defaultdict(list)
        doc_best_score = {}
        for rc in ranked_chunks:
            doc_chunks[rc.document_path].append(rc)
            if rc.document_path not in doc_best_score or rc.rerank_score > doc_best_score[rc.document_path]:
                doc_best_score[rc.document_path] = rc.rerank_score
        
        # Construire les documents, tri√©s par meilleur score de chunk
        sorted_doc_paths = sorted(doc_best_score.keys(), key=lambda p: doc_best_score[p], reverse=True)
        
        new_documents = []
        for doc_path in sorted_doc_paths:
            chunks_ranked = doc_chunks[doc_path]
            # Trier les chunks de ce document par score de reranking
            chunks_ranked.sort(key=lambda x: x.rerank_score, reverse=True)
            # Limiter au nombre de chunks par document
            chunks_ranked = chunks_ranked[:n_chunks]
            
            # Reconvertir RankedChunk ‚Üí RetrievedChunk
            original_doc = original_doc_map.get(doc_path)
            converted_chunks = []
            for rc in chunks_ranked:
                converted_chunks.append(RetrievedChunk(
                    chunk_id=rc.chunk_id,
                    text=rc.text,
                    document_path=rc.document_path,
                    chunk_nature=rc.metadata.get('chunk_nature', 'UNKNOWN'),
                    chunk_index=rc.metadata.get('chunk_index', 0),
                    confidence=rc.metadata.get('confidence', 'medium'),
                    distance=1.0 - rc.rerank_score,  # Convertir score ‚Üí distance
                    metadata=rc.metadata,
                    hybrid_score=rc.rerank_score,
                ))
            
            primary_nature = original_doc.primary_nature if original_doc else 'UNKNOWN'
            
            new_documents.append(RetrievedDocument(
                document_path=doc_path,
                chunks=converted_chunks,
                avg_similarity=doc_best_score[doc_path],
                primary_nature=primary_nature,
            ))
        
        logger.info(f"üì¶ Reranking ‚Üí {len(new_documents)} documents reconstruits")
        return new_documents
    
    def format_response(self, response: RAGResponse, show_sources: bool = True) -> str:
        """
        Formate la r√©ponse pour affichage
        
        Args:
            response: RAGResponse √† formatter
            show_sources: Inclure les sources
        
        Returns:
            Texte format√© pour CLI
        """
        lines = []
        
        # R√©ponse
        lines.append("=" * 80)
        lines.append("R√âPONSE")
        lines.append("=" * 80)
        lines.append(response.answer)
        lines.append("")
        
        # Sources
        if show_sources and response.sources:
            lines.append("=" * 80)
            lines.append(f"SOURCES ({len(response.sources)} documents)")
            lines.append("=" * 80)
            
            for source in response.sources:
                cited = "‚úì" if source['cited'] else " "
                lines.append(f"\n[{cited}] Source {source['id']} - {source['nature']}")
                # Afficher URL (CNIL) ou path (Entreprise)
                url = source.get('url', source['path'])
                lines.append(f"    URL: {url}")
                # Localisation pr√©cise
                locations = source.get('locations', [])
                if locations:
                    lines.append(f"    üìç {' | '.join(locations[:3])}")
                lines.append(f"    Score: {source['score']:.3f}")
            
            lines.append("")
        
        # M√©tadonn√©es
        lines.append("=" * 80)
        lines.append("M√âTADONN√âES")
        lines.append("=" * 80)
        lines.append(f"Mod√®le: {response.model}")
        lines.append(f"Temps total: {response.total_time:.2f}s")
        lines.append(f"  - Retrieval: {response.retrieval_time:.2f}s")
        lines.append(f"  - Generation: {response.generation_time:.2f}s")
        lines.append(f"Sources cit√©es: {len(response.cited_sources)}/{len(response.sources)}")
        lines.append("=" * 80)
        
        return "\n".join(lines)


def create_pipeline(
    collection,
    llm_provider,
    n_documents: int = 5,
    n_chunks_per_doc: int = 3,
    max_context_length: int = 32000,  # ~8000 tokens, Nemo 128K
    model: str = "mistral-nemo",
    temperature: float = 0.0,  # Factuel strict par d√©faut
    max_tokens: int = 2000,  # Assez long pour listes compl√®tes
    debug_mode: bool = False,
    enable_validation: bool = True,  # Validation ON par d√©faut
    enable_hybrid: bool = True,      # Recherche hybride BM25+semantic
    enable_reranker: bool = True,    # Cross-encoder reranking
    enable_summary_prefilter: bool = True,  # Pr√©-filtre par summaries
    enable_query_expansion: bool = True,    # LLM query expansion (multi-query)
    summaries_path: Optional[str] = None,
    rerank_candidates: int = 40,
    rerank_top_k: int = 10,
) -> RAGPipeline:
    """
    Factory function pour cr√©er un pipeline RAG complet avec toutes les optimisations.
    
    Architecture:
    1. Query ‚Üí Acronym Expansion ‚Üí LLM Query Expansion (multi-query)
    2. Summary Pre-Filter (BM25 sur summaries)
    3. Multi-Query Hybrid Retrieval (BM25 sparse + ChromaDB dense √ó N queries + RRF)
    4. Cross-Encoder Reranking (bge-reranker-v2-m3, multilingue)
    5. Relevance Validation (LLM-based)
    6. Context Building (reverse repacking)
    7. Generation (Mistral-Nemo)
    8. Grounding Validation ‚Üí Post-process
    
    Args:
        collection: Collection ChromaDB
        llm_provider: Provider LLM
        n_documents: Nombre de documents √† r√©cup√©rer
        n_chunks_per_doc: Chunks par document
        max_context_length: Longueur max du contexte (chars)
        model: Mod√®le LLM
        temperature: Temperature g√©n√©ration
        debug_mode: Mode debug
        enable_validation: Active validation pertinence + grounding
        enable_hybrid: Active BM25+semantic hybrid search
        enable_reranker: Active cross-encoder reranking
        enable_summary_prefilter: Active pr√©-filtre par summaries
        enable_query_expansion: Active LLM query expansion (multi-query retrieval)
        summaries_path: Chemin vers document_summaries.json (auto-d√©tect√© si None)
        rerank_candidates: Nombre de chunks √† passer au reranker
        rerank_top_k: Nombre de chunks √† garder apr√®s reranking
    
    Returns:
        RAGPipeline configur√© et pr√™t √† l'emploi
    """
    import time
    from pathlib import Path
    from .retriever import create_retriever
    from .context_builder import create_context_builder
    from .generator import create_generator
    from .bm25_index import SummaryBM25Index, ChunkBM25Index
    from .query_expander import QueryExpander
    
    init_start = time.time()
    
    # ‚îÄ‚îÄ Query Expander (multi-query) ‚îÄ‚îÄ
    query_expander = None
    if enable_query_expansion:
        logger.info("üîÑ Activation Query Expansion (multi-query retrieval)")
        query_expander = QueryExpander(
            llm_provider=llm_provider,
            enabled=True,
            n_expansions=3,
            temperature=0.7,
            max_tokens=300,
        )
    
    # ‚îÄ‚îÄ BM25 Indexes ‚îÄ‚îÄ
    summary_bm25 = None
    chunk_bm25 = None
    
    if enable_hybrid or enable_summary_prefilter:
        # Auto-detect summaries path
        if summaries_path is None:
            default_path = Path(__file__).parent.parent.parent / "data" / "keep" / "cnil" / "document_summaries.json"
            if default_path.exists():
                summaries_path = str(default_path)
        
        # Summary BM25 Index (pr√©-filtre)
        if enable_summary_prefilter and summaries_path:
            logger.info("üìã Construction index BM25 sur les summaries...")
            summary_bm25 = SummaryBM25Index()
            summary_bm25.build(summaries_path)
        
        # Chunk BM25 Index (sparse retrieval)
        if enable_hybrid:
            logger.info("üì¶ Construction index BM25 sur les chunks...")
            chunk_bm25 = ChunkBM25Index()
            chunk_bm25.build_from_collection(collection)
    
    # ‚îÄ‚îÄ Retriever ‚îÄ‚îÄ
    retriever = create_retriever(
        collection=collection,
        llm_provider=llm_provider,
        summary_bm25_index=summary_bm25,
        chunk_bm25_index=chunk_bm25,
        query_expander=query_expander,
        n_documents=n_documents,
        n_chunks_per_doc=n_chunks_per_doc,
        summary_prefilter_k=40,
        enable_hybrid=enable_hybrid,
        enable_summary_prefilter=enable_summary_prefilter,
    )
    
    # ‚îÄ‚îÄ Reranker ‚îÄ‚îÄ
    reranker = None
    if enable_reranker:
        logger.info("üîÑ Initialisation Cross-Encoder Reranker...")
        reranker = CrossEncoderReranker(
            device="cpu",  # Pas de VRAM consomm√©e
            batch_size=32,
            trust_remote_code=True,  # Requis pour Jina
        )
        # Le mod√®le sera charg√© en lazy au premier appel
    
    # ‚îÄ‚îÄ Context Builder ‚îÄ‚îÄ
    # D√©sactiver reverse packing quand le reranker est actif :
    # le reranker produit un ordre optimal par cross-encoder, mais le reverse
    # packing peut placer un faux positif contextuel (bien scor√© mais hors sujet
    # pr√©cis) en derni√®re position ‚Üí maximum de recency bias ‚Üí r√©ponse biais√©e.
    use_reverse_packing = not enable_reranker
    context_builder = create_context_builder(
        max_context_length=max_context_length,
        include_metadata=True,
        llm_provider=llm_provider,  # Pour r√©sum√©s intelligents
        reverse_packing=use_reverse_packing,
    )
    
    # ‚îÄ‚îÄ Generator ‚îÄ‚îÄ
    generator = create_generator(
        llm_provider=llm_provider,
        model=model,
        temperature=temperature,
    )
    
    init_time = time.time() - init_start
    logger.info(
        f"‚úÖ Pipeline initialis√© en {init_time:.1f}s "
        f"(hybrid={'ON' if enable_hybrid else 'OFF'}, "
        f"reranker={'ON' if enable_reranker else 'OFF'}, "
        f"summary_filter={'ON' if enable_summary_prefilter else 'OFF'}, "
        f"query_expansion={'ON' if enable_query_expansion else 'OFF'})"
    )
    
    return RAGPipeline(
        retriever=retriever,
        context_builder=context_builder,
        generator=generator,
        llm_provider=llm_provider,  # Pour validators
        reranker=reranker,
        debug_mode=debug_mode,
        enable_validation=enable_validation,
        rerank_candidates=rerank_candidates,
        rerank_top_k=rerank_top_k,
    )
