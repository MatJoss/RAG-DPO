"""
Validators - V√©rification de pertinence et grounding
"""
import logging
from typing import List, Optional
from dataclasses import dataclass

logger = logging.getLogger(__name__)


@dataclass
class ValidationResult:
    """R√©sultat de validation"""
    is_valid: bool
    score: float
    reason: str


class RelevanceValidator:
    """
    Valide que les chunks r√©cup√©r√©s sont pertinents pour la question
    
    Utilise le LLM pour scorer la pertinence de chaque chunk
    """
    
    def __init__(self, llm_provider, threshold: float = 0.30):
        """
        Args:
            llm_provider: Provider LLM
            threshold: Seuil de distance pour consid√©rer pertinent (plus bas = meilleur)
                      0.25 = tr√®s strict, 0.30 = √©quilibr√©, 0.35 = permissif
        """
        self.llm_provider = llm_provider
        self.threshold = threshold
    
    def validate_chunks(
        self,
        query: str,
        chunks: List,
        conversation_history: Optional[List] = None
    ) -> List:
        """
        Filtre les chunks non pertinents
        
        Args:
            query: Question utilisateur
            chunks: Liste de RetrievedChunk
            conversation_history: Historique de conversation
        
        Returns:
            Chunks filtr√©s (seulement les pertinents)
        """
        if not chunks:
            return chunks
        
        # Context complet : historique + question
        context_query = query
        if conversation_history:
            # Prendre les 3 derniers messages
            recent = conversation_history[-6:]  # 3 paires user/assistant
            history_text = "\n".join([f"{msg['role']}: {msg['content']}" for msg in recent])
            context_query = f"{history_text}\n\nQuestion actuelle: {query}"
        
        # Filtrage par distance
        filtered = []
        rejected = []
        
        for chunk in chunks:
            if chunk.distance <= self.threshold:
                filtered.append(chunk)
            else:
                rejected.append(chunk)
                logger.warning(
                    f"‚ö†Ô∏è  Chunk rejet√© (distance={chunk.distance:.3f} > {self.threshold}): "
                    f"{chunk.text[:100]}..."
                )
        
        if rejected:
            logger.info(f"‚úÇÔ∏è  {len(rejected)}/{len(chunks)} chunks filtr√©s (non pertinents)")
        
        return filtered if filtered else chunks  # Fallback: garder tout si rien ne passe


class GroundingValidator:
    """
    Valide que la r√©ponse g√©n√©r√©e est bien ground√©e dans les sources
    
    V√©rifie :
    - Pr√©sence de citations [Source X]
    - Pas d'invention de sources
    - Pas d'hallucination : chaque fait doit √™tre dans le contexte
    - Coh√©rence entre r√©ponse et contexte
    """
    
    def __init__(self, llm_provider):
        self.llm_provider = llm_provider
    
    def validate_response(
        self,
        response: str,
        available_sources: List[int],
        context: str
    ) -> ValidationResult:
        """
        Valide que la r√©ponse est bien ground√©e
        
        Args:
            response: R√©ponse g√©n√©r√©e
            available_sources: IDs des sources disponibles [1, 2, 3]
            context: Contexte fourni au LLM
        
        Returns:
            ValidationResult avec score et raison
        """
        issues = []
        
        # 1. V√©rifier pr√©sence de citations
        if "[Source" not in response and "Source " not in response:
            issues.append("Aucune citation de source")
            logger.warning("‚ö†Ô∏è  R√©ponse sans citation de source")
        
        # 2. V√©rifier invention de sources
        import re
        # Match: [Source 1], [Source 1 et Source 2], [Source 1, 2 et 3], etc.
        cited_sources = re.findall(r'Source\s+(\d+)', response)
        cited_ids = [int(s) for s in cited_sources]
        
        invalid_sources = [s for s in cited_ids if s not in available_sources]
        if invalid_sources:
            issues.append(f"Sources invent√©es: {invalid_sources}")
            logger.error(f"‚ùå Sources INVENT√âES: {invalid_sources} (disponibles: {available_sources})")
        
        # 3. V√©rifier r√©ponse vide ou trop courte
        if len(response.strip()) < 50:
            issues.append("R√©ponse trop courte")
            logger.warning("‚ö†Ô∏è  R√©ponse tr√®s courte")
        
        # 4. Info : D√©tecter phrases "consultez" (pas une erreur, juste info)
        consultez_phrases = [
            "consultez la CNIL",
            "consultez les questions",
            "consultez les guides",
            "la CNIL vous propose",
            "la CNIL met √† disposition",
            "vous pouvez consulter"
        ]
        
        response_lower = response.lower()
        found_consultez = [p for p in consultez_phrases if p in response_lower]
        if found_consultez:
            # Info seulement, pas une erreur
            logger.info(f"‚ÑπÔ∏è  Phrases 'consultez' d√©tect√©es: {found_consultez}")
        
        # 5. V√©rifier phrases d'√©vitement critiques
        critical_evasive = [
            "je ne peux pas r√©pondre",
            "je n'ai pas d'information",
            "contactez votre DPO",
            "demandez √† votre d√©l√©gu√©"
        ]
        
        found_evasive = [p for p in critical_evasive if p in response_lower]
        if found_evasive:
            issues.append(f"R√©ponse √©vasive: {found_evasive}")
            logger.warning(f"‚ö†Ô∏è  R√©ponse √©vasive d√©tect√©e: {found_evasive}")
        
        # 6. NOUVEAU : V√©rifier hallucinations (faits invent√©s)
        hallucination_check = self._check_hallucinations(response, context)
        if not hallucination_check['is_grounded']:
            issues.append(f"Hallucination d√©tect√©e: {hallucination_check['reason']}")
            logger.error(f"‚ùå HALLUCINATION: {hallucination_check['reason']}")
        
        # Score : 1.0 si OK, p√©nalit√© par issue
        is_valid = len(issues) == 0
        score = 1.0 - (len(issues) * 0.25)  # -0.25 par probl√®me
        score = max(0.0, score)
        
        reason = "; ".join(issues) if issues else "OK"
        
        if not is_valid:
            logger.warning(f"‚ö†Ô∏è  Validation r√©ponse: {reason} (score={score:.2f})")
        
        return ValidationResult(
            is_valid=is_valid,
            score=score,
            reason=reason
        )
    
    def _check_hallucinations(self, response: str, context: str) -> dict:
        """
        V√©rifie si la r√©ponse contient des hallucinations.
        
        Approche d√©terministe et rapide (pas d'appel LLM) :
        - V√©rifie que les montants ‚Ç¨ cit√©s existent dans le contexte
        - V√©rifie que les articles de loi cit√©s existent dans le contexte
        - V√©rifie que les noms d'organisations cit√©s existent dans le contexte
        
        On ne bloque PAS sur les termes techniques (PIA, AIPD, etc.) qui 
        font partie du vocabulaire RGPD courant.
        """
        import re
        
        context_lower = context.lower()
        response_lower = response.lower()
        issues = []
        
        # 1. V√©rifier les montants ‚Ç¨ invent√©s
        # Ex: "20 millions d'euros", "4% du CA", "10 000 ‚Ç¨"
        amounts_in_response = re.findall(
            r'(\d[\d\s]*(?:millions?|milliards?)?\s*(?:d\'?euros?|‚Ç¨))',
            response_lower
        )
        for amount in amounts_in_response:
            # Extraire le nombre pour chercher dans le contexte
            number = re.search(r'\d[\d\s]*', amount)
            if number:
                num_str = number.group().strip()
                if num_str not in context_lower and len(num_str) > 2:
                    issues.append(f"Montant '{amount.strip()}' non trouv√© dans le contexte")
        
        # 2. V√©rifier les articles de loi invent√©s (art. XX, article XX)
        articles_in_response = re.findall(
            r'(?:article|art\.?)\s+(\d+(?:[\-\.]\d+)?)',
            response_lower
        )
        for art_num in articles_in_response:
            if art_num not in context_lower:
                issues.append(f"Article {art_num} non trouv√© dans le contexte")
        
        # 3. V√©rifier les dates sp√©cifiques invent√©es (25 mai 2018, etc.)
        dates_in_response = re.findall(
            r'(\d{1,2}\s+(?:janvier|f√©vrier|mars|avril|mai|juin|juillet|ao√ªt|septembre|octobre|novembre|d√©cembre)\s+\d{4})',
            response_lower
        )
        for date_str in dates_in_response:
            # Tol√©rance : la date du RGPD (25 mai 2018) est connaissance g√©n√©rale
            if '25 mai 2018' in date_str or '27 avril 2016' in date_str:
                continue
            if date_str not in context_lower:
                issues.append(f"Date '{date_str}' non trouv√©e dans le contexte")
        
        if issues:
            reason = " ; ".join(issues[:3])  # Max 3 issues
            logger.warning(f"‚ö†Ô∏è  Grounding warnings: {reason}")
            return {"is_grounded": False, "reason": reason}
        
        return {"is_grounded": True, "reason": "OK"}
    
    def fix_invented_sources(
        self,
        response: str,
        available_sources: List[int]
    ) -> str:
        """
        Supprime les citations vers des sources invent√©es
        
        Args:
            response: R√©ponse avec potentiellement sources invent√©es
            available_sources: IDs valides [1, 2, 3]
        
        Returns:
            R√©ponse nettoy√©e
        """
        import re
        
        # Trouver toutes les citations
        def replace_citation(match):
            source_num = int(match.group(1))
            if source_num in available_sources:
                return match.group(0)  # Garder
            else:
                logger.info(f"üßπ Suppression citation invalide: [Source {source_num}]")
                return ""  # Supprimer
        
        fixed = re.sub(r'\[Source (\d+)\]', replace_citation, response)
        
        # Nettoyer doubles espaces
        fixed = re.sub(r'  +', ' ', fixed)
        
        return fixed.strip()
