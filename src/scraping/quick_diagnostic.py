"""
Diagnostic Rapide du Scraping
Analyse l'√©tat actuel et identifie les probl√®mes
D√©termine si le scraping est complet ou non
"""

import json
from pathlib import Path
from collections import Counter
from datetime import datetime
import sys


def analyze_completion(project_path: Path, state: dict, actual_html: int, visited: int) -> bool:
    """D√©termine si le scraping est complet"""
    
    print("\n" + "=" * 70)
    print("üîç ANALYSE DE COMPL√âTION")
    print("=" * 70)
    
    # 1. V√©rifier la date de derni√®re mise √† jour
    last_update = state.get('last_update')
    if last_update:
        try:
            last_dt = datetime.fromisoformat(last_update)
            now = datetime.now()
            hours_ago = (now - last_dt).total_seconds() / 3600
            
            print(f"\n‚è∞ Derni√®re mise √† jour : il y a {hours_ago:.1f}h")
            
            if hours_ago < 0.5:
                print(f"   ‚ö†Ô∏è  Scraping tr√®s r√©cent ou en cours")
                is_recent = True
            elif hours_ago < 2:
                print(f"   ‚ö†Ô∏è  Scraping r√©cent (moins de 2h)")
                is_recent = True
            else:
                print(f"   ‚úÖ Scraping ancien (probablement termin√©)")
                is_recent = False
        except:
            print(f"\n‚è∞ Pas de date de derni√®re mise √† jour")
            is_recent = False
    else:
        is_recent = False
    
    # 2. Analyser les HTML vs URLs visit√©es (avec vrais chiffres)
    if visited > 0:
        save_ratio = actual_html / visited
        print(f"\nüìä Ratio sauvegarde : {save_ratio*100:.1f}%")
        print(f"   ({actual_html} HTML r√©els / {visited} URLs)")
        
        if save_ratio > 0.95:
            print(f"   ‚úÖ Excellent ratio (>95%)")
            good_ratio = True
        elif save_ratio > 0.90:
            print(f"   ‚úÖ Tr√®s bon ratio (>90%)")
            good_ratio = True
        elif save_ratio > 0.85:
            print(f"   ‚ö†Ô∏è  Ratio correct (>85%)")
            good_ratio = True
        else:
            print(f"   ‚ùå Ratio faible (<85%) - beaucoup d'√©checs ou incomplet")
            good_ratio = False
    else:
        good_ratio = False
    
    # 3. V√©rifier les √©checs
    failed_urls = state.get('failed_urls', [])
    failed_count = len(failed_urls)
    
    if failed_count > 0:
        print(f"\n‚ùå √âchecs enregistr√©s : {failed_count}")
        fail_ratio = failed_count / visited if visited > 0 else 0
        if fail_ratio < 0.03:
            print(f"   ‚úÖ Tr√®s peu d'√©checs (<3%)")
        elif fail_ratio < 0.05:
            print(f"   ‚úÖ Peu d'√©checs (<5%)")
        else:
            print(f"   ‚ö†Ô∏è  Beaucoup d'√©checs (‚â•5%)")
    
    # 4. Conclusion
    print(f"\n" + "=" * 70)
    print("üéØ CONCLUSION")
    print("=" * 70)
    
    # Crit√®res de compl√©tion
    criteria = {
        'good_ratio': save_ratio > 0.95,
        'not_too_recent': not is_recent,
        'enough_html': actual_html > 5000,
    }
    
    all_good = all(criteria.values())
    
    if all_good:
        print(f"\n‚úÖ Le scraping semble COMPLET !")
        print(f"\n   Crit√®res valid√©s :")
        print(f"   ‚úì Excellent ratio ({save_ratio*100:.1f}%)")
        print(f"   ‚úì {actual_html} HTML collect√©s")
        if not is_recent:
            print(f"   ‚úì Scraping termin√© il y a {hours_ago:.1f}h")
        
        print(f"\nüí° Actions recommand√©es :")
        print(f"   1. ‚úÖ Scraping HTML TERMIN√â")
        print(f"   2. ‚ö†Ô∏è  0 PDF/Docs d√©tect√©s (probl√®me de d√©tection)")
        print(f"   3. ‚Üí Lancer patch pour r√©cup√©rer PDF/docs")
        print(f"      python src/scraping/patch_missing_files.py")
        print(f"   4. ‚Üí Retry des √©checs temporaires si besoin")
        print(f"      python src/scraping/retry_failed_urls.py")
        
        return True
    
    else:
        print(f"\n‚ö†Ô∏è  Le scraping semble INCOMPLET ou EN COURS")
        
        print(f"\n   Probl√®mes d√©tect√©s :")
        if not criteria['good_ratio']:
            print(f"   ‚ùå Ratio sauvegarde faible ({save_ratio*100:.1f}%)")
            print(f"      ‚Üí {visited - actual_html} URLs visit√©es non sauvegard√©es")
        if is_recent:
            print(f"   ‚ö†Ô∏è  Derni√®re activit√© il y a {hours_ago:.1f}h (r√©cent)")
        if not criteria['enough_html']:
            print(f"   ‚ùå Peu de HTML ({actual_html} < 5000)")
        
        print(f"\nüí° Actions recommand√©es :")
        print(f"   1. Test : relancer pour voir si de nouvelles URLs")
        print(f"      python src/scraping/cnil_scraper.py --depth 5")
        print(f"   2. Observer si √ßa scrape ou s'arr√™te imm√©diatement")
        print(f"   3. Si 0 nouvelles pages ‚Üí scraping fini")
        print(f"   4. Si nouvelles pages ‚Üí laisser finir")
        
        return False
    
    print("=" * 70)


def analyze_scraping_state(project_root: str = '.'):
    """Analyse l'√©tat du scraping"""
    
    project_path = Path(project_root)
    state_file = project_path / 'data' / 'metadata' / 'scraping_state.json'
    
    if not state_file.exists():
        print("‚ùå Fichier scraping_state.json introuvable")
        return
    
    with open(state_file, 'r', encoding='utf-8') as f:
        state = json.load(f)
    
    print("=" * 70)
    print("üìä DIAGNOSTIC SCRAPING")
    print("=" * 70)
    
    # Stats du state
    stats = state.get('stats', {})
    visited = len(state.get('visited_urls', []))
    
    html_state = stats.get('html', 0)
    pdf_state = stats.get('pdf', 0)
    docs_state = stats.get('docs', 0)
    errors = stats.get('errors', 0)
    
    print(f"\nüìÑ URLs visit√©es (state) : {visited}")
    
    # Compter les fichiers R√âELS sur disque
    html_dir = project_path / 'data' / 'raw' / 'html'
    pdf_dir = project_path / 'data' / 'raw' / 'pdf'
    docs_dir = project_path / 'data' / 'raw' / 'docs'
    
    actual_html = len(list(html_dir.glob('*.html'))) if html_dir.exists() else 0
    actual_pdf = len(list(pdf_dir.glob('*.pdf'))) if pdf_dir.exists() else 0
    actual_odt = len(list(docs_dir.glob('*.odt'))) if docs_dir.exists() else 0
    actual_xlsx = len(list(docs_dir.glob('*.xlsx'))) if docs_dir.exists() else 0
    actual_docx = len(list(docs_dir.glob('*.docx'))) if docs_dir.exists() else 0
    actual_docs = actual_odt + actual_xlsx + actual_docx
    
    print(f"\nüíæ Fichiers R√âELS sur disque :")
    print(f"   HTML       : {actual_html}")
    print(f"   PDF        : {actual_pdf}")
    print(f"   Documents  : {actual_docs} (ODT: {actual_odt}, XLSX: {actual_xlsx}, DOCX: {actual_docx})")
    print(f"   Total      : {actual_html + actual_pdf + actual_docs}")
    
    print(f"\nüìä Stats dans scraping_state.json :")
    print(f"   HTML       : {html_state}")
    print(f"   PDF        : {pdf_state}")
    print(f"   Documents  : {docs_state}")
    
    # V√©rifier coh√©rence
    if actual_html != html_state:
        print(f"\n‚ö†Ô∏è  INCOH√âRENCE D√âTECT√âE !")
        print(f"   State dit {html_state} HTML, mais il y en a {actual_html} sur disque")
        print(f"   ‚Üí Les stats du state ne sont pas √† jour")
    
    # URLs manquantes
    missing = visited - actual_html
    
    if missing > 0:
        print(f"\n‚ö†Ô∏è  URLs MANQUANTES : {missing}")
        print(f"   ({missing / visited * 100:.1f}% des URLs visit√©es)")
        print(f"   Causes possibles :")
        print(f"   - Redirections (URL compt√©e 2x)")
        print(f"   - Erreurs non logg√©es")
        print(f"   - Fichiers binaires mal d√©tect√©s")
    
    print(f"\n‚ùå Erreurs dans state : {errors}")
    
    # Analyser compl√©tion avec les VRAIS chiffres
    is_complete = analyze_completion(project_path, state, actual_html, visited)
    
    # Analyser l'√©tat "fini ou pas"
    is_complete = analyze_completion(project_path, state)
    
    print("=" * 70)
    print("üìä DIAGNOSTIC SCRAPING")
    print("=" * 70)
    
    # Stats g√©n√©rales
    stats = state.get('stats', {})
    visited = len(state.get('visited_urls', []))
    
    html = stats.get('html', 0)
    pdf = stats.get('pdf', 0)
    docs = stats.get('docs', 0)
    errors = stats.get('errors', 0)
    
    print(f"\nüìÑ URLs visit√©es : {visited}")
    print(f"\nüíæ Fichiers sauvegard√©s :")
    print(f"   HTML       : {html}")
    print(f"   PDF        : {pdf}")
    print(f"   Documents  : {docs}")
    print(f"   Total      : {html + pdf + docs}")
    
    # URLs manquantes
    missing = visited - (html + pdf + docs)
    
    if missing > 0:
        print(f"\n‚ö†Ô∏è  URLs MANQUANTES : {missing}")
        print(f"   ({missing / visited * 100:.1f}% des URLs visit√©es)")
    
    print(f"\n‚ùå Erreurs d√©tect√©es : {errors}")
    
    # Analyser les √©checs
    failed_urls = state.get('failed_urls', [])
    
    if failed_urls:
        print(f"\nüìã URLs EN √âCHEC : {len(failed_urls)}")
        
        # Classifier les erreurs
        error_types = Counter()
        error_examples = {}
        
        for failed in failed_urls:
            error = failed.get('error', 'Unknown')
            error_lower = error.lower()
            
            # Classifier
            if any(x in error_lower for x in ['timeout', '503', '502', '504']):
                error_type = 'Temporaire (timeout/503)'
            elif '429' in error_lower or 'rate limit' in error_lower:
                error_type = 'Rate Limit (429)'
            elif any(x in error_lower for x in ['404', 'not found']):
                error_type = 'Not Found (404)'
            elif any(x in error_lower for x in ['403', 'forbidden']):
                error_type = 'Forbidden (403)'
            elif any(x in error_lower for x in ['connection', 'network', 'ssl']):
                error_type = 'Erreur R√©seau'
            else:
                error_type = 'Autre'
            
            error_types[error_type] += 1
            
            if error_type not in error_examples:
                error_examples[error_type] = {
                    'url': failed.get('url', '')[:60] + '...',
                    'error': error[:80]
                }
        
        print("\n   R√©partition par type :")
        for error_type, count in error_types.most_common():
            pct = count / len(failed_urls) * 100
            print(f"   {error_type:30s} : {count:4d} ({pct:5.1f}%)")
            
            # Exemple
            example = error_examples.get(error_type)
            if example:
                print(f"      Ex: {example['url']}")
                print(f"          {example['error']}")
        
        # Recommandations
        print("\nüí° RECOMMANDATIONS :")
        
        temp_count = sum(count for error_type, count in error_types.items() 
                        if 'Temporaire' in error_type or 'Rate Limit' in error_type 
                        or 'R√©seau' in error_type)
        
        if temp_count > 0:
            print(f"\n   ‚úÖ {temp_count} erreurs temporaires d√©tect√©es")
            print(f"      ‚Üí R√âCUP√âRABLES avec retry intelligent")
            print(f"      ‚Üí Commande : python src/scraping/retry_failed_urls.py")
        
        perm_count = sum(count for error_type, count in error_types.items() 
                        if '404' in error_type or '403' in error_type)
        
        if perm_count > 0:
            print(f"\n   ‚ö†Ô∏è  {perm_count} erreurs permanentes (404/403)")
            print(f"      ‚Üí Non r√©cup√©rables (pages n'existent plus)")
    
    else:
        print("\n‚úÖ Aucune URL en √©chec enregistr√©e")
    
    # Analyse des 130 manquantes
    if missing > len(failed_urls):
        diff = missing - len(failed_urls)
        print(f"\nü§î MYST√àRE : {diff} URLs manquantes non dans failed_urls")
        print(f"   Possible causes :")
        print(f"   - Erreurs non catch√©es (bugs)")
        print(f"   - Redirections non suivies")
        print(f"   - URLs dupliqu√©es compt√©es 2 fois")
        print(f"   - Scraping interrompu puis repris")
    
    # Taille des donn√©es
    size_mb = stats.get('total_size_mb', 0)
    print(f"\nüíæ Taille totale : {size_mb:.2f} MB ({size_mb/1024:.2f} GB)")
    
    print("\n" + "=" * 70)


def main():
    if len(sys.argv) > 1:
        project_root = sys.argv[1]
    else:
        project_root = '.'
    
    analyze_scraping_state(project_root)


if __name__ == "__main__":
    main()