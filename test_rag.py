#!/usr/bin/env python
"""
Test du systÃ¨me RAG complet en CLI
"""
import os
import sys
import logging
import argparse
from pathlib import Path

# Setup path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root / "src"))

import chromadb
from src.utils.llm_provider import OllamaProvider
from src.rag.pipeline import create_pipeline


# Configuration logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)

# Calmer les logs HTTP
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("ollama").setLevel(logging.WARNING)
logging.getLogger("chromadb").setLevel(logging.WARNING)

logger = logging.getLogger(__name__)


# Questions de test DPO
TEST_QUESTIONS = [
    "Comment faire une analyse d'impact (AIPD) ?",
    "Quelles sont les obligations d'un DPO ?",
    "Comment gÃ©rer une violation de donnÃ©es personnelles ?",
    "Qu'est-ce qu'une donnÃ©e Ã  caractÃ¨re personnel ?",
    "Quelles sanctions en cas de non-conformitÃ© RGPD ?",
]


def main():
    parser = argparse.ArgumentParser(description="Test du systÃ¨me RAG")
    parser.add_argument(
        "--question",
        "-q",
        type=str,
        help="Question Ã  poser (sinon mode interactif ou test)"
    )
    parser.add_argument(
        "--test",
        action="store_true",
        help="Lance les questions de test prÃ©dÃ©finies"
    )
    parser.add_argument(
        "--filter-nature",
        type=str,
        choices=["DOCTRINE", "GUIDE", "SANCTION", "TECHNIQUE"],
        help="Filtre par nature de document"
    )
    parser.add_argument(
        "--n-documents",
        type=int,
        default=3,
        help="Nombre de documents Ã  rÃ©cupÃ©rer (dÃ©faut: 3)"
    )
    parser.add_argument(
        "--n-chunks",
        type=int,
        default=4,
        help="Chunks par document (dÃ©faut: 4)"
    )
    parser.add_argument(
        "--debug",
        action="store_true",
        help="Mode debug avec affichage du contexte"
    )
    parser.add_argument(
        "--model",
        type=str,
        default="mistral",
        help="ModÃ¨le Ollama Ã  utiliser (dÃ©faut: mistral)"
    )
    parser.add_argument(
        "--temperature",
        type=float,
        default=0.0,
        help="TempÃ©rature LLM (dÃ©faut: 0.0 = factuel strict)"
    )
    
    args = parser.parse_args()
    
    # Configuration paths
    vectordb_path = project_root / "data" / "vectordb" / "chromadb"
    
    if not vectordb_path.exists():
        logger.error(f"âŒ VectorDB non trouvÃ©e: {vectordb_path}")
        logger.error("ğŸ’¡ Lancer d'abord: python src/processing/create_chromadb_index.py")
        return 1
    
    logger.info("ğŸš€ Initialisation du systÃ¨me RAG...\n")
    
    # 1. Init ChromaDB
    logger.info(f"ğŸ“‚ Chargement ChromaDB: {vectordb_path}")
    client = chromadb.PersistentClient(path=str(vectordb_path))
    collection = client.get_collection("rag_dpo_chunks")
    logger.info(f"âœ… Collection chargÃ©e: {collection.count()} chunks\n")
    
    # 2. Init LLM Provider
    logger.info(f"ğŸ¤– Initialisation Ollama ({args.model})...")
    llm_provider = OllamaProvider(
        base_url="http://localhost:11434",
        model=args.model
    )
    logger.info("âœ… Ollama prÃªt\n")
    
    # 3. Init Pipeline
    logger.info(f"ğŸ”§ Configuration pipeline (docs={args.n_documents}, chunks={args.n_chunks})...")
    pipeline = create_pipeline(
        collection=collection,
        llm_provider=llm_provider,
        n_documents=args.n_documents,
        n_chunks_per_doc=args.n_chunks,
        model=args.model,
        temperature=args.temperature,
        debug_mode=args.debug
    )
    logger.info("âœ… Pipeline configurÃ©\n")
    
    # PrÃ©paration filtre
    where_filter = None
    if args.filter_nature:
        where_filter = {"chunk_nature": args.filter_nature}
        logger.info(f"ğŸ” Filtre actif: nature={args.filter_nature}\n")
    
    # Mode de fonctionnement
    if args.test:
        # Mode test avec questions prÃ©dÃ©finies
        logger.info("ğŸ§ª MODE TEST - Questions prÃ©dÃ©finies\n")
        run_test_mode(pipeline, where_filter)
    
    elif args.question:
        # Mode single question
        logger.info("ğŸ’¬ MODE SINGLE QUESTION\n")
        run_single_question(pipeline, args.question, where_filter)
    
    else:
        # Mode interactif
        logger.info("ğŸ’¬ MODE INTERACTIF - Tapez 'quit' pour quitter\n")
        run_interactive_mode(pipeline, where_filter)
    
    return 0


def run_single_question(pipeline, question: str, where_filter=None):
    """Mode question unique"""
    response = pipeline.query(
        question=question,
        where_filter=where_filter
    )
    
    if response.error:
        logger.error(f"âŒ Erreur: {response.error}")
        return
    
    print(pipeline.format_response(response, show_sources=True))


def run_test_mode(pipeline, where_filter=None):
    """Mode test avec questions prÃ©dÃ©finies"""
    results = []
    
    for i, question in enumerate(TEST_QUESTIONS, 1):
        logger.info(f"\n{'='*80}")
        logger.info(f"TEST {i}/{len(TEST_QUESTIONS)}")
        logger.info(f"{'='*80}\n")
        
        response = pipeline.query(
            question=question,
            where_filter=where_filter
        )
        
        results.append({
            "question": question,
            "success": not response.error,
            "time": response.total_time,
            "cited_sources": len(response.cited_sources)
        })
        
        print(pipeline.format_response(response, show_sources=True))
        print("\n")
    
    # RÃ©sumÃ©
    print("\n" + "="*80)
    print("RÃ‰SUMÃ‰ DES TESTS")
    print("="*80)
    
    success_count = sum(1 for r in results if r["success"])
    avg_time = sum(r["time"] for r in results) / len(results)
    avg_sources = sum(r["cited_sources"] for r in results) / len(results)
    
    print(f"SuccÃ¨s: {success_count}/{len(results)}")
    print(f"Temps moyen: {avg_time:.2f}s")
    print(f"Sources citÃ©es (moyenne): {avg_sources:.1f}")
    print("="*80)


def run_interactive_mode(pipeline, where_filter=None):
    """Mode interactif"""
    conversation_history = []
    
    while True:
        try:
            question = input("\nğŸ¤” Votre question (ou 'quit'): ").strip()
            
            if question.lower() in ['quit', 'exit', 'q']:
                logger.info("ğŸ‘‹ Au revoir!")
                break
            
            if not question:
                continue
            
            print()  # Ligne vide
            
            # Query avec historique
            response = pipeline.query(
                question=question,
                where_filter=where_filter,
                conversation_history=conversation_history
            )
            
            if response.error:
                logger.error(f"âŒ Erreur: {response.error}")
                continue
            
            print(pipeline.format_response(response, show_sources=True))
            
            # Mise Ã  jour historique
            conversation_history.append({"role": "user", "content": question})
            conversation_history.append({"role": "assistant", "content": response.answer})
            
            # Garde seulement les 5 derniers Ã©changes
            if len(conversation_history) > 10:
                conversation_history = conversation_history[-10:]
        
        except KeyboardInterrupt:
            logger.info("\nğŸ‘‹ Interrupted, au revoir!")
            break
        except Exception as e:
            logger.error(f"âŒ Erreur: {e}", exc_info=True)


if __name__ == "__main__":
    sys.exit(main())
